2025-01-04 19:59:05,201 - root - INFO - Loaded the base_model: google/flan-t5-base
2025-01-04 20:03:15,153 - root - INFO - Loaded the traindataset
2025-01-04 20:07:06,562 - root - INFO - Loaded the validataset
2025-01-04 20:07:06,571 - root - INFO - number of trainable parameters: 255369728
2025-01-04 20:07:12,256 - root - INFO - [Train] Epoch 0 [0/35182]: Training loss 11.613805770874023, current lr 5.000000000000001e-07, contrastive loss 854.6229858398438, task loss 3.0675766468048096
2025-01-04 20:08:52,879 - root - INFO - [Train] Epoch 0 [100/35182]: Training loss 1.8109843730926514, current lr 5.05e-05, contrastive loss 126.78535461425781, task loss 0.5431308150291443
2025-01-04 20:10:33,263 - root - INFO - [Train] Epoch 0 [200/35182]: Training loss 1.6511976718902588, current lr 9.999998578778318e-05, contrastive loss 112.2227783203125, task loss 0.5289698839187622
2025-01-04 20:12:13,844 - root - INFO - [Train] Epoch 0 [300/35182]: Training loss 1.5276968479156494, current lr 9.999856456610102e-05, contrastive loss 100.67811584472656, task loss 0.5209158062934875
2025-01-04 20:13:54,203 - root - INFO - [Train] Epoch 0 [400/35182]: Training loss 1.2682462930679321, current lr 9.999714334441887e-05, contrastive loss 79.0538330078125, task loss 0.47770795226097107
2025-01-04 20:15:34,819 - root - INFO - [Train] Epoch 0 [500/35182]: Training loss 1.2443581819534302, current lr 9.999572212273671e-05, contrastive loss 76.83956909179688, task loss 0.4759625494480133
2025-01-04 20:17:15,173 - root - INFO - [Train] Epoch 0 [600/35182]: Training loss 1.1518369913101196, current lr 9.999430090105455e-05, contrastive loss 60.89997100830078, task loss 0.5428373217582703
2025-01-04 20:18:55,525 - root - INFO - [Train] Epoch 0 [700/35182]: Training loss 0.8677252531051636, current lr 9.99928796793724e-05, contrastive loss 37.523719787597656, task loss 0.4924880564212799
2025-01-04 20:20:36,148 - root - INFO - [Train] Epoch 0 [800/35182]: Training loss 0.9374760389328003, current lr 9.999145845769024e-05, contrastive loss 37.88351821899414, task loss 0.5586408376693726
2025-01-04 20:22:16,514 - root - INFO - [Train] Epoch 0 [900/35182]: Training loss 0.9166219830513, current lr 9.999003723600808e-05, contrastive loss 40.17946243286133, task loss 0.5148273706436157
2025-01-04 20:23:57,111 - root - INFO - [Train] Epoch 0 [1000/35182]: Training loss 0.9162378311157227, current lr 9.998861601432593e-05, contrastive loss 36.518959045410156, task loss 0.551048219203949
2025-01-04 20:25:37,492 - root - INFO - [Train] Epoch 0 [1100/35182]: Training loss 0.7904555201530457, current lr 9.998719479264376e-05, contrastive loss 34.864864349365234, task loss 0.4418068826198578
2025-01-04 20:27:18,084 - root - INFO - [Train] Epoch 0 [1200/35182]: Training loss 0.7521911263465881, current lr 9.99857735709616e-05, contrastive loss 25.381385803222656, task loss 0.49837726354599
2025-01-04 20:28:58,446 - root - INFO - [Train] Epoch 0 [1300/35182]: Training loss 0.706656813621521, current lr 9.998435234927944e-05, contrastive loss 21.517436981201172, task loss 0.4914824366569519
2025-01-04 20:30:39,042 - root - INFO - [Train] Epoch 0 [1400/35182]: Training loss 0.768936276435852, current lr 9.998293112759729e-05, contrastive loss 24.521038055419922, task loss 0.5237259268760681
2025-01-04 20:32:19,406 - root - INFO - [Train] Epoch 0 [1500/35182]: Training loss 0.7712526321411133, current lr 9.998150990591513e-05, contrastive loss 22.952259063720703, task loss 0.5417300462722778
2025-01-04 20:34:00,019 - root - INFO - [Train] Epoch 0 [1600/35182]: Training loss 0.7512512803077698, current lr 9.998008868423297e-05, contrastive loss 24.549442291259766, task loss 0.5057568550109863
2025-01-04 20:35:40,382 - root - INFO - [Train] Epoch 0 [1700/35182]: Training loss 0.6647235155105591, current lr 9.997866746255082e-05, contrastive loss 23.752708435058594, task loss 0.4271964728832245
2025-01-04 20:37:20,991 - root - INFO - [Train] Epoch 0 [1800/35182]: Training loss 0.6735206842422485, current lr 9.997724624086866e-05, contrastive loss 16.4127197265625, task loss 0.5093935132026672
2025-01-04 20:39:01,366 - root - INFO - [Train] Epoch 0 [1900/35182]: Training loss 0.7222797870635986, current lr 9.99758250191865e-05, contrastive loss 23.905248641967773, task loss 0.48322731256484985
2025-01-04 20:40:41,952 - root - INFO - [Train] Epoch 0 [2000/35182]: Training loss 0.864856481552124, current lr 9.997440379750433e-05, contrastive loss 29.62350082397461, task loss 0.5686214566230774
2025-01-04 20:42:22,323 - root - INFO - [Train] Epoch 0 [2100/35182]: Training loss 0.7161749601364136, current lr 9.997298257582218e-05, contrastive loss 26.991901397705078, task loss 0.4462559223175049
2025-01-04 20:44:02,915 - root - INFO - [Train] Epoch 0 [2200/35182]: Training loss 0.6863676905632019, current lr 9.997156135414002e-05, contrastive loss 17.075387954711914, task loss 0.5156137943267822
2025-01-04 20:45:43,270 - root - INFO - [Train] Epoch 0 [2300/35182]: Training loss 0.742962121963501, current lr 9.997014013245786e-05, contrastive loss 26.151432037353516, task loss 0.48144781589508057
2025-01-04 20:47:23,883 - root - INFO - [Train] Epoch 0 [2400/35182]: Training loss 0.6848325729370117, current lr 9.99687189107757e-05, contrastive loss 25.681201934814453, task loss 0.4280205965042114
2025-01-04 20:49:04,271 - root - INFO - [Train] Epoch 0 [2500/35182]: Training loss 0.7545977234840393, current lr 9.996729768909355e-05, contrastive loss 16.219669342041016, task loss 0.5924010276794434
2025-01-04 20:50:44,848 - root - INFO - [Train] Epoch 0 [2600/35182]: Training loss 0.6887946128845215, current lr 9.99658764674114e-05, contrastive loss 21.066804885864258, task loss 0.47812655568122864
2025-01-04 20:52:25,228 - root - INFO - [Train] Epoch 0 [2700/35182]: Training loss 0.9078842401504517, current lr 9.996445524572924e-05, contrastive loss 29.216981887817383, task loss 0.615714430809021
2025-01-04 20:54:05,818 - root - INFO - [Train] Epoch 0 [2800/35182]: Training loss 0.6588654518127441, current lr 9.996303402404708e-05, contrastive loss 20.31316375732422, task loss 0.4557338356971741
2025-01-04 20:55:46,203 - root - INFO - [Train] Epoch 0 [2900/35182]: Training loss 0.7299783825874329, current lr 9.996161280236491e-05, contrastive loss 23.047225952148438, task loss 0.4995061159133911
2025-01-04 20:57:26,568 - root - INFO - [Train] Epoch 0 [3000/35182]: Training loss 0.8361457586288452, current lr 9.996019158068277e-05, contrastive loss 29.63360023498535, task loss 0.5398097634315491
2025-01-04 20:59:07,160 - root - INFO - [Train] Epoch 0 [3100/35182]: Training loss 0.7301230430603027, current lr 9.99587703590006e-05, contrastive loss 21.9548282623291, task loss 0.5105747580528259
2025-01-04 21:00:47,537 - root - INFO - [Train] Epoch 0 [3200/35182]: Training loss 0.7057492733001709, current lr 9.995734913731844e-05, contrastive loss 22.38514518737793, task loss 0.4818978011608124
2025-01-04 21:02:28,126 - root - INFO - [Train] Epoch 0 [3300/35182]: Training loss 0.7399709224700928, current lr 9.995592791563628e-05, contrastive loss 24.07417869567871, task loss 0.49922916293144226
2025-01-04 21:04:08,503 - root - INFO - [Train] Epoch 0 [3400/35182]: Training loss 0.7728553414344788, current lr 9.995450669395413e-05, contrastive loss 25.51728630065918, task loss 0.5176824927330017
2025-01-04 21:05:49,097 - root - INFO - [Train] Epoch 0 [3500/35182]: Training loss 0.713919997215271, current lr 9.995308547227197e-05, contrastive loss 26.795196533203125, task loss 0.44596806168556213
2025-01-04 21:07:29,486 - root - INFO - [Train] Epoch 0 [3600/35182]: Training loss 0.620213508605957, current lr 9.995166425058981e-05, contrastive loss 17.58213233947754, task loss 0.4443921744823456
2025-01-04 21:09:10,070 - root - INFO - [Train] Epoch 0 [3700/35182]: Training loss 0.6141060590744019, current lr 9.995024302890766e-05, contrastive loss 18.786035537719727, task loss 0.42624568939208984
2025-01-04 21:10:50,435 - root - INFO - [Train] Epoch 0 [3800/35182]: Training loss 0.729638934135437, current lr 9.99488218072255e-05, contrastive loss 24.07073974609375, task loss 0.4889315664768219
2025-01-04 21:12:31,022 - root - INFO - [Train] Epoch 0 [3900/35182]: Training loss 0.7677567005157471, current lr 9.994740058554334e-05, contrastive loss 21.352252960205078, task loss 0.5542341470718384
2025-01-04 21:14:11,395 - root - INFO - [Train] Epoch 0 [4000/35182]: Training loss 0.810532808303833, current lr 9.994597936386117e-05, contrastive loss 22.13446044921875, task loss 0.5891882181167603
2025-01-04 21:15:51,994 - root - INFO - [Train] Epoch 0 [4100/35182]: Training loss 0.7526134848594666, current lr 9.994455814217902e-05, contrastive loss 20.377574920654297, task loss 0.5488377213478088
2025-01-04 21:17:32,363 - root - INFO - [Train] Epoch 0 [4200/35182]: Training loss 0.8187474012374878, current lr 9.994313692049686e-05, contrastive loss 25.160890579223633, task loss 0.5671384930610657
2025-01-04 21:19:12,950 - root - INFO - [Train] Epoch 0 [4300/35182]: Training loss 0.6957287192344666, current lr 9.99417156988147e-05, contrastive loss 17.958160400390625, task loss 0.5161471366882324
2025-01-04 21:20:53,319 - root - INFO - [Train] Epoch 0 [4400/35182]: Training loss 0.6982281804084778, current lr 9.994029447713255e-05, contrastive loss 19.242897033691406, task loss 0.5057992339134216
2025-01-04 21:22:33,918 - root - INFO - [Train] Epoch 0 [4500/35182]: Training loss 0.6526448130607605, current lr 9.993887325545039e-05, contrastive loss 23.070466995239258, task loss 0.4219401478767395
2025-01-04 21:24:14,273 - root - INFO - [Train] Epoch 0 [4600/35182]: Training loss 0.8132537603378296, current lr 9.993745203376823e-05, contrastive loss 34.565921783447266, task loss 0.46759456396102905
2025-01-04 21:25:54,883 - root - INFO - [Train] Epoch 0 [4700/35182]: Training loss 0.7885430455207825, current lr 9.993603081208608e-05, contrastive loss 24.779766082763672, task loss 0.5407453775405884
2025-01-04 21:27:35,261 - root - INFO - [Train] Epoch 0 [4800/35182]: Training loss 0.7466227412223816, current lr 9.993460959040392e-05, contrastive loss 24.799650192260742, task loss 0.4986262619495392
2025-01-04 21:29:15,858 - root - INFO - [Train] Epoch 0 [4900/35182]: Training loss 0.7287577986717224, current lr 9.993318836872175e-05, contrastive loss 26.731189727783203, task loss 0.4614458978176117
2025-01-04 21:30:56,228 - root - INFO - [Train] Epoch 0 [5000/35182]: Training loss 0.6283270716667175, current lr 9.993176714703961e-05, contrastive loss 20.394025802612305, task loss 0.4243868291378021
2025-01-04 21:32:36,822 - root - INFO - [Train] Epoch 0 [5100/35182]: Training loss 0.7924408912658691, current lr 9.993034592535744e-05, contrastive loss 24.2468318939209, task loss 0.5499725937843323
2025-01-04 21:34:17,189 - root - INFO - [Train] Epoch 0 [5200/35182]: Training loss 0.5881779789924622, current lr 9.992892470367528e-05, contrastive loss 21.56230926513672, task loss 0.3725548982620239
2025-01-04 21:35:57,787 - root - INFO - [Train] Epoch 0 [5300/35182]: Training loss 0.7040337920188904, current lr 9.992750348199312e-05, contrastive loss 21.18804931640625, task loss 0.4921533167362213
2025-01-04 21:37:38,141 - root - INFO - [Train] Epoch 0 [5400/35182]: Training loss 0.6767370104789734, current lr 9.992608226031097e-05, contrastive loss 19.127601623535156, task loss 0.4854609966278076
2025-01-04 21:39:18,523 - root - INFO - [Train] Epoch 0 [5500/35182]: Training loss 0.7125340104103088, current lr 9.992466103862881e-05, contrastive loss 20.030914306640625, task loss 0.5122248530387878
2025-01-04 21:40:59,120 - root - INFO - [Train] Epoch 0 [5600/35182]: Training loss 0.7242864370346069, current lr 9.992323981694665e-05, contrastive loss 18.42266082763672, task loss 0.5400598049163818
2025-01-04 21:42:39,475 - root - INFO - [Train] Epoch 0 [5700/35182]: Training loss 0.7501464486122131, current lr 9.99218185952645e-05, contrastive loss 20.38492202758789, task loss 0.5462972521781921
2025-01-04 21:44:20,082 - root - INFO - [Train] Epoch 0 [5800/35182]: Training loss 0.6684802174568176, current lr 9.992039737358233e-05, contrastive loss 21.69207763671875, task loss 0.4515594244003296
2025-01-04 21:46:00,447 - root - INFO - [Train] Epoch 0 [5900/35182]: Training loss 0.7332851886749268, current lr 9.991897615190018e-05, contrastive loss 19.868675231933594, task loss 0.5345984101295471
2025-01-04 21:47:41,056 - root - INFO - [Train] Epoch 0 [6000/35182]: Training loss 0.7803962230682373, current lr 9.991755493021801e-05, contrastive loss 18.735977172851562, task loss 0.5930364727973938
2025-01-04 21:49:21,430 - root - INFO - [Train] Epoch 0 [6100/35182]: Training loss 0.6451100707054138, current lr 9.991613370853586e-05, contrastive loss 19.31682777404785, task loss 0.4519417881965637
2025-01-04 21:51:02,028 - root - INFO - [Train] Epoch 0 [6200/35182]: Training loss 0.6435068845748901, current lr 9.99147124868537e-05, contrastive loss 17.238590240478516, task loss 0.4711209535598755
2025-01-04 21:52:42,405 - root - INFO - [Train] Epoch 0 [6300/35182]: Training loss 0.7134689688682556, current lr 9.991329126517154e-05, contrastive loss 17.22093391418457, task loss 0.5412596464157104
2025-01-04 21:54:23,004 - root - INFO - [Train] Epoch 0 [6400/35182]: Training loss 0.6549190282821655, current lr 9.991187004348939e-05, contrastive loss 16.27230453491211, task loss 0.4921959936618805
2025-01-04 21:56:03,401 - root - INFO - [Train] Epoch 0 [6500/35182]: Training loss 0.6192029714584351, current lr 9.991044882180723e-05, contrastive loss 13.764334678649902, task loss 0.4815596342086792
2025-01-04 21:57:43,959 - root - INFO - [Train] Epoch 0 [6600/35182]: Training loss 0.6443132162094116, current lr 9.990902760012507e-05, contrastive loss 11.854475975036621, task loss 0.5257684588432312
2025-01-04 21:59:24,341 - root - INFO - [Train] Epoch 0 [6700/35182]: Training loss 0.6991190910339355, current lr 9.990760637844292e-05, contrastive loss 15.25662612915039, task loss 0.546552836894989
2025-01-04 22:01:04,931 - root - INFO - [Train] Epoch 0 [6800/35182]: Training loss 0.6165198683738708, current lr 9.990618515676076e-05, contrastive loss 11.310332298278809, task loss 0.5034165382385254
2025-01-04 22:02:45,313 - root - INFO - [Train] Epoch 0 [6900/35182]: Training loss 0.604192316532135, current lr 9.990476393507859e-05, contrastive loss 11.707672119140625, task loss 0.4871155917644501
2025-01-04 22:04:25,907 - root - INFO - [Train] Epoch 0 [7000/35182]: Training loss 0.6483672857284546, current lr 9.990334271339645e-05, contrastive loss 11.613425254821777, task loss 0.5322330594062805
2025-01-04 22:06:06,293 - root - INFO - [Train] Epoch 0 [7100/35182]: Training loss 0.5624248385429382, current lr 9.990192149171428e-05, contrastive loss 10.337536811828613, task loss 0.4590494930744171
2025-01-04 22:07:46,863 - root - INFO - [Train] Epoch 0 [7200/35182]: Training loss 0.6529636383056641, current lr 9.990050027003212e-05, contrastive loss 9.67552375793457, task loss 0.5562083721160889
2025-01-04 22:09:27,243 - root - INFO - [Train] Epoch 0 [7300/35182]: Training loss 0.5661982297897339, current lr 9.989907904834996e-05, contrastive loss 9.277043342590332, task loss 0.47342780232429504
2025-01-04 22:11:07,829 - root - INFO - [Train] Epoch 0 [7400/35182]: Training loss 0.6437849402427673, current lr 9.989765782666781e-05, contrastive loss 11.410733222961426, task loss 0.5296776294708252
2025-01-04 22:12:48,201 - root - INFO - [Train] Epoch 0 [7500/35182]: Training loss 0.6219975352287292, current lr 9.989623660498565e-05, contrastive loss 11.432722091674805, task loss 0.5076703429222107
2025-01-04 22:14:28,793 - root - INFO - [Train] Epoch 0 [7600/35182]: Training loss 0.6678586006164551, current lr 9.98948153833035e-05, contrastive loss 7.634846210479736, task loss 0.5915101170539856
2025-01-04 22:16:09,160 - root - INFO - [Train] Epoch 0 [7700/35182]: Training loss 0.6320231556892395, current lr 9.989339416162134e-05, contrastive loss 8.203729629516602, task loss 0.5499858856201172
2025-01-04 22:17:49,527 - root - INFO - [Train] Epoch 0 [7800/35182]: Training loss 0.5375372171401978, current lr 9.989197293993917e-05, contrastive loss 8.665872573852539, task loss 0.4508785009384155
2025-01-04 22:19:30,131 - root - INFO - [Train] Epoch 0 [7900/35182]: Training loss 0.5644015073776245, current lr 9.989055171825703e-05, contrastive loss 7.032687187194824, task loss 0.49407464265823364
2025-01-04 22:21:10,518 - root - INFO - [Train] Epoch 0 [8000/35182]: Training loss 0.5808228254318237, current lr 9.988913049657486e-05, contrastive loss 7.579201698303223, task loss 0.5050308108329773
2025-01-04 22:22:51,097 - root - INFO - [Train] Epoch 0 [8100/35182]: Training loss 0.5313003063201904, current lr 9.98877092748927e-05, contrastive loss 6.796160697937012, task loss 0.4633387327194214
2025-01-04 22:24:31,477 - root - INFO - [Train] Epoch 0 [8200/35182]: Training loss 0.49392685294151306, current lr 9.988628805321054e-05, contrastive loss 5.862109184265137, task loss 0.43530577421188354
2025-01-04 22:26:12,064 - root - INFO - [Train] Epoch 0 [8300/35182]: Training loss 0.557486891746521, current lr 9.988486683152839e-05, contrastive loss 7.594201564788818, task loss 0.4815448522567749
2025-01-04 22:27:52,435 - root - INFO - [Train] Epoch 0 [8400/35182]: Training loss 0.5575379729270935, current lr 9.988344560984623e-05, contrastive loss 6.36289119720459, task loss 0.4939090609550476
2025-01-04 22:29:33,031 - root - INFO - [Train] Epoch 0 [8500/35182]: Training loss 0.5247682929039001, current lr 9.988202438816407e-05, contrastive loss 5.947000026702881, task loss 0.46529829502105713
2025-01-04 22:31:13,397 - root - INFO - [Train] Epoch 0 [8600/35182]: Training loss 0.5805028676986694, current lr 9.988060316648192e-05, contrastive loss 6.715122222900391, task loss 0.5133516788482666
2025-01-04 22:32:53,997 - root - INFO - [Train] Epoch 0 [8700/35182]: Training loss 0.5479376316070557, current lr 9.987918194479976e-05, contrastive loss 6.23365592956543, task loss 0.4856010675430298
2025-01-04 22:34:34,357 - root - INFO - [Train] Epoch 0 [8800/35182]: Training loss 0.5346015095710754, current lr 9.98777607231176e-05, contrastive loss 5.977432727813721, task loss 0.4748271703720093
2025-01-04 22:36:14,955 - root - INFO - [Train] Epoch 0 [8900/35182]: Training loss 0.5104548931121826, current lr 9.987633950143543e-05, contrastive loss 6.024619102478027, task loss 0.4502086937427521
2025-01-04 22:37:55,315 - root - INFO - [Train] Epoch 0 [9000/35182]: Training loss 0.5596485137939453, current lr 9.987491827975329e-05, contrastive loss 6.288219451904297, task loss 0.4967663288116455
2025-01-04 22:39:35,924 - root - INFO - [Train] Epoch 0 [9100/35182]: Training loss 0.5275271534919739, current lr 9.987349705807112e-05, contrastive loss 6.548933029174805, task loss 0.4620378017425537
2025-01-04 22:41:16,298 - root - INFO - [Train] Epoch 0 [9200/35182]: Training loss 0.5651780962944031, current lr 9.987207583638896e-05, contrastive loss 6.014918327331543, task loss 0.5050289034843445
2025-01-04 22:42:56,890 - root - INFO - [Train] Epoch 0 [9300/35182]: Training loss 0.6168081760406494, current lr 9.98706546147068e-05, contrastive loss 7.74395227432251, task loss 0.5393686294555664
2025-01-04 22:44:37,279 - root - INFO - [Train] Epoch 0 [9400/35182]: Training loss 0.5307507514953613, current lr 9.986923339302465e-05, contrastive loss 5.651083946228027, task loss 0.47423988580703735
2025-01-04 22:46:17,861 - root - INFO - [Train] Epoch 0 [9500/35182]: Training loss 0.53169846534729, current lr 9.986781217134249e-05, contrastive loss 6.646028518676758, task loss 0.46523821353912354
2025-01-04 22:47:58,231 - root - INFO - [Train] Epoch 0 [9600/35182]: Training loss 0.5487141609191895, current lr 9.986639094966034e-05, contrastive loss 5.5121870040893555, task loss 0.4935922622680664
2025-01-04 22:49:38,832 - root - INFO - [Train] Epoch 0 [9700/35182]: Training loss 0.5541043877601624, current lr 9.986496972797818e-05, contrastive loss 6.287984848022461, task loss 0.4912245571613312
2025-01-04 22:51:19,191 - root - INFO - [Train] Epoch 0 [9800/35182]: Training loss 0.5302397012710571, current lr 9.986354850629601e-05, contrastive loss 6.215130805969238, task loss 0.46808838844299316
2025-01-04 22:52:59,797 - root - INFO - [Train] Epoch 0 [9900/35182]: Training loss 0.5664267539978027, current lr 9.986212728461387e-05, contrastive loss 5.704509735107422, task loss 0.5093816518783569
2025-01-04 22:54:40,175 - root - INFO - [Train] Epoch 0 [10000/35182]: Training loss 0.45883435010910034, current lr 9.98607060629317e-05, contrastive loss 5.363078594207764, task loss 0.40520355105400085
2025-01-04 22:56:20,537 - root - INFO - [Train] Epoch 0 [10100/35182]: Training loss 0.49430254101753235, current lr 9.985928484124954e-05, contrastive loss 5.23301887512207, task loss 0.4419723451137543
2025-01-04 22:58:01,141 - root - INFO - [Train] Epoch 0 [10200/35182]: Training loss 0.6021658778190613, current lr 9.985786361956738e-05, contrastive loss 5.573936939239502, task loss 0.54642653465271
2025-01-04 22:59:41,503 - root - INFO - [Train] Epoch 0 [10300/35182]: Training loss 0.5388780832290649, current lr 9.985644239788523e-05, contrastive loss 5.612773895263672, task loss 0.4827503561973572
2025-01-04 23:01:22,104 - root - INFO - [Train] Epoch 0 [10400/35182]: Training loss 0.5755914449691772, current lr 9.985502117620307e-05, contrastive loss 7.921811103820801, task loss 0.49637332558631897
2025-01-04 23:03:02,474 - root - INFO - [Train] Epoch 0 [10500/35182]: Training loss 0.5722986459732056, current lr 9.985359995452091e-05, contrastive loss 6.607111930847168, task loss 0.5062275528907776
2025-01-04 23:04:43,070 - root - INFO - [Train] Epoch 0 [10600/35182]: Training loss 0.6228269934654236, current lr 9.985217873283876e-05, contrastive loss 5.986691474914551, task loss 0.5629600882530212
2025-01-04 23:06:23,446 - root - INFO - [Train] Epoch 0 [10700/35182]: Training loss 0.5919449329376221, current lr 9.985075751115659e-05, contrastive loss 5.343216896057129, task loss 0.5385127663612366
2025-01-04 23:08:04,052 - root - INFO - [Train] Epoch 0 [10800/35182]: Training loss 0.5754768252372742, current lr 9.984933628947444e-05, contrastive loss 5.49457311630249, task loss 0.5205311179161072
2025-01-04 23:09:44,431 - root - INFO - [Train] Epoch 0 [10900/35182]: Training loss 0.5887070298194885, current lr 9.984791506779227e-05, contrastive loss 5.244350433349609, task loss 0.5362635254859924
2025-01-04 23:11:25,000 - root - INFO - [Train] Epoch 0 [11000/35182]: Training loss 0.5405064821243286, current lr 9.984649384611013e-05, contrastive loss 5.180445194244385, task loss 0.48870205879211426
2025-01-04 23:13:05,370 - root - INFO - [Train] Epoch 0 [11100/35182]: Training loss 0.4990690052509308, current lr 9.984507262442796e-05, contrastive loss 5.280064582824707, task loss 0.44626834988594055
2025-01-04 23:14:45,961 - root - INFO - [Train] Epoch 0 [11200/35182]: Training loss 0.5716785192489624, current lr 9.98436514027458e-05, contrastive loss 5.278904914855957, task loss 0.5188894867897034
2025-01-04 23:16:26,330 - root - INFO - [Train] Epoch 0 [11300/35182]: Training loss 0.5205270648002625, current lr 9.984223018106365e-05, contrastive loss 5.14267110824585, task loss 0.46910035610198975
2025-01-04 23:18:06,945 - root - INFO - [Train] Epoch 0 [11400/35182]: Training loss 0.5205364227294922, current lr 9.984080895938149e-05, contrastive loss 6.380993366241455, task loss 0.4567265212535858
2025-01-04 23:19:47,326 - root - INFO - [Train] Epoch 0 [11500/35182]: Training loss 0.5291845202445984, current lr 9.983938773769933e-05, contrastive loss 5.528825759887695, task loss 0.4738962650299072
2025-01-04 23:21:27,905 - root - INFO - [Train] Epoch 0 [11600/35182]: Training loss 0.5689559578895569, current lr 9.983796651601718e-05, contrastive loss 7.264330863952637, task loss 0.4963126480579376
2025-01-04 23:23:08,301 - root - INFO - [Train] Epoch 0 [11700/35182]: Training loss 0.6083717346191406, current lr 9.983654529433502e-05, contrastive loss 6.189093589782715, task loss 0.5464807748794556
2025-01-04 23:24:48,875 - root - INFO - [Train] Epoch 0 [11800/35182]: Training loss 0.5939344763755798, current lr 9.983512407265285e-05, contrastive loss 6.509685516357422, task loss 0.5288376212120056
2025-01-04 23:26:29,253 - root - INFO - [Train] Epoch 0 [11900/35182]: Training loss 0.5321991443634033, current lr 9.98337028509707e-05, contrastive loss 6.767844200134277, task loss 0.46452072262763977
2025-01-04 23:28:09,838 - root - INFO - [Train] Epoch 0 [12000/35182]: Training loss 0.543968915939331, current lr 9.983228162928854e-05, contrastive loss 5.8218841552734375, task loss 0.4857500493526459
2025-01-04 23:29:50,211 - root - INFO - [Train] Epoch 0 [12100/35182]: Training loss 0.5523709654808044, current lr 9.983086040760638e-05, contrastive loss 5.451786041259766, task loss 0.4978531002998352
2025-01-04 23:31:30,803 - root - INFO - [Train] Epoch 0 [12200/35182]: Training loss 0.5280282497406006, current lr 9.982943918592422e-05, contrastive loss 5.753280162811279, task loss 0.47049546241760254
2025-01-04 23:33:11,172 - root - INFO - [Train] Epoch 0 [12300/35182]: Training loss 0.591995358467102, current lr 9.982801796424207e-05, contrastive loss 5.387546539306641, task loss 0.538119912147522
2025-01-04 23:34:51,530 - root - INFO - [Train] Epoch 0 [12400/35182]: Training loss 0.5277454257011414, current lr 9.982659674255991e-05, contrastive loss 5.6438727378845215, task loss 0.471306711435318
2025-01-04 23:36:32,133 - root - INFO - [Train] Epoch 0 [12500/35182]: Training loss 0.49502021074295044, current lr 9.982517552087775e-05, contrastive loss 5.599396228790283, task loss 0.43902626633644104
2025-01-04 23:38:12,500 - root - INFO - [Train] Epoch 0 [12600/35182]: Training loss 0.5179123878479004, current lr 9.98237542991956e-05, contrastive loss 6.138002872467041, task loss 0.4565323293209076
2025-01-04 23:39:53,099 - root - INFO - [Train] Epoch 0 [12700/35182]: Training loss 0.5577563047409058, current lr 9.982233307751343e-05, contrastive loss 5.795191764831543, task loss 0.49980437755584717
2025-01-04 23:41:33,468 - root - INFO - [Train] Epoch 0 [12800/35182]: Training loss 0.6512906551361084, current lr 9.982091185583128e-05, contrastive loss 5.483271598815918, task loss 0.5964579582214355
2025-01-04 23:43:14,073 - root - INFO - [Train] Epoch 0 [12900/35182]: Training loss 0.5614357590675354, current lr 9.981949063414911e-05, contrastive loss 5.131401062011719, task loss 0.510121762752533
2025-01-04 23:44:54,445 - root - INFO - [Train] Epoch 0 [13000/35182]: Training loss 0.6490016579627991, current lr 9.981806941246696e-05, contrastive loss 5.479205131530762, task loss 0.594209611415863
2025-01-04 23:46:35,038 - root - INFO - [Train] Epoch 0 [13100/35182]: Training loss 0.5540944337844849, current lr 9.98166481907848e-05, contrastive loss 6.050970554351807, task loss 0.4935847520828247
2025-01-04 23:48:15,415 - root - INFO - [Train] Epoch 0 [13200/35182]: Training loss 0.4744036793708801, current lr 9.981522696910264e-05, contrastive loss 6.268026351928711, task loss 0.41172340512275696
2025-01-04 23:49:56,004 - root - INFO - [Train] Epoch 0 [13300/35182]: Training loss 0.6463651061058044, current lr 9.981380574742049e-05, contrastive loss 5.634671211242676, task loss 0.5900183916091919
2025-01-04 23:51:36,382 - root - INFO - [Train] Epoch 0 [13400/35182]: Training loss 0.506186842918396, current lr 9.981238452573833e-05, contrastive loss 6.080770492553711, task loss 0.4453791677951813
2025-01-04 23:53:16,977 - root - INFO - [Train] Epoch 0 [13500/35182]: Training loss 0.5792691707611084, current lr 9.981096330405617e-05, contrastive loss 5.167140960693359, task loss 0.5275977849960327
2025-01-04 23:54:57,348 - root - INFO - [Train] Epoch 0 [13600/35182]: Training loss 0.5894766449928284, current lr 9.9809542082374e-05, contrastive loss 5.39023494720459, task loss 0.5355743169784546
2025-01-04 23:56:37,947 - root - INFO - [Train] Epoch 0 [13700/35182]: Training loss 0.5302402377128601, current lr 9.980812086069186e-05, contrastive loss 5.227643013000488, task loss 0.4779638350009918
2025-01-04 23:58:18,318 - root - INFO - [Train] Epoch 0 [13800/35182]: Training loss 0.492024302482605, current lr 9.980669963900969e-05, contrastive loss 5.006361484527588, task loss 0.4419606924057007
2025-01-04 23:59:58,906 - root - INFO - [Train] Epoch 0 [13900/35182]: Training loss 0.5728274583816528, current lr 9.980527841732755e-05, contrastive loss 5.267151355743408, task loss 0.5201559662818909
2025-01-05 00:01:39,285 - root - INFO - [Train] Epoch 0 [14000/35182]: Training loss 0.5884436964988708, current lr 9.980385719564538e-05, contrastive loss 5.121577262878418, task loss 0.5372279286384583
2025-01-05 00:03:19,869 - root - INFO - [Train] Epoch 0 [14100/35182]: Training loss 0.5679405927658081, current lr 9.980243597396322e-05, contrastive loss 5.191899299621582, task loss 0.5160216093063354
2025-01-05 00:05:00,246 - root - INFO - [Train] Epoch 0 [14200/35182]: Training loss 0.5552353858947754, current lr 9.980101475228106e-05, contrastive loss 5.113000392913818, task loss 0.5041053891181946
2025-01-05 00:06:40,838 - root - INFO - [Train] Epoch 0 [14300/35182]: Training loss 0.5137448906898499, current lr 9.979959353059891e-05, contrastive loss 5.064435005187988, task loss 0.46310052275657654
2025-01-05 00:08:21,211 - root - INFO - [Train] Epoch 0 [14400/35182]: Training loss 0.5862450003623962, current lr 9.979817230891675e-05, contrastive loss 4.885998725891113, task loss 0.5373849868774414
2025-01-05 00:10:01,807 - root - INFO - [Train] Epoch 0 [14500/35182]: Training loss 0.5519677996635437, current lr 9.97967510872346e-05, contrastive loss 5.249573707580566, task loss 0.499472051858902
2025-01-05 00:11:42,164 - root - INFO - [Train] Epoch 0 [14600/35182]: Training loss 0.5268732905387878, current lr 9.979532986555244e-05, contrastive loss 5.389521598815918, task loss 0.47297805547714233
2025-01-05 00:13:22,525 - root - INFO - [Train] Epoch 0 [14700/35182]: Training loss 0.48103490471839905, current lr 9.979390864387027e-05, contrastive loss 5.060422897338867, task loss 0.43043068051338196
2025-01-05 00:15:03,135 - root - INFO - [Train] Epoch 0 [14800/35182]: Training loss 0.6252714395523071, current lr 9.979248742218812e-05, contrastive loss 4.991692543029785, task loss 0.5753545165061951
2025-01-05 00:16:43,498 - root - INFO - [Train] Epoch 0 [14900/35182]: Training loss 0.6592448353767395, current lr 9.979106620050595e-05, contrastive loss 5.070029258728027, task loss 0.6085445284843445
2025-01-05 00:18:24,109 - root - INFO - [Train] Epoch 0 [15000/35182]: Training loss 0.5061384439468384, current lr 9.97896449788238e-05, contrastive loss 5.561610221862793, task loss 0.4505223333835602
2025-01-05 00:20:04,490 - root - INFO - [Train] Epoch 0 [15100/35182]: Training loss 0.5523406863212585, current lr 9.978822375714164e-05, contrastive loss 5.460110664367676, task loss 0.4977395534515381
2025-01-05 00:21:45,078 - root - INFO - [Train] Epoch 0 [15200/35182]: Training loss 0.5515307188034058, current lr 9.978680253545948e-05, contrastive loss 5.075839996337891, task loss 0.5007722973823547
2025-01-05 00:23:25,443 - root - INFO - [Train] Epoch 0 [15300/35182]: Training loss 0.5545015335083008, current lr 9.978538131377733e-05, contrastive loss 5.226224899291992, task loss 0.5022392868995667
2025-01-05 00:25:06,049 - root - INFO - [Train] Epoch 0 [15400/35182]: Training loss 0.5434412360191345, current lr 9.978396009209517e-05, contrastive loss 5.0148797035217285, task loss 0.4932924509048462
2025-01-05 00:26:46,419 - root - INFO - [Train] Epoch 0 [15500/35182]: Training loss 0.5557595491409302, current lr 9.978253887041301e-05, contrastive loss 5.2524895668029785, task loss 0.5032346844673157
2025-01-05 00:28:27,017 - root - INFO - [Train] Epoch 0 [15600/35182]: Training loss 0.4949335753917694, current lr 9.978111764873084e-05, contrastive loss 5.113226890563965, task loss 0.44380131363868713
2025-01-05 00:30:07,395 - root - INFO - [Train] Epoch 0 [15700/35182]: Training loss 0.5898266434669495, current lr 9.97796964270487e-05, contrastive loss 4.886070251464844, task loss 0.5409659147262573
2025-01-05 00:31:47,974 - root - INFO - [Train] Epoch 0 [15800/35182]: Training loss 0.5672686696052551, current lr 9.977827520536653e-05, contrastive loss 5.104547500610352, task loss 0.5162231922149658
2025-01-05 00:33:28,351 - root - INFO - [Train] Epoch 0 [15900/35182]: Training loss 0.5714308023452759, current lr 9.977685398368439e-05, contrastive loss 5.0405378341674805, task loss 0.5210254192352295
2025-01-05 00:35:08,950 - root - INFO - [Train] Epoch 0 [16000/35182]: Training loss 0.5321468114852905, current lr 9.977543276200222e-05, contrastive loss 5.141087532043457, task loss 0.4807359576225281
2025-01-05 00:36:49,318 - root - INFO - [Train] Epoch 0 [16100/35182]: Training loss 0.5519379377365112, current lr 9.977401154032006e-05, contrastive loss 4.851406097412109, task loss 0.5034238696098328
2025-01-05 00:38:29,909 - root - INFO - [Train] Epoch 0 [16200/35182]: Training loss 0.538533627986908, current lr 9.97725903186379e-05, contrastive loss 5.038336753845215, task loss 0.4881502389907837
2025-01-05 00:40:10,275 - root - INFO - [Train] Epoch 0 [16300/35182]: Training loss 0.5404308438301086, current lr 9.977116909695575e-05, contrastive loss 4.800994873046875, task loss 0.49242088198661804
2025-01-05 00:41:50,872 - root - INFO - [Train] Epoch 0 [16400/35182]: Training loss 0.5693212747573853, current lr 9.976974787527359e-05, contrastive loss 5.033273220062256, task loss 0.5189885497093201
2025-01-05 00:43:31,236 - root - INFO - [Train] Epoch 0 [16500/35182]: Training loss 0.5069789290428162, current lr 9.976832665359143e-05, contrastive loss 4.849368572235107, task loss 0.4584852457046509
2025-01-05 00:45:11,840 - root - INFO - [Train] Epoch 0 [16600/35182]: Training loss 0.5269715785980225, current lr 9.976690543190928e-05, contrastive loss 5.08929443359375, task loss 0.4760786294937134
2025-01-05 00:46:52,201 - root - INFO - [Train] Epoch 0 [16700/35182]: Training loss 0.5880346298217773, current lr 9.976548421022711e-05, contrastive loss 4.771052837371826, task loss 0.5403240919113159
2025-01-05 00:48:32,809 - root - INFO - [Train] Epoch 0 [16800/35182]: Training loss 0.5171365141868591, current lr 9.976406298854496e-05, contrastive loss 4.755919933319092, task loss 0.4695773422718048
2025-01-05 00:50:13,178 - root - INFO - [Train] Epoch 0 [16900/35182]: Training loss 0.46438243985176086, current lr 9.97626417668628e-05, contrastive loss 4.62565803527832, task loss 0.41812586784362793
2025-01-05 00:51:53,548 - root - INFO - [Train] Epoch 0 [17000/35182]: Training loss 0.5487903952598572, current lr 9.976122054518064e-05, contrastive loss 4.940700531005859, task loss 0.4993833899497986
2025-01-05 00:53:34,143 - root - INFO - [Train] Epoch 0 [17100/35182]: Training loss 0.5909419059753418, current lr 9.975979932349848e-05, contrastive loss 4.892706394195557, task loss 0.5420148372650146
2025-01-05 00:55:14,500 - root - INFO - [Train] Epoch 0 [17200/35182]: Training loss 0.5672718286514282, current lr 9.975837810181632e-05, contrastive loss 4.739530563354492, task loss 0.5198765397071838
2025-01-05 00:56:55,117 - root - INFO - [Train] Epoch 0 [17300/35182]: Training loss 0.5666319727897644, current lr 9.975695688013417e-05, contrastive loss 4.843468189239502, task loss 0.5181972980499268
2025-01-05 00:58:35,481 - root - INFO - [Train] Epoch 0 [17400/35182]: Training loss 0.570395827293396, current lr 9.975553565845201e-05, contrastive loss 4.806862831115723, task loss 0.522327184677124
2025-01-05 01:00:16,075 - root - INFO - [Train] Epoch 0 [17500/35182]: Training loss 0.5275543928146362, current lr 9.975411443676986e-05, contrastive loss 5.521862983703613, task loss 0.4723357856273651
2025-01-05 01:01:56,435 - root - INFO - [Train] Epoch 0 [17600/35182]: Training loss 0.521619439125061, current lr 9.975269321508768e-05, contrastive loss 4.8729658126831055, task loss 0.47288978099823
2025-01-05 01:03:37,043 - root - INFO - [Train] Epoch 0 [17700/35182]: Training loss 0.562704861164093, current lr 9.975127199340554e-05, contrastive loss 4.995721340179443, task loss 0.5127476453781128
2025-01-05 01:05:17,405 - root - INFO - [Train] Epoch 0 [17800/35182]: Training loss 0.5916430354118347, current lr 9.974985077172337e-05, contrastive loss 4.840340614318848, task loss 0.5432396531105042
2025-01-05 01:06:58,002 - root - INFO - [Train] Epoch 0 [17900/35182]: Training loss 0.5319385528564453, current lr 9.974842955004123e-05, contrastive loss 4.87865686416626, task loss 0.48315197229385376
2025-01-05 01:08:38,368 - root - INFO - [Train] Epoch 0 [18000/35182]: Training loss 0.589171290397644, current lr 9.974700832835906e-05, contrastive loss 4.909233093261719, task loss 0.5400789380073547
2025-01-05 01:10:18,976 - root - INFO - [Train] Epoch 0 [18100/35182]: Training loss 0.5737468004226685, current lr 9.97455871066769e-05, contrastive loss 4.824464797973633, task loss 0.5255021452903748
2025-01-05 01:11:59,338 - root - INFO - [Train] Epoch 0 [18200/35182]: Training loss 0.5287307500839233, current lr 9.974416588499475e-05, contrastive loss 4.9337873458862305, task loss 0.4793928563594818
2025-01-05 01:13:39,950 - root - INFO - [Train] Epoch 0 [18300/35182]: Training loss 0.581841230392456, current lr 9.974274466331259e-05, contrastive loss 5.202302932739258, task loss 0.5298181772232056
2025-01-05 01:15:20,325 - root - INFO - [Train] Epoch 0 [18400/35182]: Training loss 0.5243194699287415, current lr 9.974132344163043e-05, contrastive loss 4.79472017288208, task loss 0.47637227177619934
2025-01-05 01:17:00,915 - root - INFO - [Train] Epoch 0 [18500/35182]: Training loss 0.5436792969703674, current lr 9.973990221994826e-05, contrastive loss 4.726966381072998, task loss 0.49640965461730957
2025-01-05 01:18:41,280 - root - INFO - [Train] Epoch 0 [18600/35182]: Training loss 0.5028066635131836, current lr 9.973848099826612e-05, contrastive loss 5.169622421264648, task loss 0.4511104226112366
2025-01-05 01:20:21,876 - root - INFO - [Train] Epoch 0 [18700/35182]: Training loss 0.5849844217300415, current lr 9.973705977658395e-05, contrastive loss 4.83786153793335, task loss 0.5366058349609375
2025-01-05 01:22:02,243 - root - INFO - [Train] Epoch 0 [18800/35182]: Training loss 0.6152129173278809, current lr 9.97356385549018e-05, contrastive loss 4.838703632354736, task loss 0.5668258666992188
2025-01-05 01:23:42,845 - root - INFO - [Train] Epoch 0 [18900/35182]: Training loss 0.5365349054336548, current lr 9.973421733321964e-05, contrastive loss 4.855240821838379, task loss 0.48798248171806335
2025-01-05 01:25:23,211 - root - INFO - [Train] Epoch 0 [19000/35182]: Training loss 0.5251979827880859, current lr 9.973279611153748e-05, contrastive loss 4.795453071594238, task loss 0.47724342346191406
2025-01-05 01:27:03,814 - root - INFO - [Train] Epoch 0 [19100/35182]: Training loss 0.5613266229629517, current lr 9.973137488985532e-05, contrastive loss 4.912522315979004, task loss 0.5122014284133911
2025-01-05 01:28:44,176 - root - INFO - [Train] Epoch 0 [19200/35182]: Training loss 0.5204870104789734, current lr 9.972995366817317e-05, contrastive loss 4.744969844818115, task loss 0.4730373024940491
2025-01-05 01:30:24,536 - root - INFO - [Train] Epoch 0 [19300/35182]: Training loss 0.5293083190917969, current lr 9.972853244649101e-05, contrastive loss 4.643415451049805, task loss 0.48287415504455566
2025-01-05 01:32:05,142 - root - INFO - [Train] Epoch 0 [19400/35182]: Training loss 0.6112983822822571, current lr 9.972711122480885e-05, contrastive loss 5.910860538482666, task loss 0.5521897673606873
2025-01-05 01:33:45,507 - root - INFO - [Train] Epoch 0 [19500/35182]: Training loss 0.49353671073913574, current lr 9.97256900031267e-05, contrastive loss 4.697724342346191, task loss 0.44655945897102356
2025-01-05 01:35:26,122 - root - INFO - [Train] Epoch 0 [19600/35182]: Training loss 0.5268381834030151, current lr 9.972426878144453e-05, contrastive loss 4.812769889831543, task loss 0.47871047258377075
2025-01-05 01:37:06,485 - root - INFO - [Train] Epoch 0 [19700/35182]: Training loss 0.5019770264625549, current lr 9.972284755976238e-05, contrastive loss 4.688087463378906, task loss 0.45509612560272217
2025-01-05 01:38:47,085 - root - INFO - [Train] Epoch 0 [19800/35182]: Training loss 0.5284210443496704, current lr 9.972142633808021e-05, contrastive loss 4.935585021972656, task loss 0.4790652096271515
2025-01-05 01:40:27,454 - root - INFO - [Train] Epoch 0 [19900/35182]: Training loss 0.604020893573761, current lr 9.972000511639807e-05, contrastive loss 4.650712966918945, task loss 0.5575137734413147
2025-01-05 01:42:08,047 - root - INFO - [Train] Epoch 0 [20000/35182]: Training loss 0.5679939985275269, current lr 9.97185838947159e-05, contrastive loss 4.813530921936035, task loss 0.519858717918396
2025-01-05 01:43:48,401 - root - INFO - [Train] Epoch 0 [20100/35182]: Training loss 0.5545257925987244, current lr 9.971716267303374e-05, contrastive loss 4.66423225402832, task loss 0.5078834891319275
2025-01-05 01:45:29,013 - root - INFO - [Train] Epoch 0 [20200/35182]: Training loss 0.5819826722145081, current lr 9.971574145135159e-05, contrastive loss 5.02374267578125, task loss 0.5317452549934387
2025-01-05 01:47:09,389 - root - INFO - [Train] Epoch 0 [20300/35182]: Training loss 0.5902393460273743, current lr 9.971432022966943e-05, contrastive loss 4.731517314910889, task loss 0.542924165725708
2025-01-05 01:48:49,987 - root - INFO - [Train] Epoch 0 [20400/35182]: Training loss 0.561726987361908, current lr 9.971289900798727e-05, contrastive loss 4.685958385467529, task loss 0.5148674249649048
2025-01-05 01:50:30,362 - root - INFO - [Train] Epoch 0 [20500/35182]: Training loss 0.5723130106925964, current lr 9.97114777863051e-05, contrastive loss 5.0117692947387695, task loss 0.5221953392028809
2025-01-05 01:52:10,946 - root - INFO - [Train] Epoch 0 [20600/35182]: Training loss 0.6141147017478943, current lr 9.971005656462296e-05, contrastive loss 5.432196140289307, task loss 0.5597927570343018
2025-01-05 01:53:51,323 - root - INFO - [Train] Epoch 0 [20700/35182]: Training loss 0.5561858415603638, current lr 9.970863534294079e-05, contrastive loss 4.859768390655518, task loss 0.5075881481170654
2025-01-05 01:55:31,916 - root - INFO - [Train] Epoch 0 [20800/35182]: Training loss 0.5314309000968933, current lr 9.970721412125865e-05, contrastive loss 4.957553386688232, task loss 0.4818553626537323
2025-01-05 01:57:12,287 - root - INFO - [Train] Epoch 0 [20900/35182]: Training loss 0.5751897096633911, current lr 9.970579289957648e-05, contrastive loss 4.778242111206055, task loss 0.5274072885513306
2025-01-05 01:58:52,879 - root - INFO - [Train] Epoch 0 [21000/35182]: Training loss 0.5650386214256287, current lr 9.970437167789432e-05, contrastive loss 4.9474778175354, task loss 0.5155638456344604
2025-01-05 02:00:33,252 - root - INFO - [Train] Epoch 0 [21100/35182]: Training loss 0.5707672834396362, current lr 9.970295045621216e-05, contrastive loss 4.683367729187012, task loss 0.5239335894584656
2025-01-05 02:02:13,848 - root - INFO - [Train] Epoch 0 [21200/35182]: Training loss 0.5019134879112244, current lr 9.970152923453e-05, contrastive loss 4.819637298583984, task loss 0.4537171423435211
2025-01-05 02:03:54,207 - root - INFO - [Train] Epoch 0 [21300/35182]: Training loss 0.5780183672904968, current lr 9.970010801284785e-05, contrastive loss 4.840922832489014, task loss 0.5296091437339783
2025-01-05 02:05:34,816 - root - INFO - [Train] Epoch 0 [21400/35182]: Training loss 0.5435653924942017, current lr 9.969868679116569e-05, contrastive loss 4.923351287841797, task loss 0.49433189630508423
2025-01-05 02:07:15,173 - root - INFO - [Train] Epoch 0 [21500/35182]: Training loss 0.5598172545433044, current lr 9.969726556948354e-05, contrastive loss 4.595715522766113, task loss 0.5138601064682007
2025-01-05 02:08:55,529 - root - INFO - [Train] Epoch 0 [21600/35182]: Training loss 0.5194122195243835, current lr 9.969584434780137e-05, contrastive loss 4.747297286987305, task loss 0.47193923592567444
2025-01-05 02:10:36,149 - root - INFO - [Train] Epoch 0 [21700/35182]: Training loss 0.5731537938117981, current lr 9.969442312611922e-05, contrastive loss 4.6015238761901855, task loss 0.5271385312080383
2025-01-05 02:12:16,513 - root - INFO - [Train] Epoch 0 [21800/35182]: Training loss 0.5389291644096375, current lr 9.969300190443705e-05, contrastive loss 4.8625102043151855, task loss 0.4903040826320648
2025-01-05 02:13:57,118 - root - INFO - [Train] Epoch 0 [21900/35182]: Training loss 0.6683330535888672, current lr 9.969158068275491e-05, contrastive loss 4.668623924255371, task loss 0.6216468214988708
2025-01-05 02:15:37,489 - root - INFO - [Train] Epoch 0 [22000/35182]: Training loss 0.5293301343917847, current lr 9.969015946107274e-05, contrastive loss 4.782071590423584, task loss 0.4815094470977783
2025-01-05 02:17:18,080 - root - INFO - [Train] Epoch 0 [22100/35182]: Training loss 0.5440784096717834, current lr 9.968873823939058e-05, contrastive loss 4.762879371643066, task loss 0.4964495897293091
2025-01-05 02:18:58,451 - root - INFO - [Train] Epoch 0 [22200/35182]: Training loss 0.518871009349823, current lr 9.968731701770843e-05, contrastive loss 4.826581954956055, task loss 0.47060519456863403
2025-01-05 02:20:39,044 - root - INFO - [Train] Epoch 0 [22300/35182]: Training loss 0.5501283407211304, current lr 9.968589579602627e-05, contrastive loss 4.609435081481934, task loss 0.5040339827537537
2025-01-05 02:22:19,414 - root - INFO - [Train] Epoch 0 [22400/35182]: Training loss 0.5255448222160339, current lr 9.968447457434411e-05, contrastive loss 4.724340438842773, task loss 0.47830143570899963
2025-01-05 02:24:00,020 - root - INFO - [Train] Epoch 0 [22500/35182]: Training loss 0.5631209015846252, current lr 9.968305335266194e-05, contrastive loss 4.682878017425537, task loss 0.5162920951843262
2025-01-05 02:25:40,392 - root - INFO - [Train] Epoch 0 [22600/35182]: Training loss 0.5292253494262695, current lr 9.96816321309798e-05, contrastive loss 4.774813652038574, task loss 0.4814772307872772
2025-01-05 02:27:20,988 - root - INFO - [Train] Epoch 0 [22700/35182]: Training loss 0.5693777799606323, current lr 9.968021090929763e-05, contrastive loss 4.737059593200684, task loss 0.522007167339325
2025-01-05 02:29:01,346 - root - INFO - [Train] Epoch 0 [22800/35182]: Training loss 0.5736523866653442, current lr 9.967878968761549e-05, contrastive loss 4.768424987792969, task loss 0.5259681344032288
2025-01-05 02:30:41,949 - root - INFO - [Train] Epoch 0 [22900/35182]: Training loss 0.5187827348709106, current lr 9.967736846593332e-05, contrastive loss 4.715273380279541, task loss 0.4716299772262573
2025-01-05 02:32:22,316 - root - INFO - [Train] Epoch 0 [23000/35182]: Training loss 0.48303839564323425, current lr 9.967594724425116e-05, contrastive loss 4.8114423751831055, task loss 0.4349239766597748
2025-01-05 02:34:02,921 - root - INFO - [Train] Epoch 0 [23100/35182]: Training loss 0.6192973256111145, current lr 9.9674526022569e-05, contrastive loss 4.659202575683594, task loss 0.5727053284645081
2025-01-05 02:35:43,312 - root - INFO - [Train] Epoch 0 [23200/35182]: Training loss 0.5282275676727295, current lr 9.967310480088685e-05, contrastive loss 4.784793853759766, task loss 0.4803796410560608
2025-01-05 02:37:23,888 - root - INFO - [Train] Epoch 0 [23300/35182]: Training loss 0.49005523324012756, current lr 9.967168357920469e-05, contrastive loss 4.723237037658691, task loss 0.4428228735923767
2025-01-05 02:39:04,249 - root - INFO - [Train] Epoch 0 [23400/35182]: Training loss 0.5404069423675537, current lr 9.967026235752252e-05, contrastive loss 4.769665718078613, task loss 0.49271029233932495
2025-01-05 02:40:44,853 - root - INFO - [Train] Epoch 0 [23500/35182]: Training loss 0.549015462398529, current lr 9.966884113584038e-05, contrastive loss 4.66840934753418, task loss 0.5023313760757446
2025-01-05 02:42:25,219 - root - INFO - [Train] Epoch 0 [23600/35182]: Training loss 0.5087619423866272, current lr 9.966741991415821e-05, contrastive loss 4.779212951660156, task loss 0.46096983551979065
2025-01-05 02:44:05,819 - root - INFO - [Train] Epoch 0 [23700/35182]: Training loss 0.5532985925674438, current lr 9.966599869247606e-05, contrastive loss 4.8339385986328125, task loss 0.504959225654602
2025-01-05 02:45:46,181 - root - INFO - [Train] Epoch 0 [23800/35182]: Training loss 0.5135084390640259, current lr 9.96645774707939e-05, contrastive loss 4.688957214355469, task loss 0.4666188955307007
2025-01-05 02:47:26,534 - root - INFO - [Train] Epoch 0 [23900/35182]: Training loss 0.5139915943145752, current lr 9.966315624911175e-05, contrastive loss 4.771602153778076, task loss 0.4662756025791168
2025-01-05 02:49:07,146 - root - INFO - [Train] Epoch 0 [24000/35182]: Training loss 0.5876892805099487, current lr 9.966173502742958e-05, contrastive loss 4.765817165374756, task loss 0.5400311350822449
2025-01-05 02:50:47,520 - root - INFO - [Train] Epoch 0 [24100/35182]: Training loss 0.5615354776382446, current lr 9.966031380574742e-05, contrastive loss 4.603522300720215, task loss 0.5155002474784851
2025-01-05 02:52:28,123 - root - INFO - [Train] Epoch 0 [24200/35182]: Training loss 0.5429026484489441, current lr 9.965889258406527e-05, contrastive loss 4.9123992919921875, task loss 0.49377864599227905
2025-01-05 02:54:08,482 - root - INFO - [Train] Epoch 0 [24300/35182]: Training loss 0.6043277382850647, current lr 9.965747136238311e-05, contrastive loss 4.927543640136719, task loss 0.5550522804260254
2025-01-05 02:55:49,096 - root - INFO - [Train] Epoch 0 [24400/35182]: Training loss 0.5272916555404663, current lr 9.965605014070095e-05, contrastive loss 4.86873722076416, task loss 0.4786042869091034
2025-01-05 02:57:29,467 - root - INFO - [Train] Epoch 0 [24500/35182]: Training loss 0.5414679050445557, current lr 9.965462891901878e-05, contrastive loss 4.8062920570373535, task loss 0.49340498447418213
2025-01-05 02:59:10,057 - root - INFO - [Train] Epoch 0 [24600/35182]: Training loss 0.5022650957107544, current lr 9.965320769733664e-05, contrastive loss 4.81053352355957, task loss 0.45415976643562317
2025-01-05 03:00:50,423 - root - INFO - [Train] Epoch 0 [24700/35182]: Training loss 0.6487262845039368, current lr 9.965178647565447e-05, contrastive loss 4.802079200744629, task loss 0.6007055044174194
2025-01-05 03:02:31,019 - root - INFO - [Train] Epoch 0 [24800/35182]: Training loss 0.574215829372406, current lr 9.965036525397233e-05, contrastive loss 4.81275749206543, task loss 0.5260882377624512
2025-01-05 03:04:11,376 - root - INFO - [Train] Epoch 0 [24900/35182]: Training loss 0.5259345173835754, current lr 9.964894403229016e-05, contrastive loss 4.790773391723633, task loss 0.47802677750587463
2025-01-05 03:05:51,990 - root - INFO - [Train] Epoch 0 [25000/35182]: Training loss 0.5824570059776306, current lr 9.9647522810608e-05, contrastive loss 4.743431091308594, task loss 0.5350226759910583
2025-01-05 03:07:32,356 - root - INFO - [Train] Epoch 0 [25100/35182]: Training loss 0.5654776096343994, current lr 9.964610158892584e-05, contrastive loss 4.837929725646973, task loss 0.5170983076095581
2025-01-05 03:09:12,963 - root - INFO - [Train] Epoch 0 [25200/35182]: Training loss 0.5563240051269531, current lr 9.964468036724369e-05, contrastive loss 4.809078693389893, task loss 0.5082331895828247
2025-01-05 03:10:53,330 - root - INFO - [Train] Epoch 0 [25300/35182]: Training loss 0.4942322373390198, current lr 9.964325914556153e-05, contrastive loss 4.794074058532715, task loss 0.4462915062904358
2025-01-05 03:12:33,925 - root - INFO - [Train] Epoch 0 [25400/35182]: Training loss 0.5127743482589722, current lr 9.964183792387936e-05, contrastive loss 4.776613235473633, task loss 0.4650082290172577
2025-01-05 03:14:14,287 - root - INFO - [Train] Epoch 0 [25500/35182]: Training loss 0.6201676726341248, current lr 9.964041670219722e-05, contrastive loss 4.811509132385254, task loss 0.5720525979995728
2025-01-05 03:15:54,888 - root - INFO - [Train] Epoch 0 [25600/35182]: Training loss 0.5272807478904724, current lr 9.963899548051505e-05, contrastive loss 4.61232852935791, task loss 0.48115748167037964
2025-01-05 03:17:35,257 - root - INFO - [Train] Epoch 0 [25700/35182]: Training loss 0.5850560069084167, current lr 9.96375742588329e-05, contrastive loss 4.8108110427856445, task loss 0.5369479060173035
2025-01-05 03:19:15,858 - root - INFO - [Train] Epoch 0 [25800/35182]: Training loss 0.5042129158973694, current lr 9.963615303715073e-05, contrastive loss 4.6763715744018555, task loss 0.4574492275714874
2025-01-05 03:20:56,230 - root - INFO - [Train] Epoch 0 [25900/35182]: Training loss 0.4725138545036316, current lr 9.963473181546859e-05, contrastive loss 4.822865009307861, task loss 0.4242852032184601
2025-01-05 03:22:36,826 - root - INFO - [Train] Epoch 0 [26000/35182]: Training loss 0.6161975264549255, current lr 9.963331059378642e-05, contrastive loss 4.761479377746582, task loss 0.5685827136039734
2025-01-05 03:24:17,191 - root - INFO - [Train] Epoch 0 [26100/35182]: Training loss 0.51263028383255, current lr 9.963188937210426e-05, contrastive loss 4.783428192138672, task loss 0.4647959768772125
2025-01-05 03:25:57,791 - root - INFO - [Train] Epoch 0 [26200/35182]: Training loss 0.48317044973373413, current lr 9.963046815042211e-05, contrastive loss 4.891020774841309, task loss 0.4342602491378784
2025-01-05 03:27:38,162 - root - INFO - [Train] Epoch 0 [26300/35182]: Training loss 0.5579218864440918, current lr 9.962904692873994e-05, contrastive loss 4.8925371170043945, task loss 0.5089964866638184
2025-01-05 03:29:18,524 - root - INFO - [Train] Epoch 0 [26400/35182]: Training loss 0.5416956543922424, current lr 9.96276257070578e-05, contrastive loss 4.777769088745117, task loss 0.49391797184944153
2025-01-05 03:30:59,124 - root - INFO - [Train] Epoch 0 [26500/35182]: Training loss 0.5274409651756287, current lr 9.962620448537562e-05, contrastive loss 4.696625709533691, task loss 0.48047471046447754
2025-01-05 03:32:39,488 - root - INFO - [Train] Epoch 0 [26600/35182]: Training loss 0.5644170641899109, current lr 9.962478326369348e-05, contrastive loss 4.857142448425293, task loss 0.5158456563949585
2025-01-05 03:34:20,089 - root - INFO - [Train] Epoch 0 [26700/35182]: Training loss 0.5427362322807312, current lr 9.962336204201131e-05, contrastive loss 4.628341197967529, task loss 0.49645283818244934
2025-01-05 03:36:00,455 - root - INFO - [Train] Epoch 0 [26800/35182]: Training loss 0.5399541258811951, current lr 9.962194082032917e-05, contrastive loss 4.799107551574707, task loss 0.4919630289077759
2025-01-05 03:37:41,059 - root - INFO - [Train] Epoch 0 [26900/35182]: Training loss 0.6359365582466125, current lr 9.9620519598647e-05, contrastive loss 4.818700790405273, task loss 0.5877495408058167
2025-01-05 03:39:21,417 - root - INFO - [Train] Epoch 0 [27000/35182]: Training loss 0.5390755534172058, current lr 9.961909837696484e-05, contrastive loss 4.583802700042725, task loss 0.49323752522468567
2025-01-05 03:41:02,027 - root - INFO - [Train] Epoch 0 [27100/35182]: Training loss 0.5404365658760071, current lr 9.961767715528268e-05, contrastive loss 4.568781852722168, task loss 0.4947487711906433
2025-01-05 03:42:42,388 - root - INFO - [Train] Epoch 0 [27200/35182]: Training loss 0.6039642691612244, current lr 9.961625593360053e-05, contrastive loss 4.598489761352539, task loss 0.5579793453216553
2025-01-05 03:44:22,998 - root - INFO - [Train] Epoch 0 [27300/35182]: Training loss 0.5678674578666687, current lr 9.961483471191837e-05, contrastive loss 4.678418159484863, task loss 0.5210832953453064
2025-01-05 03:46:03,368 - root - INFO - [Train] Epoch 0 [27400/35182]: Training loss 0.5533960461616516, current lr 9.96134134902362e-05, contrastive loss 4.659780502319336, task loss 0.506798267364502
2025-01-05 03:47:43,964 - root - INFO - [Train] Epoch 0 [27500/35182]: Training loss 0.46185892820358276, current lr 9.961199226855406e-05, contrastive loss 4.691688060760498, task loss 0.41494205594062805
2025-01-05 03:49:24,332 - root - INFO - [Train] Epoch 0 [27600/35182]: Training loss 0.5676724314689636, current lr 9.961057104687189e-05, contrastive loss 4.49273681640625, task loss 0.5227450728416443
2025-01-05 03:51:04,927 - root - INFO - [Train] Epoch 0 [27700/35182]: Training loss 0.5168196558952332, current lr 9.960914982518975e-05, contrastive loss 4.810810089111328, task loss 0.4687115550041199
2025-01-05 03:52:45,290 - root - INFO - [Train] Epoch 0 [27800/35182]: Training loss 0.5794714689254761, current lr 9.960772860350757e-05, contrastive loss 4.515096664428711, task loss 0.5343204736709595
2025-01-05 03:54:25,893 - root - INFO - [Train] Epoch 0 [27900/35182]: Training loss 0.47619593143463135, current lr 9.960630738182543e-05, contrastive loss 4.627877235412598, task loss 0.4299171566963196
2025-01-05 03:56:06,268 - root - INFO - [Train] Epoch 0 [28000/35182]: Training loss 0.4805954396724701, current lr 9.960488616014326e-05, contrastive loss 4.583166122436523, task loss 0.4347637891769409
2025-01-05 03:57:46,864 - root - INFO - [Train] Epoch 0 [28100/35182]: Training loss 0.5417662858963013, current lr 9.96034649384611e-05, contrastive loss 4.635878562927246, task loss 0.4954075217247009
2025-01-05 03:59:27,230 - root - INFO - [Train] Epoch 0 [28200/35182]: Training loss 0.5542809367179871, current lr 9.960204371677895e-05, contrastive loss 4.7433929443359375, task loss 0.5068470239639282
2025-01-05 04:01:07,829 - root - INFO - [Train] Epoch 0 [28300/35182]: Training loss 0.486366868019104, current lr 9.960062249509678e-05, contrastive loss 5.666773796081543, task loss 0.4296991229057312
2025-01-05 04:02:48,197 - root - INFO - [Train] Epoch 0 [28400/35182]: Training loss 0.5701027512550354, current lr 9.959920127341464e-05, contrastive loss 4.751638889312744, task loss 0.5225863456726074
2025-01-05 04:04:28,795 - root - INFO - [Train] Epoch 0 [28500/35182]: Training loss 0.4584319591522217, current lr 9.959778005173247e-05, contrastive loss 4.60578727722168, task loss 0.4123740792274475
2025-01-05 04:06:09,168 - root - INFO - [Train] Epoch 0 [28600/35182]: Training loss 0.6371623277664185, current lr 9.959635883005032e-05, contrastive loss 4.761536598205566, task loss 0.5895469784736633
2025-01-05 04:07:49,521 - root - INFO - [Train] Epoch 0 [28700/35182]: Training loss 0.5360363125801086, current lr 9.959493760836815e-05, contrastive loss 4.741123199462891, task loss 0.48862507939338684
2025-01-05 04:09:30,127 - root - INFO - [Train] Epoch 0 [28800/35182]: Training loss 0.6164473295211792, current lr 9.959351638668601e-05, contrastive loss 4.528928756713867, task loss 0.5711580514907837
2025-01-05 04:11:10,487 - root - INFO - [Train] Epoch 0 [28900/35182]: Training loss 0.5914700627326965, current lr 9.959209516500384e-05, contrastive loss 4.585345268249512, task loss 0.545616626739502
2025-01-05 04:12:51,090 - root - INFO - [Train] Epoch 0 [29000/35182]: Training loss 0.5194132328033447, current lr 9.959067394332168e-05, contrastive loss 4.620279312133789, task loss 0.4732104539871216
2025-01-05 04:14:31,460 - root - INFO - [Train] Epoch 0 [29100/35182]: Training loss 0.5587202906608582, current lr 9.958925272163953e-05, contrastive loss 4.655016899108887, task loss 0.512170135974884
2025-01-05 04:16:12,062 - root - INFO - [Train] Epoch 0 [29200/35182]: Training loss 0.5863446593284607, current lr 9.958783149995737e-05, contrastive loss 5.029839038848877, task loss 0.5360462665557861
2025-01-05 04:17:52,427 - root - INFO - [Train] Epoch 0 [29300/35182]: Training loss 0.522482693195343, current lr 9.958641027827521e-05, contrastive loss 4.726529121398926, task loss 0.47521740198135376
2025-01-05 04:19:33,036 - root - INFO - [Train] Epoch 0 [29400/35182]: Training loss 0.6011223793029785, current lr 9.958498905659304e-05, contrastive loss 4.63095760345459, task loss 0.5548127889633179
2025-01-05 04:21:13,409 - root - INFO - [Train] Epoch 0 [29500/35182]: Training loss 0.49865975975990295, current lr 9.95835678349109e-05, contrastive loss 4.598852157592773, task loss 0.45267122983932495
2025-01-05 04:22:54,001 - root - INFO - [Train] Epoch 0 [29600/35182]: Training loss 0.6198355555534363, current lr 9.958214661322873e-05, contrastive loss 4.70638370513916, task loss 0.5727717280387878
2025-01-05 04:24:34,366 - root - INFO - [Train] Epoch 0 [29700/35182]: Training loss 0.5721896886825562, current lr 9.958072539154659e-05, contrastive loss 4.463591575622559, task loss 0.5275537967681885
2025-01-05 04:26:14,962 - root - INFO - [Train] Epoch 0 [29800/35182]: Training loss 0.5598495006561279, current lr 9.957930416986442e-05, contrastive loss 4.586646556854248, task loss 0.5139830112457275
2025-01-05 04:27:55,325 - root - INFO - [Train] Epoch 0 [29900/35182]: Training loss 0.6172776222229004, current lr 9.957788294818227e-05, contrastive loss 4.529502868652344, task loss 0.5719826221466064
2025-01-05 04:29:35,933 - root - INFO - [Train] Epoch 0 [30000/35182]: Training loss 0.5267714858055115, current lr 9.95764617265001e-05, contrastive loss 9.263439178466797, task loss 0.43413710594177246
2025-01-05 04:31:16,287 - root - INFO - [Train] Epoch 0 [30100/35182]: Training loss 0.5498074293136597, current lr 9.957504050481795e-05, contrastive loss 4.892632007598877, task loss 0.5008811354637146
2025-01-05 04:32:56,895 - root - INFO - [Train] Epoch 0 [30200/35182]: Training loss 0.5492492318153381, current lr 9.957361928313579e-05, contrastive loss 4.82869815826416, task loss 0.5009622573852539
2025-01-05 04:34:37,264 - root - INFO - [Train] Epoch 0 [30300/35182]: Training loss 0.5110998749732971, current lr 9.957219806145362e-05, contrastive loss 4.779573917388916, task loss 0.4633041322231293
2025-01-05 04:36:17,867 - root - INFO - [Train] Epoch 0 [30400/35182]: Training loss 0.562798261642456, current lr 9.957077683977148e-05, contrastive loss 4.669008731842041, task loss 0.5161081552505493
2025-01-05 04:37:58,236 - root - INFO - [Train] Epoch 0 [30500/35182]: Training loss 0.5694488286972046, current lr 9.95693556180893e-05, contrastive loss 4.735934257507324, task loss 0.5220894813537598
2025-01-05 04:39:38,835 - root - INFO - [Train] Epoch 0 [30600/35182]: Training loss 0.5755543112754822, current lr 9.956793439640716e-05, contrastive loss 4.769291877746582, task loss 0.5278614163398743
2025-01-05 04:41:19,204 - root - INFO - [Train] Epoch 0 [30700/35182]: Training loss 0.5738497972488403, current lr 9.956651317472499e-05, contrastive loss 4.657193183898926, task loss 0.5272778868675232
2025-01-05 04:42:59,802 - root - INFO - [Train] Epoch 0 [30800/35182]: Training loss 0.5325959324836731, current lr 9.956509195304285e-05, contrastive loss 4.693414688110352, task loss 0.4856618046760559
2025-01-05 04:44:40,177 - root - INFO - [Train] Epoch 0 [30900/35182]: Training loss 0.5058115720748901, current lr 9.956367073136068e-05, contrastive loss 4.6234822273254395, task loss 0.4595767557621002
2025-01-05 04:46:20,543 - root - INFO - [Train] Epoch 0 [31000/35182]: Training loss 0.5962736010551453, current lr 9.956224950967852e-05, contrastive loss 4.678591728210449, task loss 0.5494877099990845
2025-01-05 04:48:01,135 - root - INFO - [Train] Epoch 0 [31100/35182]: Training loss 0.5582621097564697, current lr 9.956082828799637e-05, contrastive loss 4.64176082611084, task loss 0.5118445158004761
2025-01-05 04:49:41,505 - root - INFO - [Train] Epoch 0 [31200/35182]: Training loss 0.5121002197265625, current lr 9.955940706631421e-05, contrastive loss 4.675894737243652, task loss 0.4653412699699402
2025-01-05 04:51:22,111 - root - INFO - [Train] Epoch 0 [31300/35182]: Training loss 0.5893612504005432, current lr 9.955798584463205e-05, contrastive loss 4.725698471069336, task loss 0.5421042442321777
2025-01-05 04:53:02,483 - root - INFO - [Train] Epoch 0 [31400/35182]: Training loss 0.5548232197761536, current lr 9.955656462294988e-05, contrastive loss 4.622018814086914, task loss 0.508603036403656
2025-01-05 04:54:43,082 - root - INFO - [Train] Epoch 0 [31500/35182]: Training loss 0.6039285063743591, current lr 9.955514340126774e-05, contrastive loss 4.65425968170166, task loss 0.5573859214782715
2025-01-05 04:56:23,449 - root - INFO - [Train] Epoch 0 [31600/35182]: Training loss 0.5603955984115601, current lr 9.955372217958557e-05, contrastive loss 4.601141929626465, task loss 0.5143842101097107
2025-01-05 04:58:04,030 - root - INFO - [Train] Epoch 0 [31700/35182]: Training loss 0.5699599385261536, current lr 9.955230095790343e-05, contrastive loss 4.742002964019775, task loss 0.5225399136543274
2025-01-05 04:59:44,393 - root - INFO - [Train] Epoch 0 [31800/35182]: Training loss 0.5267050862312317, current lr 9.955087973622126e-05, contrastive loss 4.541176795959473, task loss 0.48129332065582275
2025-01-05 05:01:24,996 - root - INFO - [Train] Epoch 0 [31900/35182]: Training loss 0.5452145934104919, current lr 9.954945851453911e-05, contrastive loss 4.534445762634277, task loss 0.4998701512813568
2025-01-05 05:03:05,360 - root - INFO - [Train] Epoch 0 [32000/35182]: Training loss 0.5342788696289062, current lr 9.954803729285694e-05, contrastive loss 4.6799397468566895, task loss 0.48747944831848145
2025-01-05 05:04:45,972 - root - INFO - [Train] Epoch 0 [32100/35182]: Training loss 0.5273304581642151, current lr 9.954661607117479e-05, contrastive loss 4.659662246704102, task loss 0.48073384165763855
2025-01-05 05:06:26,336 - root - INFO - [Train] Epoch 0 [32200/35182]: Training loss 0.556285560131073, current lr 9.954519484949263e-05, contrastive loss 4.651965141296387, task loss 0.5097659230232239
2025-01-05 05:08:06,932 - root - INFO - [Train] Epoch 0 [32300/35182]: Training loss 0.5198655724525452, current lr 9.954377362781046e-05, contrastive loss 4.642338752746582, task loss 0.4734421968460083
2025-01-05 05:09:47,309 - root - INFO - [Train] Epoch 0 [32400/35182]: Training loss 0.5960643291473389, current lr 9.954235240612832e-05, contrastive loss 4.773936748504639, task loss 0.5483249425888062
2025-01-05 05:11:27,906 - root - INFO - [Train] Epoch 0 [32500/35182]: Training loss 0.5413092970848083, current lr 9.954093118444615e-05, contrastive loss 4.737388610839844, task loss 0.49393540620803833
2025-01-05 05:13:08,279 - root - INFO - [Train] Epoch 0 [32600/35182]: Training loss 0.5803572535514832, current lr 9.9539509962764e-05, contrastive loss 4.645709991455078, task loss 0.5339001417160034
2025-01-05 05:14:48,870 - root - INFO - [Train] Epoch 0 [32700/35182]: Training loss 0.5120958685874939, current lr 9.953808874108183e-05, contrastive loss 4.628029823303223, task loss 0.46581557393074036
2025-01-05 05:16:29,244 - root - INFO - [Train] Epoch 0 [32800/35182]: Training loss 0.5983669757843018, current lr 9.953666751939969e-05, contrastive loss 4.618579864501953, task loss 0.5521811842918396
2025-01-05 05:18:09,836 - root - INFO - [Train] Epoch 0 [32900/35182]: Training loss 0.5152886509895325, current lr 9.953524629771752e-05, contrastive loss 4.934030532836914, task loss 0.46594834327697754
2025-01-05 05:19:50,206 - root - INFO - [Train] Epoch 0 [33000/35182]: Training loss 0.5337526798248291, current lr 9.953382507603536e-05, contrastive loss 4.706170082092285, task loss 0.4866909682750702
2025-01-05 05:21:30,805 - root - INFO - [Train] Epoch 0 [33100/35182]: Training loss 0.512514591217041, current lr 9.95324038543532e-05, contrastive loss 4.661075115203857, task loss 0.4659038186073303
2025-01-05 05:23:11,171 - root - INFO - [Train] Epoch 0 [33200/35182]: Training loss 0.5026512145996094, current lr 9.953098263267105e-05, contrastive loss 4.636933326721191, task loss 0.4562819004058838
2025-01-05 05:24:51,534 - root - INFO - [Train] Epoch 0 [33300/35182]: Training loss 0.5679645538330078, current lr 9.95295614109889e-05, contrastive loss 4.777111053466797, task loss 0.5201934576034546
2025-01-05 05:26:32,140 - root - INFO - [Train] Epoch 0 [33400/35182]: Training loss 0.48527735471725464, current lr 9.952814018930672e-05, contrastive loss 4.618977069854736, task loss 0.43908756971359253
2025-01-05 05:28:12,496 - root - INFO - [Train] Epoch 0 [33500/35182]: Training loss 0.5094016790390015, current lr 9.952671896762458e-05, contrastive loss 4.679525375366211, task loss 0.46260640025138855
2025-01-05 05:29:53,099 - root - INFO - [Train] Epoch 0 [33600/35182]: Training loss 0.5359832644462585, current lr 9.952529774594241e-05, contrastive loss 4.546209335327148, task loss 0.4905211925506592
2025-01-05 05:31:33,457 - root - INFO - [Train] Epoch 0 [33700/35182]: Training loss 0.6092793941497803, current lr 9.952387652426027e-05, contrastive loss 4.691190719604492, task loss 0.5623674988746643
2025-01-05 05:33:14,069 - root - INFO - [Train] Epoch 0 [33800/35182]: Training loss 0.556178629398346, current lr 9.95224553025781e-05, contrastive loss 4.749610900878906, task loss 0.5086825489997864
2025-01-05 05:34:54,439 - root - INFO - [Train] Epoch 0 [33900/35182]: Training loss 0.46442726254463196, current lr 9.952103408089595e-05, contrastive loss 4.662522792816162, task loss 0.41780203580856323
2025-01-05 05:36:35,043 - root - INFO - [Train] Epoch 0 [34000/35182]: Training loss 0.5143305063247681, current lr 9.951961285921378e-05, contrastive loss 5.418526649475098, task loss 0.4601452648639679
2025-01-05 05:38:15,406 - root - INFO - [Train] Epoch 0 [34100/35182]: Training loss 0.5345079898834229, current lr 9.951819163753163e-05, contrastive loss 4.920798301696777, task loss 0.4853000342845917
2025-01-05 05:39:56,005 - root - INFO - [Train] Epoch 0 [34200/35182]: Training loss 0.5375730395317078, current lr 9.951677041584947e-05, contrastive loss 5.043032646179199, task loss 0.48714274168014526
2025-01-05 05:41:36,371 - root - INFO - [Train] Epoch 0 [34300/35182]: Training loss 0.5586578845977783, current lr 9.95153491941673e-05, contrastive loss 5.166521072387695, task loss 0.5069926977157593
2025-01-05 05:43:16,976 - root - INFO - [Train] Epoch 0 [34400/35182]: Training loss 0.5878700613975525, current lr 9.951392797248516e-05, contrastive loss 4.6253862380981445, task loss 0.5416162014007568
2025-01-05 05:44:57,330 - root - INFO - [Train] Epoch 0 [34500/35182]: Training loss 0.6388491988182068, current lr 9.951250675080299e-05, contrastive loss 4.897836208343506, task loss 0.589870810508728
2025-01-05 05:46:37,944 - root - INFO - [Train] Epoch 0 [34600/35182]: Training loss 0.5541591644287109, current lr 9.951108552912084e-05, contrastive loss 5.2819132804870605, task loss 0.5013400316238403
2025-01-05 05:48:18,295 - root - INFO - [Train] Epoch 0 [34700/35182]: Training loss 0.5083010792732239, current lr 9.950966430743867e-05, contrastive loss 4.806909561157227, task loss 0.4602319598197937
2025-01-05 05:49:58,908 - root - INFO - [Train] Epoch 0 [34800/35182]: Training loss 0.5679522156715393, current lr 9.950824308575653e-05, contrastive loss 4.722982406616211, task loss 0.5207223892211914
2025-01-05 05:51:39,265 - root - INFO - [Train] Epoch 0 [34900/35182]: Training loss 0.5762414336204529, current lr 9.950682186407436e-05, contrastive loss 4.829681396484375, task loss 0.5279446244239807
2025-01-05 05:53:19,878 - root - INFO - [Train] Epoch 0 [35000/35182]: Training loss 0.5627756118774414, current lr 9.95054006423922e-05, contrastive loss 4.804732322692871, task loss 0.514728307723999
2025-01-05 05:55:00,240 - root - INFO - [Train] Epoch 0 [35100/35182]: Training loss 0.5825479030609131, current lr 9.950397942071005e-05, contrastive loss 4.762928009033203, task loss 0.5349186062812805
2025-01-05 05:56:24,230 - root - INFO - [Train] Epoch 0: Overall Training loss 0.0
2025-01-05 05:56:24,231 - root - INFO - [Time] Epoch 0: Time taken 9.82 hours
2025-01-05 05:56:29,300 - root - INFO - [Train] Epoch 1 [0/35182]: Training loss 0.5499151349067688, current lr 9.950281401893068e-05, contrastive loss 4.731222152709961, task loss 0.5026029348373413
2025-01-05 05:58:09,953 - root - INFO - [Train] Epoch 1 [100/35182]: Training loss 0.563226580619812, current lr 9.950139279724851e-05, contrastive loss 4.840546607971191, task loss 0.5148211121559143
2025-01-05 05:59:50,325 - root - INFO - [Train] Epoch 1 [200/35182]: Training loss 0.5632113814353943, current lr 9.949997157556637e-05, contrastive loss 4.943935394287109, task loss 0.5137720108032227
2025-01-05 06:01:30,919 - root - INFO - [Train] Epoch 1 [300/35182]: Training loss 0.5545589923858643, current lr 9.94985503538842e-05, contrastive loss 4.827688217163086, task loss 0.5062820911407471
2025-01-05 06:03:11,296 - root - INFO - [Train] Epoch 1 [400/35182]: Training loss 0.5412023663520813, current lr 9.949712913220206e-05, contrastive loss 4.823993682861328, task loss 0.49296244978904724
2025-01-05 06:04:51,884 - root - INFO - [Train] Epoch 1 [500/35182]: Training loss 0.5138714909553528, current lr 9.949570791051989e-05, contrastive loss 4.848423957824707, task loss 0.4653872549533844
2025-01-05 06:06:32,261 - root - INFO - [Train] Epoch 1 [600/35182]: Training loss 0.5864996314048767, current lr 9.949428668883773e-05, contrastive loss 4.67573356628418, task loss 0.5397422909736633
2025-01-05 06:08:12,851 - root - INFO - [Train] Epoch 1 [700/35182]: Training loss 0.5310128331184387, current lr 9.949286546715557e-05, contrastive loss 4.655673027038574, task loss 0.4844560921192169
2025-01-05 06:09:53,218 - root - INFO - [Train] Epoch 1 [800/35182]: Training loss 0.6100540161132812, current lr 9.94914442454734e-05, contrastive loss 4.77326774597168, task loss 0.5623213648796082
2025-01-05 06:11:33,817 - root - INFO - [Train] Epoch 1 [900/35182]: Training loss 0.5621689558029175, current lr 9.949002302379126e-05, contrastive loss 4.727984428405762, task loss 0.514889121055603
2025-01-05 06:13:14,190 - root - INFO - [Train] Epoch 1 [1000/35182]: Training loss 0.5924153327941895, current lr 9.948860180210909e-05, contrastive loss 4.771542072296143, task loss 0.5446999073028564
2025-01-05 06:14:54,571 - root - INFO - [Train] Epoch 1 [1100/35182]: Training loss 0.4830394685268402, current lr 9.948718058042695e-05, contrastive loss 4.798022747039795, task loss 0.43505924940109253
2025-01-05 06:16:35,154 - root - INFO - [Train] Epoch 1 [1200/35182]: Training loss 0.5429277420043945, current lr 9.948575935874478e-05, contrastive loss 4.856313705444336, task loss 0.49436458945274353
2025-01-05 06:18:15,525 - root - INFO - [Train] Epoch 1 [1300/35182]: Training loss 0.541211724281311, current lr 9.948433813706263e-05, contrastive loss 4.741774559020996, task loss 0.4937939941883087
2025-01-05 06:19:56,142 - root - INFO - [Train] Epoch 1 [1400/35182]: Training loss 0.5637528300285339, current lr 9.948291691538046e-05, contrastive loss 4.709712982177734, task loss 0.516655683517456
2025-01-05 06:21:36,519 - root - INFO - [Train] Epoch 1 [1500/35182]: Training loss 0.5770539045333862, current lr 9.94814956936983e-05, contrastive loss 4.6388163566589355, task loss 0.5306657552719116
2025-01-05 06:23:17,092 - root - INFO - [Train] Epoch 1 [1600/35182]: Training loss 0.5555589199066162, current lr 9.948007447201615e-05, contrastive loss 4.838352203369141, task loss 0.5071753859519958
2025-01-05 06:24:57,469 - root - INFO - [Train] Epoch 1 [1700/35182]: Training loss 0.47295522689819336, current lr 9.947865325033399e-05, contrastive loss 4.62531042098999, task loss 0.4267021119594574
2025-01-05 06:26:38,078 - root - INFO - [Train] Epoch 1 [1800/35182]: Training loss 0.5470691323280334, current lr 9.947723202865184e-05, contrastive loss 4.59283971786499, task loss 0.5011407136917114
2025-01-05 06:28:18,454 - root - INFO - [Train] Epoch 1 [1900/35182]: Training loss 0.5223906636238098, current lr 9.947581080696967e-05, contrastive loss 4.897243976593018, task loss 0.4734182059764862
2025-01-05 06:29:59,040 - root - INFO - [Train] Epoch 1 [2000/35182]: Training loss 0.6135963797569275, current lr 9.947438958528752e-05, contrastive loss 4.938959121704102, task loss 0.5642067790031433
2025-01-05 06:31:39,432 - root - INFO - [Train] Epoch 1 [2100/35182]: Training loss 0.4763285219669342, current lr 9.947296836360535e-05, contrastive loss 4.845123767852783, task loss 0.427877277135849
2025-01-05 06:33:19,996 - root - INFO - [Train] Epoch 1 [2200/35182]: Training loss 0.5601565837860107, current lr 9.947154714192321e-05, contrastive loss 4.7017621994018555, task loss 0.5131389498710632
2025-01-05 06:35:00,384 - root - INFO - [Train] Epoch 1 [2300/35182]: Training loss 0.5289797186851501, current lr 9.947012592024104e-05, contrastive loss 4.844907760620117, task loss 0.48053064942359924
2025-01-05 06:36:40,963 - root - INFO - [Train] Epoch 1 [2400/35182]: Training loss 0.4677640199661255, current lr 9.94687046985589e-05, contrastive loss 4.626420497894287, task loss 0.4214998185634613
2025-01-05 06:38:21,348 - root - INFO - [Train] Epoch 1 [2500/35182]: Training loss 0.6392019987106323, current lr 9.946728347687673e-05, contrastive loss 4.734952926635742, task loss 0.5918524861335754
2025-01-05 06:40:01,928 - root - INFO - [Train] Epoch 1 [2600/35182]: Training loss 0.5230817794799805, current lr 9.946586225519457e-05, contrastive loss 4.813858985900879, task loss 0.4749431610107422
2025-01-05 06:41:42,315 - root - INFO - [Train] Epoch 1 [2700/35182]: Training loss 0.6487218737602234, current lr 9.946444103351241e-05, contrastive loss 4.757236480712891, task loss 0.6011494994163513
2025-01-05 06:43:22,894 - root - INFO - [Train] Epoch 1 [2800/35182]: Training loss 0.5061061382293701, current lr 9.946301981183024e-05, contrastive loss 4.689163684844971, task loss 0.4592145085334778
2025-01-05 06:45:03,280 - root - INFO - [Train] Epoch 1 [2900/35182]: Training loss 0.5477738380432129, current lr 9.94615985901481e-05, contrastive loss 4.657182693481445, task loss 0.5012019872665405
2025-01-05 06:46:43,856 - root - INFO - [Train] Epoch 1 [3000/35182]: Training loss 0.5813727974891663, current lr 9.946017736846593e-05, contrastive loss 4.680383682250977, task loss 0.5345689654350281
2025-01-05 06:48:24,244 - root - INFO - [Train] Epoch 1 [3100/35182]: Training loss 0.5545793175697327, current lr 9.945875614678379e-05, contrastive loss 4.600310802459717, task loss 0.5085762143135071
2025-01-05 06:50:04,825 - root - INFO - [Train] Epoch 1 [3200/35182]: Training loss 0.5336939096450806, current lr 9.945733492510162e-05, contrastive loss 4.731685638427734, task loss 0.4863770604133606
2025-01-05 06:51:45,207 - root - INFO - [Train] Epoch 1 [3300/35182]: Training loss 0.5503381490707397, current lr 9.945591370341947e-05, contrastive loss 4.848988056182861, task loss 0.5018482804298401
2025-01-05 06:53:25,579 - root - INFO - [Train] Epoch 1 [3400/35182]: Training loss 0.5568268299102783, current lr 9.94544924817373e-05, contrastive loss 4.856667518615723, task loss 0.5082601308822632
2025-01-05 06:55:06,165 - root - INFO - [Train] Epoch 1 [3500/35182]: Training loss 0.49683672189712524, current lr 9.945307126005515e-05, contrastive loss 4.712371826171875, task loss 0.44971299171447754
2025-01-05 06:56:46,526 - root - INFO - [Train] Epoch 1 [3600/35182]: Training loss 0.49241045117378235, current lr 9.945165003837299e-05, contrastive loss 4.655327796936035, task loss 0.4458571672439575
2025-01-05 06:58:27,126 - root - INFO - [Train] Epoch 1 [3700/35182]: Training loss 0.4774569272994995, current lr 9.945022881669083e-05, contrastive loss 4.790378570556641, task loss 0.42955315113067627
2025-01-05 07:00:07,496 - root - INFO - [Train] Epoch 1 [3800/35182]: Training loss 0.5270533561706543, current lr 9.944880759500868e-05, contrastive loss 4.683858871459961, task loss 0.48021477460861206
2025-01-05 07:01:48,103 - root - INFO - [Train] Epoch 1 [3900/35182]: Training loss 0.5981249809265137, current lr 9.94473863733265e-05, contrastive loss 4.817680835723877, task loss 0.5499481558799744
2025-01-05 07:03:28,471 - root - INFO - [Train] Epoch 1 [4000/35182]: Training loss 0.6176822185516357, current lr 9.944596515164436e-05, contrastive loss 4.655851364135742, task loss 0.5711237192153931
2025-01-05 07:05:09,061 - root - INFO - [Train] Epoch 1 [4100/35182]: Training loss 0.5928112864494324, current lr 9.944454392996219e-05, contrastive loss 4.599123001098633, task loss 0.5468200445175171
2025-01-05 07:06:49,432 - root - INFO - [Train] Epoch 1 [4200/35182]: Training loss 0.6083507537841797, current lr 9.944312270828005e-05, contrastive loss 4.7963762283325195, task loss 0.5603870153427124
2025-01-05 07:08:30,030 - root - INFO - [Train] Epoch 1 [4300/35182]: Training loss 0.5610091090202332, current lr 9.944170148659788e-05, contrastive loss 4.622318267822266, task loss 0.5147859454154968
2025-01-05 07:10:10,409 - root - INFO - [Train] Epoch 1 [4400/35182]: Training loss 0.553369402885437, current lr 9.944028026491574e-05, contrastive loss 4.542473316192627, task loss 0.507944643497467
2025-01-05 07:11:50,990 - root - INFO - [Train] Epoch 1 [4500/35182]: Training loss 0.46894344687461853, current lr 9.943885904323357e-05, contrastive loss 4.637851715087891, task loss 0.42256492376327515
2025-01-05 07:13:31,361 - root - INFO - [Train] Epoch 1 [4600/35182]: Training loss 0.5110004544258118, current lr 9.943743782155141e-05, contrastive loss 4.586156845092773, task loss 0.46513888239860535
2025-01-05 07:15:11,963 - root - INFO - [Train] Epoch 1 [4700/35182]: Training loss 0.5779181718826294, current lr 9.943601659986925e-05, contrastive loss 4.513264179229736, task loss 0.5327855348587036
2025-01-05 07:16:52,345 - root - INFO - [Train] Epoch 1 [4800/35182]: Training loss 0.5469793081283569, current lr 9.943459537818708e-05, contrastive loss 4.839542388916016, task loss 0.49858391284942627
2025-01-05 07:18:32,925 - root - INFO - [Train] Epoch 1 [4900/35182]: Training loss 0.505259096622467, current lr 9.943317415650494e-05, contrastive loss 4.703440189361572, task loss 0.45822468400001526
2025-01-05 07:20:13,295 - root - INFO - [Train] Epoch 1 [5000/35182]: Training loss 0.4694242477416992, current lr 9.943175293482277e-05, contrastive loss 4.526136875152588, task loss 0.4241628646850586
2025-01-05 07:21:53,887 - root - INFO - [Train] Epoch 1 [5100/35182]: Training loss 0.5873216390609741, current lr 9.943033171314063e-05, contrastive loss 4.679404258728027, task loss 0.5405275821685791
2025-01-05 07:23:34,268 - root - INFO - [Train] Epoch 1 [5200/35182]: Training loss 0.42997151613235474, current lr 9.942891049145846e-05, contrastive loss 4.621298789978027, task loss 0.383758544921875
2025-01-05 07:25:14,856 - root - INFO - [Train] Epoch 1 [5300/35182]: Training loss 0.535267174243927, current lr 9.942748926977631e-05, contrastive loss 4.643424034118652, task loss 0.48883292078971863
2025-01-05 07:26:55,243 - root - INFO - [Train] Epoch 1 [5400/35182]: Training loss 0.524883508682251, current lr 9.942606804809414e-05, contrastive loss 4.538649559020996, task loss 0.4794970452785492
2025-01-05 07:28:35,827 - root - INFO - [Train] Epoch 1 [5500/35182]: Training loss 0.5530524849891663, current lr 9.942464682641199e-05, contrastive loss 4.447573661804199, task loss 0.5085767507553101
2025-01-05 07:30:16,207 - root - INFO - [Train] Epoch 1 [5600/35182]: Training loss 0.5845190286636353, current lr 9.942322560472983e-05, contrastive loss 4.378457069396973, task loss 0.5407344698905945
2025-01-05 07:31:56,580 - root - INFO - [Train] Epoch 1 [5700/35182]: Training loss 0.5866735577583313, current lr 9.942180438304767e-05, contrastive loss 4.363347053527832, task loss 0.5430400967597961
2025-01-05 07:33:37,181 - root - INFO - [Train] Epoch 1 [5800/35182]: Training loss 0.48763182759284973, current lr 9.942038316136552e-05, contrastive loss 4.413334846496582, task loss 0.44349849224090576
2025-01-05 07:35:17,575 - root - INFO - [Train] Epoch 1 [5900/35182]: Training loss 0.5684720873832703, current lr 9.941896193968335e-05, contrastive loss 4.274299621582031, task loss 0.5257291197776794
2025-01-05 07:36:58,128 - root - INFO - [Train] Epoch 1 [6000/35182]: Training loss 0.6325917840003967, current lr 9.94175407180012e-05, contrastive loss 4.21258020401001, task loss 0.5904659628868103
2025-01-05 07:38:38,503 - root - INFO - [Train] Epoch 1 [6100/35182]: Training loss 0.5003865957260132, current lr 9.941611949631903e-05, contrastive loss 4.179320335388184, task loss 0.4585933983325958
2025-01-05 07:40:19,095 - root - INFO - [Train] Epoch 1 [6200/35182]: Training loss 0.5155593156814575, current lr 9.941469827463689e-05, contrastive loss 4.239902496337891, task loss 0.4731602966785431
2025-01-05 07:41:59,462 - root - INFO - [Train] Epoch 1 [6300/35182]: Training loss 0.5769546627998352, current lr 9.941327705295472e-05, contrastive loss 4.287334442138672, task loss 0.5340813398361206
2025-01-05 07:43:40,069 - root - INFO - [Train] Epoch 1 [6400/35182]: Training loss 0.5354774594306946, current lr 9.941185583127258e-05, contrastive loss 4.213018417358398, task loss 0.49334725737571716
2025-01-05 07:45:20,449 - root - INFO - [Train] Epoch 1 [6500/35182]: Training loss 0.5156049132347107, current lr 9.941043460959041e-05, contrastive loss 4.266871452331543, task loss 0.47293621301651
2025-01-05 07:47:01,050 - root - INFO - [Train] Epoch 1 [6600/35182]: Training loss 0.564411461353302, current lr 9.940901338790825e-05, contrastive loss 4.224821090698242, task loss 0.5221632719039917
2025-01-05 07:48:41,438 - root - INFO - [Train] Epoch 1 [6700/35182]: Training loss 0.5947226881980896, current lr 9.94075921662261e-05, contrastive loss 4.148367881774902, task loss 0.5532389879226685
2025-01-05 07:50:22,019 - root - INFO - [Train] Epoch 1 [6800/35182]: Training loss 0.5414496660232544, current lr 9.940617094454392e-05, contrastive loss 4.163261413574219, task loss 0.49981704354286194
2025-01-05 07:52:02,408 - root - INFO - [Train] Epoch 1 [6900/35182]: Training loss 0.5292052626609802, current lr 9.940474972286178e-05, contrastive loss 4.3606977462768555, task loss 0.48559829592704773
2025-01-05 07:53:42,964 - root - INFO - [Train] Epoch 1 [7000/35182]: Training loss 0.5676692128181458, current lr 9.940332850117961e-05, contrastive loss 4.0156731605529785, task loss 0.5275124907493591
2025-01-05 07:55:23,362 - root - INFO - [Train] Epoch 1 [7100/35182]: Training loss 0.4811246991157532, current lr 9.940190727949747e-05, contrastive loss 4.048414707183838, task loss 0.44064056873321533
2025-01-05 07:57:03,944 - root - INFO - [Train] Epoch 1 [7200/35182]: Training loss 0.5939075946807861, current lr 9.94004860578153e-05, contrastive loss 4.103580474853516, task loss 0.5528717637062073
2025-01-05 07:58:44,320 - root - INFO - [Train] Epoch 1 [7300/35182]: Training loss 0.5152271389961243, current lr 9.939906483613315e-05, contrastive loss 4.041324615478516, task loss 0.47481387853622437
2025-01-05 08:00:24,898 - root - INFO - [Train] Epoch 1 [7400/35182]: Training loss 0.5705764293670654, current lr 9.939764361445098e-05, contrastive loss 4.241447448730469, task loss 0.5281619429588318
2025-01-05 08:02:05,295 - root - INFO - [Train] Epoch 1 [7500/35182]: Training loss 0.5406244397163391, current lr 9.939622239276883e-05, contrastive loss 4.274965286254883, task loss 0.49787476658821106
2025-01-05 08:03:45,865 - root - INFO - [Train] Epoch 1 [7600/35182]: Training loss 0.6336145401000977, current lr 9.939480117108667e-05, contrastive loss 4.020737648010254, task loss 0.593407154083252
2025-01-05 08:05:26,242 - root - INFO - [Train] Epoch 1 [7700/35182]: Training loss 0.5881214141845703, current lr 9.939337994940451e-05, contrastive loss 4.148855209350586, task loss 0.5466328859329224
2025-01-05 08:07:06,832 - root - INFO - [Train] Epoch 1 [7800/35182]: Training loss 0.4901539385318756, current lr 9.939195872772236e-05, contrastive loss 4.163754463195801, task loss 0.4485163986682892
2025-01-05 08:08:47,232 - root - INFO - [Train] Epoch 1 [7900/35182]: Training loss 0.5343904495239258, current lr 9.939053750604019e-05, contrastive loss 3.9479713439941406, task loss 0.49491074681282043
2025-01-05 08:10:27,797 - root - INFO - [Train] Epoch 1 [8000/35182]: Training loss 0.5397604703903198, current lr 9.938911628435804e-05, contrastive loss 4.191617965698242, task loss 0.49784430861473083
2025-01-05 08:12:08,203 - root - INFO - [Train] Epoch 1 [8100/35182]: Training loss 0.5088809132575989, current lr 9.938769506267587e-05, contrastive loss 4.207804203033447, task loss 0.4668028950691223
2025-01-05 08:13:48,867 - root - INFO - [Train] Epoch 1 [8200/35182]: Training loss 0.47341638803482056, current lr 9.938627384099373e-05, contrastive loss 4.298242568969727, task loss 0.4304339587688446
2025-01-05 08:15:29,236 - root - INFO - [Train] Epoch 1 [8300/35182]: Training loss 0.5222989916801453, current lr 9.938485261931156e-05, contrastive loss 4.093677043914795, task loss 0.4813622236251831
2025-01-05 08:17:09,837 - root - INFO - [Train] Epoch 1 [8400/35182]: Training loss 0.5327227115631104, current lr 9.938343139762942e-05, contrastive loss 4.0614471435546875, task loss 0.4921082556247711
2025-01-05 08:18:50,208 - root - INFO - [Train] Epoch 1 [8500/35182]: Training loss 0.5034884810447693, current lr 9.938201017594725e-05, contrastive loss 4.063806533813477, task loss 0.4628503918647766
2025-01-05 08:20:30,807 - root - INFO - [Train] Epoch 1 [8600/35182]: Training loss 0.548771858215332, current lr 9.938058895426509e-05, contrastive loss 4.095678806304932, task loss 0.5078150629997253
2025-01-05 08:22:11,178 - root - INFO - [Train] Epoch 1 [8700/35182]: Training loss 0.5303524136543274, current lr 9.937916773258293e-05, contrastive loss 4.135711669921875, task loss 0.4889952838420868
2025-01-05 08:23:51,547 - root - INFO - [Train] Epoch 1 [8800/35182]: Training loss 0.5131062269210815, current lr 9.937774651090076e-05, contrastive loss 4.118930816650391, task loss 0.47191691398620605
2025-01-05 08:25:32,157 - root - INFO - [Train] Epoch 1 [8900/35182]: Training loss 0.495258629322052, current lr 9.937632528921862e-05, contrastive loss 4.080533027648926, task loss 0.4544532895088196
2025-01-05 08:27:12,534 - root - INFO - [Train] Epoch 1 [9000/35182]: Training loss 0.5360634922981262, current lr 9.937490406753645e-05, contrastive loss 4.060180187225342, task loss 0.49546170234680176
2025-01-05 08:28:53,106 - root - INFO - [Train] Epoch 1 [9100/35182]: Training loss 0.5002253651618958, current lr 9.937348284585431e-05, contrastive loss 4.143582344055176, task loss 0.45878952741622925
2025-01-05 08:30:33,479 - root - INFO - [Train] Epoch 1 [9200/35182]: Training loss 0.5474777221679688, current lr 9.937206162417214e-05, contrastive loss 3.9095935821533203, task loss 0.5083817839622498
2025-01-05 08:32:14,077 - root - INFO - [Train] Epoch 1 [9300/35182]: Training loss 0.5764471292495728, current lr 9.937064040249e-05, contrastive loss 3.9474658966064453, task loss 0.5369724631309509
2025-01-05 08:33:54,441 - root - INFO - [Train] Epoch 1 [9400/35182]: Training loss 0.5112957954406738, current lr 9.936921918080782e-05, contrastive loss 3.9127466678619385, task loss 0.47216832637786865
2025-01-05 08:35:35,049 - root - INFO - [Train] Epoch 1 [9500/35182]: Training loss 0.5051412582397461, current lr 9.936779795912567e-05, contrastive loss 4.1630377769470215, task loss 0.4635109007358551
2025-01-05 08:37:15,432 - root - INFO - [Train] Epoch 1 [9600/35182]: Training loss 0.5338725447654724, current lr 9.936637673744351e-05, contrastive loss 3.8048667907714844, task loss 0.4958238899707794
2025-01-05 08:38:56,013 - root - INFO - [Train] Epoch 1 [9700/35182]: Training loss 0.5310000777244568, current lr 9.936495551576136e-05, contrastive loss 4.026393890380859, task loss 0.4907361567020416
2025-01-05 08:40:36,393 - root - INFO - [Train] Epoch 1 [9800/35182]: Training loss 0.5091325044631958, current lr 9.93635342940792e-05, contrastive loss 3.994762420654297, task loss 0.46918490529060364
2025-01-05 08:42:16,980 - root - INFO - [Train] Epoch 1 [9900/35182]: Training loss 0.5473578572273254, current lr 9.936211307239703e-05, contrastive loss 3.999934673309326, task loss 0.5073584914207458
2025-01-05 08:43:57,356 - root - INFO - [Train] Epoch 1 [10000/35182]: Training loss 0.44973134994506836, current lr 9.936069185071489e-05, contrastive loss 4.15277099609375, task loss 0.4082036316394806
2025-01-05 08:45:37,949 - root - INFO - [Train] Epoch 1 [10100/35182]: Training loss 0.4853333532810211, current lr 9.935927062903272e-05, contrastive loss 3.874725818634033, task loss 0.44658610224723816
2025-01-05 08:47:18,315 - root - INFO - [Train] Epoch 1 [10200/35182]: Training loss 0.5905271172523499, current lr 9.935784940735057e-05, contrastive loss 4.046754837036133, task loss 0.5500595569610596
2025-01-05 08:48:58,911 - root - INFO - [Train] Epoch 1 [10300/35182]: Training loss 0.5186886787414551, current lr 9.93564281856684e-05, contrastive loss 3.834501028060913, task loss 0.48034369945526123
2025-01-05 08:50:39,283 - root - INFO - [Train] Epoch 1 [10400/35182]: Training loss 0.5338727831840515, current lr 9.935500696398626e-05, contrastive loss 4.027439117431641, task loss 0.49359840154647827
2025-01-05 08:52:19,886 - root - INFO - [Train] Epoch 1 [10500/35182]: Training loss 0.5481885075569153, current lr 9.935358574230409e-05, contrastive loss 4.143244743347168, task loss 0.506756067276001
2025-01-05 08:54:00,266 - root - INFO - [Train] Epoch 1 [10600/35182]: Training loss 0.603502094745636, current lr 9.935216452062193e-05, contrastive loss 3.970280647277832, task loss 0.563799262046814
2025-01-05 08:55:40,843 - root - INFO - [Train] Epoch 1 [10700/35182]: Training loss 0.5827110409736633, current lr 9.935074329893978e-05, contrastive loss 3.971196174621582, task loss 0.5429990887641907
2025-01-05 08:57:21,222 - root - INFO - [Train] Epoch 1 [10800/35182]: Training loss 0.5595596432685852, current lr 9.93493220772576e-05, contrastive loss 3.947023391723633, task loss 0.5200893878936768
2025-01-05 08:59:01,813 - root - INFO - [Train] Epoch 1 [10900/35182]: Training loss 0.5828428864479065, current lr 9.934790085557546e-05, contrastive loss 3.9580299854278564, task loss 0.5432626008987427
2025-01-05 09:00:42,193 - root - INFO - [Train] Epoch 1 [11000/35182]: Training loss 0.5268643498420715, current lr 9.934647963389329e-05, contrastive loss 3.8598618507385254, task loss 0.488265722990036
2025-01-05 09:02:22,574 - root - INFO - [Train] Epoch 1 [11100/35182]: Training loss 0.48690760135650635, current lr 9.934505841221115e-05, contrastive loss 4.0512590408325195, task loss 0.44639500975608826
2025-01-05 09:04:03,165 - root - INFO - [Train] Epoch 1 [11200/35182]: Training loss 0.554712176322937, current lr 9.934363719052898e-05, contrastive loss 3.9474806785583496, task loss 0.5152373909950256
2025-01-05 09:05:43,535 - root - INFO - [Train] Epoch 1 [11300/35182]: Training loss 0.5112797617912292, current lr 9.934221596884684e-05, contrastive loss 4.104056358337402, task loss 0.47023922204971313
2025-01-05 09:07:24,121 - root - INFO - [Train] Epoch 1 [11400/35182]: Training loss 0.4958929717540741, current lr 9.934079474716467e-05, contrastive loss 3.798037528991699, task loss 0.4579125940799713
2025-01-05 09:09:04,504 - root - INFO - [Train] Epoch 1 [11500/35182]: Training loss 0.5155102014541626, current lr 9.933937352548251e-05, contrastive loss 4.06801700592041, task loss 0.4748300313949585
2025-01-05 09:10:45,082 - root - INFO - [Train] Epoch 1 [11600/35182]: Training loss 0.5331830978393555, current lr 9.933795230380035e-05, contrastive loss 3.9088358879089355, task loss 0.49409475922584534
2025-01-05 09:12:25,453 - root - INFO - [Train] Epoch 1 [11700/35182]: Training loss 0.5829217433929443, current lr 9.93365310821182e-05, contrastive loss 4.012664318084717, task loss 0.5427951216697693
2025-01-05 09:14:06,047 - root - INFO - [Train] Epoch 1 [11800/35182]: Training loss 0.5672280192375183, current lr 9.933510986043604e-05, contrastive loss 3.788455009460449, task loss 0.5293434858322144
2025-01-05 09:15:46,422 - root - INFO - [Train] Epoch 1 [11900/35182]: Training loss 0.49717286229133606, current lr 9.933368863875387e-05, contrastive loss 3.803056240081787, task loss 0.4591422975063324
2025-01-05 09:17:27,013 - root - INFO - [Train] Epoch 1 [12000/35182]: Training loss 0.5238841772079468, current lr 9.933226741707173e-05, contrastive loss 3.889739513397217, task loss 0.484986811876297
2025-01-05 09:19:07,396 - root - INFO - [Train] Epoch 1 [12100/35182]: Training loss 0.5389969348907471, current lr 9.933084619538956e-05, contrastive loss 3.8362526893615723, task loss 0.5006344318389893
2025-01-05 09:20:47,999 - root - INFO - [Train] Epoch 1 [12200/35182]: Training loss 0.5127738118171692, current lr 9.932942497370741e-05, contrastive loss 3.867748737335205, task loss 0.47409629821777344
2025-01-05 09:22:28,384 - root - INFO - [Train] Epoch 1 [12300/35182]: Training loss 0.5721438527107239, current lr 9.932800375202524e-05, contrastive loss 3.8159284591674805, task loss 0.5339845418930054
2025-01-05 09:24:08,961 - root - INFO - [Train] Epoch 1 [12400/35182]: Training loss 0.515849232673645, current lr 9.93265825303431e-05, contrastive loss 4.013180732727051, task loss 0.4757174253463745
2025-01-05 09:25:49,350 - root - INFO - [Train] Epoch 1 [12500/35182]: Training loss 0.4798821806907654, current lr 9.932516130866093e-05, contrastive loss 3.7466936111450195, task loss 0.4424152374267578
2025-01-05 09:27:29,918 - root - INFO - [Train] Epoch 1 [12600/35182]: Training loss 0.4892914891242981, current lr 9.932374008697877e-05, contrastive loss 3.7136361598968506, task loss 0.45215511322021484
2025-01-05 09:29:10,303 - root - INFO - [Train] Epoch 1 [12700/35182]: Training loss 0.5372004508972168, current lr 9.932231886529662e-05, contrastive loss 3.6864874362945557, task loss 0.5003355741500854
2025-01-05 09:30:50,882 - root - INFO - [Train] Epoch 1 [12800/35182]: Training loss 0.6260885000228882, current lr 9.932089764361445e-05, contrastive loss 3.906862497329712, task loss 0.5870198607444763
2025-01-05 09:32:31,269 - root - INFO - [Train] Epoch 1 [12900/35182]: Training loss 0.5528964400291443, current lr 9.93194764219323e-05, contrastive loss 3.6786601543426514, task loss 0.516109824180603
2025-01-05 09:34:11,850 - root - INFO - [Train] Epoch 1 [13000/35182]: Training loss 0.6372125744819641, current lr 9.931805520025013e-05, contrastive loss 3.989011764526367, task loss 0.5973224639892578
2025-01-05 09:35:52,230 - root - INFO - [Train] Epoch 1 [13100/35182]: Training loss 0.5315325260162354, current lr 9.931663397856799e-05, contrastive loss 3.831590175628662, task loss 0.4932166337966919
2025-01-05 09:37:32,815 - root - INFO - [Train] Epoch 1 [13200/35182]: Training loss 0.4583837687969208, current lr 9.931521275688582e-05, contrastive loss 3.8502094745635986, task loss 0.419881671667099
2025-01-05 09:39:13,200 - root - INFO - [Train] Epoch 1 [13300/35182]: Training loss 0.6286097764968872, current lr 9.931379153520368e-05, contrastive loss 3.964982748031616, task loss 0.5889599323272705
2025-01-05 09:40:53,588 - root - INFO - [Train] Epoch 1 [13400/35182]: Training loss 0.48869964480400085, current lr 9.93123703135215e-05, contrastive loss 3.955165386199951, task loss 0.4491479992866516
2025-01-05 09:42:34,162 - root - INFO - [Train] Epoch 1 [13500/35182]: Training loss 0.5649494528770447, current lr 9.931094909183935e-05, contrastive loss 3.6262216567993164, task loss 0.5286872386932373
2025-01-05 09:44:14,534 - root - INFO - [Train] Epoch 1 [13600/35182]: Training loss 0.5733413696289062, current lr 9.930952787015719e-05, contrastive loss 3.9709115028381348, task loss 0.5336322784423828
2025-01-05 09:45:55,131 - root - INFO - [Train] Epoch 1 [13700/35182]: Training loss 0.5199525356292725, current lr 9.930810664847504e-05, contrastive loss 3.917877197265625, task loss 0.48077377676963806
2025-01-05 09:47:35,514 - root - INFO - [Train] Epoch 1 [13800/35182]: Training loss 0.48329609632492065, current lr 9.930668542679288e-05, contrastive loss 3.8437721729278564, task loss 0.4448583722114563
2025-01-05 09:49:16,098 - root - INFO - [Train] Epoch 1 [13900/35182]: Training loss 0.5587676763534546, current lr 9.930526420511071e-05, contrastive loss 3.7824816703796387, task loss 0.5209428668022156
2025-01-05 09:50:56,486 - root - INFO - [Train] Epoch 1 [14000/35182]: Training loss 0.5740624070167542, current lr 9.930384298342857e-05, contrastive loss 3.8476665019989014, task loss 0.5355857610702515
2025-01-05 09:52:37,054 - root - INFO - [Train] Epoch 1 [14100/35182]: Training loss 0.5543697476387024, current lr 9.93024217617464e-05, contrastive loss 3.7844913005828857, task loss 0.5165248513221741
2025-01-05 09:54:17,427 - root - INFO - [Train] Epoch 1 [14200/35182]: Training loss 0.5426220893859863, current lr 9.930100054006425e-05, contrastive loss 3.799558639526367, task loss 0.5046265125274658
2025-01-05 09:55:58,021 - root - INFO - [Train] Epoch 1 [14300/35182]: Training loss 0.5047524571418762, current lr 9.929957931838208e-05, contrastive loss 4.117399215698242, task loss 0.463578462600708
2025-01-05 09:57:38,419 - root - INFO - [Train] Epoch 1 [14400/35182]: Training loss 0.5750311613082886, current lr 9.929815809669993e-05, contrastive loss 3.824686288833618, task loss 0.536784291267395
2025-01-05 09:59:18,985 - root - INFO - [Train] Epoch 1 [14500/35182]: Training loss 0.5400568246841431, current lr 9.929673687501777e-05, contrastive loss 3.9454054832458496, task loss 0.5006027817726135
2025-01-05 10:00:59,366 - root - INFO - [Train] Epoch 1 [14600/35182]: Training loss 0.5106576681137085, current lr 9.929531565333561e-05, contrastive loss 3.9740047454833984, task loss 0.47091761231422424
2025-01-05 10:02:39,960 - root - INFO - [Train] Epoch 1 [14700/35182]: Training loss 0.4672820568084717, current lr 9.929389443165346e-05, contrastive loss 3.789768934249878, task loss 0.42938438057899475
2025-01-05 10:04:20,340 - root - INFO - [Train] Epoch 1 [14800/35182]: Training loss 0.6153373122215271, current lr 9.929247320997129e-05, contrastive loss 3.7809982299804688, task loss 0.5775273442268372
2025-01-05 10:06:00,920 - root - INFO - [Train] Epoch 1 [14900/35182]: Training loss 0.6461743712425232, current lr 9.929105198828914e-05, contrastive loss 3.8588385581970215, task loss 0.6075859665870667
2025-01-05 10:07:41,317 - root - INFO - [Train] Epoch 1 [15000/35182]: Training loss 0.48715120553970337, current lr 9.928963076660697e-05, contrastive loss 3.7489118576049805, task loss 0.44966208934783936
2025-01-05 10:09:21,988 - root - INFO - [Train] Epoch 1 [15100/35182]: Training loss 0.5335871577262878, current lr 9.928820954492483e-05, contrastive loss 3.8229637145996094, task loss 0.4953575134277344
2025-01-05 10:11:02,375 - root - INFO - [Train] Epoch 1 [15200/35182]: Training loss 0.5396144390106201, current lr 9.928678832324266e-05, contrastive loss 3.8758766651153564, task loss 0.5008556842803955
2025-01-05 10:12:42,967 - root - INFO - [Train] Epoch 1 [15300/35182]: Training loss 0.5371942520141602, current lr 9.928536710156052e-05, contrastive loss 3.8085761070251465, task loss 0.4991084635257721
2025-01-05 10:14:23,344 - root - INFO - [Train] Epoch 1 [15400/35182]: Training loss 0.5308229327201843, current lr 9.928394587987835e-05, contrastive loss 3.8743574619293213, task loss 0.49207934737205505
2025-01-05 10:16:03,931 - root - INFO - [Train] Epoch 1 [15500/35182]: Training loss 0.5368700623512268, current lr 9.928252465819619e-05, contrastive loss 3.7158961296081543, task loss 0.49971112608909607
2025-01-05 10:17:44,320 - root - INFO - [Train] Epoch 1 [15600/35182]: Training loss 0.485196053981781, current lr 9.928110343651403e-05, contrastive loss 3.8273489475250244, task loss 0.44692257046699524
2025-01-05 10:19:24,896 - root - INFO - [Train] Epoch 1 [15700/35182]: Training loss 0.5821237564086914, current lr 9.927968221483188e-05, contrastive loss 3.9537267684936523, task loss 0.5425865054130554
2025-01-05 10:21:05,312 - root - INFO - [Train] Epoch 1 [15800/35182]: Training loss 0.5522152185440063, current lr 9.927826099314972e-05, contrastive loss 3.6619160175323486, task loss 0.5155960321426392
2025-01-05 10:22:45,977 - root - INFO - [Train] Epoch 1 [15900/35182]: Training loss 0.5590515732765198, current lr 9.927683977146755e-05, contrastive loss 3.8457703590393066, task loss 0.5205938816070557
2025-01-05 10:24:26,360 - root - INFO - [Train] Epoch 1 [16000/35182]: Training loss 0.5183444023132324, current lr 9.927541854978541e-05, contrastive loss 3.7972359657287598, task loss 0.4803720712661743
2025-01-05 10:26:06,944 - root - INFO - [Train] Epoch 1 [16100/35182]: Training loss 0.5377379655838013, current lr 9.927399732810324e-05, contrastive loss 3.7358016967773438, task loss 0.5003799796104431
2025-01-05 10:27:47,335 - root - INFO - [Train] Epoch 1 [16200/35182]: Training loss 0.5221666693687439, current lr 9.92725761064211e-05, contrastive loss 3.731889486312866, task loss 0.4848477840423584
2025-01-05 10:29:27,908 - root - INFO - [Train] Epoch 1 [16300/35182]: Training loss 0.5314221382141113, current lr 9.927115488473892e-05, contrastive loss 3.781541347503662, task loss 0.4936067461967468
2025-01-05 10:31:08,283 - root - INFO - [Train] Epoch 1 [16400/35182]: Training loss 0.5528476238250732, current lr 9.926973366305677e-05, contrastive loss 3.8531646728515625, task loss 0.5143159627914429
2025-01-05 10:32:48,878 - root - INFO - [Train] Epoch 1 [16500/35182]: Training loss 0.49546927213668823, current lr 9.926831244137461e-05, contrastive loss 3.743474006652832, task loss 0.45803454518318176
2025-01-05 10:34:29,261 - root - INFO - [Train] Epoch 1 [16600/35182]: Training loss 0.5107336044311523, current lr 9.926689121969245e-05, contrastive loss 3.759726047515869, task loss 0.4731363356113434
2025-01-05 10:36:09,848 - root - INFO - [Train] Epoch 1 [16700/35182]: Training loss 0.5773406624794006, current lr 9.92654699980103e-05, contrastive loss 3.725785255432129, task loss 0.5400828123092651
2025-01-05 10:37:50,236 - root - INFO - [Train] Epoch 1 [16800/35182]: Training loss 0.5087299346923828, current lr 9.926404877632813e-05, contrastive loss 3.7665576934814453, task loss 0.47106438875198364
2025-01-05 10:39:30,812 - root - INFO - [Train] Epoch 1 [16900/35182]: Training loss 0.4531194865703583, current lr 9.926262755464598e-05, contrastive loss 3.6002392768859863, task loss 0.41711708903312683
2025-01-05 10:41:11,193 - root - INFO - [Train] Epoch 1 [17000/35182]: Training loss 0.5378015041351318, current lr 9.926120633296381e-05, contrastive loss 3.752567768096924, task loss 0.5002758502960205
2025-01-05 10:42:51,556 - root - INFO - [Train] Epoch 1 [17100/35182]: Training loss 0.5789718627929688, current lr 9.925978511128167e-05, contrastive loss 3.5623888969421387, task loss 0.543347954750061
2025-01-05 10:44:32,165 - root - INFO - [Train] Epoch 1 [17200/35182]: Training loss 0.5578581094741821, current lr 9.92583638895995e-05, contrastive loss 3.6610357761383057, task loss 0.5212477445602417
2025-01-05 10:46:12,545 - root - INFO - [Train] Epoch 1 [17300/35182]: Training loss 0.5576035380363464, current lr 9.925694266791736e-05, contrastive loss 3.6422014236450195, task loss 0.5211815237998962
2025-01-05 10:47:53,114 - root - INFO - [Train] Epoch 1 [17400/35182]: Training loss 0.5551501512527466, current lr 9.925552144623519e-05, contrastive loss 3.590147018432617, task loss 0.5192486643791199
2025-01-05 10:49:33,493 - root - INFO - [Train] Epoch 1 [17500/35182]: Training loss 0.5108111500740051, current lr 9.925410022455303e-05, contrastive loss 3.7179958820343018, task loss 0.4736311733722687
2025-01-05 10:51:14,088 - root - INFO - [Train] Epoch 1 [17600/35182]: Training loss 0.5071791410446167, current lr 9.925267900287087e-05, contrastive loss 3.6225247383117676, task loss 0.47095391154289246
2025-01-05 10:52:54,462 - root - INFO - [Train] Epoch 1 [17700/35182]: Training loss 0.5516377687454224, current lr 9.925125778118872e-05, contrastive loss 3.8104944229125977, task loss 0.513532817363739
2025-01-05 10:54:35,046 - root - INFO - [Train] Epoch 1 [17800/35182]: Training loss 0.5771725177764893, current lr 9.924983655950656e-05, contrastive loss 3.74450421333313, task loss 0.5397274494171143
2025-01-05 10:56:15,426 - root - INFO - [Train] Epoch 1 [17900/35182]: Training loss 0.5174751877784729, current lr 9.924841533782439e-05, contrastive loss 3.668633460998535, task loss 0.48078882694244385
2025-01-05 10:57:56,017 - root - INFO - [Train] Epoch 1 [18000/35182]: Training loss 0.5784087181091309, current lr 9.924699411614225e-05, contrastive loss 3.718475103378296, task loss 0.54122394323349
2025-01-05 10:59:36,406 - root - INFO - [Train] Epoch 1 [18100/35182]: Training loss 0.5657567977905273, current lr 9.924557289446008e-05, contrastive loss 3.578221321105957, task loss 0.5299745798110962
2025-01-05 11:01:16,984 - root - INFO - [Train] Epoch 1 [18200/35182]: Training loss 0.5192325711250305, current lr 9.924415167277793e-05, contrastive loss 3.81084942817688, task loss 0.48112407326698303
2025-01-05 11:02:57,370 - root - INFO - [Train] Epoch 1 [18300/35182]: Training loss 0.5685473084449768, current lr 9.924273045109576e-05, contrastive loss 3.7306480407714844, task loss 0.5312408208847046
2025-01-05 11:04:37,965 - root - INFO - [Train] Epoch 1 [18400/35182]: Training loss 0.5132114887237549, current lr 9.924130922941361e-05, contrastive loss 3.4534101486206055, task loss 0.478677362203598
2025-01-05 11:06:18,333 - root - INFO - [Train] Epoch 1 [18500/35182]: Training loss 0.5306214094161987, current lr 9.923988800773145e-05, contrastive loss 3.65714955329895, task loss 0.49404990673065186
2025-01-05 11:07:58,910 - root - INFO - [Train] Epoch 1 [18600/35182]: Training loss 0.48472505807876587, current lr 9.92384667860493e-05, contrastive loss 3.5659677982330322, task loss 0.4490653872489929
2025-01-05 11:09:39,290 - root - INFO - [Train] Epoch 1 [18700/35182]: Training loss 0.5713217258453369, current lr 9.923704556436714e-05, contrastive loss 3.6087679862976074, task loss 0.5352340340614319
2025-01-05 11:11:19,880 - root - INFO - [Train] Epoch 1 [18800/35182]: Training loss 0.6053155660629272, current lr 9.923562434268497e-05, contrastive loss 3.7353200912475586, task loss 0.5679623484611511
2025-01-05 11:13:00,253 - root - INFO - [Train] Epoch 1 [18900/35182]: Training loss 0.523120641708374, current lr 9.923420312100282e-05, contrastive loss 3.7039499282836914, task loss 0.4860811233520508
2025-01-05 11:14:40,845 - root - INFO - [Train] Epoch 1 [19000/35182]: Training loss 0.5181615352630615, current lr 9.923278189932065e-05, contrastive loss 3.7171812057495117, task loss 0.4809897243976593
2025-01-05 11:16:21,235 - root - INFO - [Train] Epoch 1 [19100/35182]: Training loss 0.5517790913581848, current lr 9.923136067763851e-05, contrastive loss 3.814054489135742, task loss 0.5136385560035706
2025-01-05 11:18:01,815 - root - INFO - [Train] Epoch 1 [19200/35182]: Training loss 0.5113437175750732, current lr 9.922993945595634e-05, contrastive loss 3.687742233276367, task loss 0.4744662940502167
2025-01-05 11:19:42,198 - root - INFO - [Train] Epoch 1 [19300/35182]: Training loss 0.5175315737724304, current lr 9.922851823427418e-05, contrastive loss 3.683898448944092, task loss 0.4806925654411316
2025-01-05 11:21:22,571 - root - INFO - [Train] Epoch 1 [19400/35182]: Training loss 0.5872527956962585, current lr 9.922709701259203e-05, contrastive loss 3.765960216522217, task loss 0.5495932102203369
2025-01-05 11:23:03,153 - root - INFO - [Train] Epoch 1 [19500/35182]: Training loss 0.4834583103656769, current lr 9.922567579090987e-05, contrastive loss 3.6155781745910645, task loss 0.44730252027511597
2025-01-05 11:24:43,542 - root - INFO - [Train] Epoch 1 [19600/35182]: Training loss 0.5158381462097168, current lr 9.922425456922771e-05, contrastive loss 3.5917625427246094, task loss 0.47992050647735596
2025-01-05 11:26:24,141 - root - INFO - [Train] Epoch 1 [19700/35182]: Training loss 0.490093857049942, current lr 9.922283334754556e-05, contrastive loss 3.593834161758423, task loss 0.45415550470352173
2025-01-05 11:28:04,525 - root - INFO - [Train] Epoch 1 [19800/35182]: Training loss 0.5156596302986145, current lr 9.92214121258634e-05, contrastive loss 3.68141508102417, task loss 0.478845477104187
2025-01-05 11:29:45,094 - root - INFO - [Train] Epoch 1 [19900/35182]: Training loss 0.5884116291999817, current lr 9.921999090418123e-05, contrastive loss 3.473391532897949, task loss 0.5536777377128601
2025-01-05 11:31:25,471 - root - INFO - [Train] Epoch 1 [20000/35182]: Training loss 0.5572004318237305, current lr 9.921856968249909e-05, contrastive loss 3.6174912452697754, task loss 0.521025538444519
2025-01-05 11:33:06,061 - root - INFO - [Train] Epoch 1 [20100/35182]: Training loss 0.5468066930770874, current lr 9.921714846081692e-05, contrastive loss 3.7329139709472656, task loss 0.5094775557518005
2025-01-05 11:34:46,473 - root - INFO - [Train] Epoch 1 [20200/35182]: Training loss 0.5626279711723328, current lr 9.921572723913478e-05, contrastive loss 3.5666146278381348, task loss 0.5269618034362793
2025-01-05 11:36:27,136 - root - INFO - [Train] Epoch 1 [20300/35182]: Training loss 0.5768423080444336, current lr 9.92143060174526e-05, contrastive loss 3.4545328617095947, task loss 0.5422970056533813
2025-01-05 11:38:07,531 - root - INFO - [Train] Epoch 1 [20400/35182]: Training loss 0.5515069961547852, current lr 9.921288479577045e-05, contrastive loss 3.779186725616455, task loss 0.5137151479721069
2025-01-05 11:39:48,100 - root - INFO - [Train] Epoch 1 [20500/35182]: Training loss 0.5569296479225159, current lr 9.921146357408829e-05, contrastive loss 3.706791877746582, task loss 0.5198617577552795
2025-01-05 11:41:28,476 - root - INFO - [Train] Epoch 1 [20600/35182]: Training loss 0.5950037837028503, current lr 9.921004235240614e-05, contrastive loss 3.589120864868164, task loss 0.559112548828125
2025-01-05 11:43:09,058 - root - INFO - [Train] Epoch 1 [20700/35182]: Training loss 0.5449464321136475, current lr 9.920862113072398e-05, contrastive loss 3.734558582305908, task loss 0.5076008439064026
2025-01-05 11:44:49,416 - root - INFO - [Train] Epoch 1 [20800/35182]: Training loss 0.5143094658851624, current lr 9.920719990904181e-05, contrastive loss 3.5465643405914307, task loss 0.4788438081741333
2025-01-05 11:46:30,031 - root - INFO - [Train] Epoch 1 [20900/35182]: Training loss 0.5622145533561707, current lr 9.920577868735967e-05, contrastive loss 3.622988700866699, task loss 0.5259846448898315
2025-01-05 11:48:10,399 - root - INFO - [Train] Epoch 1 [21000/35182]: Training loss 0.5506505966186523, current lr 9.92043574656775e-05, contrastive loss 3.6882858276367188, task loss 0.5137677192687988
2025-01-05 11:49:51,001 - root - INFO - [Train] Epoch 1 [21100/35182]: Training loss 0.5570150017738342, current lr 9.920293624399535e-05, contrastive loss 3.5119271278381348, task loss 0.521895706653595
2025-01-05 11:51:31,388 - root - INFO - [Train] Epoch 1 [21200/35182]: Training loss 0.4905134439468384, current lr 9.920151502231318e-05, contrastive loss 3.6825554370880127, task loss 0.4536879062652588
2025-01-05 11:53:11,968 - root - INFO - [Train] Epoch 1 [21300/35182]: Training loss 0.5672883987426758, current lr 9.920009380063103e-05, contrastive loss 3.5939724445343018, task loss 0.531348705291748
2025-01-05 11:54:52,341 - root - INFO - [Train] Epoch 1 [21400/35182]: Training loss 0.527199923992157, current lr 9.919867257894887e-05, contrastive loss 3.6395983695983887, task loss 0.49080395698547363
2025-01-05 11:56:32,932 - root - INFO - [Train] Epoch 1 [21500/35182]: Training loss 0.5537147521972656, current lr 9.919725135726671e-05, contrastive loss 3.640666961669922, task loss 0.5173080563545227
2025-01-05 11:58:13,299 - root - INFO - [Train] Epoch 1 [21600/35182]: Training loss 0.5083096027374268, current lr 9.919583013558456e-05, contrastive loss 3.6709532737731934, task loss 0.47160008549690247
2025-01-05 11:59:53,904 - root - INFO - [Train] Epoch 1 [21700/35182]: Training loss 0.5632643699645996, current lr 9.91944089139024e-05, contrastive loss 3.4518790245056152, task loss 0.5287455916404724
2025-01-05 12:01:34,296 - root - INFO - [Train] Epoch 1 [21800/35182]: Training loss 0.5225675702095032, current lr 9.919298769222024e-05, contrastive loss 3.542469024658203, task loss 0.4871428608894348
2025-01-05 12:03:14,967 - root - INFO - [Train] Epoch 1 [21900/35182]: Training loss 0.6566174030303955, current lr 9.919156647053807e-05, contrastive loss 3.689155101776123, task loss 0.6197258830070496
2025-01-05 12:04:55,341 - root - INFO - [Train] Epoch 1 [22000/35182]: Training loss 0.5137288570404053, current lr 9.919014524885593e-05, contrastive loss 3.472658157348633, task loss 0.47900229692459106
2025-01-05 12:06:35,950 - root - INFO - [Train] Epoch 1 [22100/35182]: Training loss 0.5290158987045288, current lr 9.918872402717376e-05, contrastive loss 3.584991216659546, task loss 0.4931659996509552
2025-01-05 12:08:16,332 - root - INFO - [Train] Epoch 1 [22200/35182]: Training loss 0.5115851759910583, current lr 9.91873028054916e-05, contrastive loss 3.8553075790405273, task loss 0.4730320870876312
2025-01-05 12:09:56,907 - root - INFO - [Train] Epoch 1 [22300/35182]: Training loss 0.5383813381195068, current lr 9.918588158380945e-05, contrastive loss 3.5707991123199463, task loss 0.502673327922821
2025-01-05 12:11:37,291 - root - INFO - [Train] Epoch 1 [22400/35182]: Training loss 0.5107319355010986, current lr 9.918446036212729e-05, contrastive loss 3.6520779132843018, task loss 0.474211186170578
2025-01-05 12:13:17,868 - root - INFO - [Train] Epoch 1 [22500/35182]: Training loss 0.5507619976997375, current lr 9.918303914044513e-05, contrastive loss 3.6274280548095703, task loss 0.5144877433776855
2025-01-05 12:14:58,249 - root - INFO - [Train] Epoch 1 [22600/35182]: Training loss 0.511712372303009, current lr 9.918161791876298e-05, contrastive loss 3.620964527130127, task loss 0.47550272941589355
2025-01-05 12:16:38,838 - root - INFO - [Train] Epoch 1 [22700/35182]: Training loss 0.5571820735931396, current lr 9.918019669708082e-05, contrastive loss 3.6979787349700928, task loss 0.5202022790908813
2025-01-05 12:18:19,206 - root - INFO - [Train] Epoch 1 [22800/35182]: Training loss 0.5564264059066772, current lr 9.917877547539865e-05, contrastive loss 3.4925763607025146, task loss 0.5215006470680237
2025-01-05 12:19:59,805 - root - INFO - [Train] Epoch 1 [22900/35182]: Training loss 0.5085047483444214, current lr 9.91773542537165e-05, contrastive loss 3.6095118522644043, task loss 0.4724096357822418
2025-01-05 12:21:40,189 - root - INFO - [Train] Epoch 1 [23000/35182]: Training loss 0.4707637131214142, current lr 9.917593303203434e-05, contrastive loss 3.761967420578003, task loss 0.4331440329551697
2025-01-05 12:23:20,567 - root - INFO - [Train] Epoch 1 [23100/35182]: Training loss 0.6119711399078369, current lr 9.917451181035219e-05, contrastive loss 3.6066408157348633, task loss 0.5759047269821167
2025-01-05 12:25:01,158 - root - INFO - [Train] Epoch 1 [23200/35182]: Training loss 0.5176671743392944, current lr 9.917309058867002e-05, contrastive loss 3.7448740005493164, task loss 0.48021844029426575
2025-01-05 12:26:41,548 - root - INFO - [Train] Epoch 1 [23300/35182]: Training loss 0.4790497124195099, current lr 9.917166936698787e-05, contrastive loss 3.4096455574035645, task loss 0.4449532628059387
2025-01-05 12:28:22,124 - root - INFO - [Train] Epoch 1 [23400/35182]: Training loss 0.528495728969574, current lr 9.917024814530571e-05, contrastive loss 3.4458494186401367, task loss 0.4940372109413147
2025-01-05 12:30:02,812 - root - INFO - [Train] Epoch 1 [23500/35182]: Training loss 0.5374881029129028, current lr 9.916882692362355e-05, contrastive loss 3.552483558654785, task loss 0.5019632577896118
2025-01-05 12:31:43,190 - root - INFO - [Train] Epoch 1 [23600/35182]: Training loss 0.4935508966445923, current lr 9.91674057019414e-05, contrastive loss 3.529606819152832, task loss 0.4582548439502716
2025-01-05 12:33:23,562 - root - INFO - [Train] Epoch 1 [23700/35182]: Training loss 0.5449896454811096, current lr 9.916598448025924e-05, contrastive loss 3.631392002105713, task loss 0.508675754070282
2025-01-05 12:35:04,168 - root - INFO - [Train] Epoch 1 [23800/35182]: Training loss 0.5029860138893127, current lr 9.916456325857708e-05, contrastive loss 3.56569242477417, task loss 0.46732911467552185
2025-01-05 12:36:44,543 - root - INFO - [Train] Epoch 1 [23900/35182]: Training loss 0.5019866824150085, current lr 9.916314203689491e-05, contrastive loss 3.5807642936706543, task loss 0.4661790132522583
2025-01-05 12:38:25,162 - root - INFO - [Train] Epoch 1 [24000/35182]: Training loss 0.5785963535308838, current lr 9.916172081521277e-05, contrastive loss 3.431295156478882, task loss 0.544283390045166
2025-01-05 12:40:05,831 - root - INFO - [Train] Epoch 1 [24100/35182]: Training loss 0.549862802028656, current lr 9.91602995935306e-05, contrastive loss 3.466404914855957, task loss 0.5151987671852112
2025-01-05 12:41:46,222 - root - INFO - [Train] Epoch 1 [24200/35182]: Training loss 0.5326356887817383, current lr 9.915887837184844e-05, contrastive loss 3.868401050567627, task loss 0.4939516484737396
2025-01-05 12:43:26,590 - root - INFO - [Train] Epoch 1 [24300/35182]: Training loss 0.5925179123878479, current lr 9.915745715016629e-05, contrastive loss 3.6656293869018555, task loss 0.5558615922927856
2025-01-05 12:45:07,173 - root - INFO - [Train] Epoch 1 [24400/35182]: Training loss 0.5176181197166443, current lr 9.915603592848413e-05, contrastive loss 3.761509418487549, task loss 0.4800029993057251
2025-01-05 12:46:47,551 - root - INFO - [Train] Epoch 1 [24500/35182]: Training loss 0.530300498008728, current lr 9.915461470680197e-05, contrastive loss 3.636212110519409, task loss 0.4939383566379547
2025-01-05 12:48:28,137 - root - INFO - [Train] Epoch 1 [24600/35182]: Training loss 0.48645883798599243, current lr 9.915319348511982e-05, contrastive loss 3.4824447631835938, task loss 0.45163440704345703
2025-01-05 12:50:08,523 - root - INFO - [Train] Epoch 1 [24700/35182]: Training loss 0.6340102553367615, current lr 9.915177226343766e-05, contrastive loss 3.38950777053833, task loss 0.600115180015564
2025-01-05 12:51:49,100 - root - INFO - [Train] Epoch 1 [24800/35182]: Training loss 0.562157154083252, current lr 9.915035104175549e-05, contrastive loss 3.578181743621826, task loss 0.5263753533363342
2025-01-05 12:53:29,486 - root - INFO - [Train] Epoch 1 [24900/35182]: Training loss 0.5158913731575012, current lr 9.914892982007335e-05, contrastive loss 3.6974382400512695, task loss 0.4789170026779175
2025-01-05 12:55:10,074 - root - INFO - [Train] Epoch 1 [25000/35182]: Training loss 0.5699892044067383, current lr 9.914750859839118e-05, contrastive loss 3.472224235534668, task loss 0.5352669358253479
2025-01-05 12:56:50,442 - root - INFO - [Train] Epoch 1 [25100/35182]: Training loss 0.5519298315048218, current lr 9.914608737670903e-05, contrastive loss 3.5226192474365234, task loss 0.5167036652565002
2025-01-05 12:58:31,039 - root - INFO - [Train] Epoch 1 [25200/35182]: Training loss 0.5420849323272705, current lr 9.914466615502686e-05, contrastive loss 3.4529597759246826, task loss 0.507555365562439
2025-01-05 13:00:11,420 - root - INFO - [Train] Epoch 1 [25300/35182]: Training loss 0.47996920347213745, current lr 9.914324493334471e-05, contrastive loss 3.384514808654785, task loss 0.44612404704093933
2025-01-05 13:01:52,006 - root - INFO - [Train] Epoch 1 [25400/35182]: Training loss 0.5003849267959595, current lr 9.914182371166255e-05, contrastive loss 3.5762648582458496, task loss 0.46462225914001465
2025-01-05 13:03:32,387 - root - INFO - [Train] Epoch 1 [25500/35182]: Training loss 0.6063967347145081, current lr 9.91404024899804e-05, contrastive loss 3.5638999938964844, task loss 0.5707577466964722
2025-01-05 13:05:12,967 - root - INFO - [Train] Epoch 1 [25600/35182]: Training loss 0.5164090394973755, current lr 9.913898126829824e-05, contrastive loss 3.419673442840576, task loss 0.4822123348712921
2025-01-05 13:06:53,357 - root - INFO - [Train] Epoch 1 [25700/35182]: Training loss 0.5686559081077576, current lr 9.913756004661608e-05, contrastive loss 3.3317086696624756, task loss 0.535338819026947
2025-01-05 13:08:33,938 - root - INFO - [Train] Epoch 1 [25800/35182]: Training loss 0.4930345118045807, current lr 9.913613882493392e-05, contrastive loss 3.5587854385375977, task loss 0.4574466645717621
2025-01-05 13:10:14,319 - root - INFO - [Train] Epoch 1 [25900/35182]: Training loss 0.4626099169254303, current lr 9.913471760325175e-05, contrastive loss 3.516263008117676, task loss 0.42744728922843933
2025-01-05 13:11:54,900 - root - INFO - [Train] Epoch 1 [26000/35182]: Training loss 0.5998151302337646, current lr 9.913329638156961e-05, contrastive loss 3.330406665802002, task loss 0.5665110945701599
2025-01-05 13:13:35,294 - root - INFO - [Train] Epoch 1 [26100/35182]: Training loss 0.5010496973991394, current lr 9.913187515988744e-05, contrastive loss 3.3936867713928223, task loss 0.46711280941963196
2025-01-05 13:15:15,870 - root - INFO - [Train] Epoch 1 [26200/35182]: Training loss 0.4706120789051056, current lr 9.913045393820528e-05, contrastive loss 3.5417490005493164, task loss 0.43519458174705505
2025-01-05 13:16:56,249 - root - INFO - [Train] Epoch 1 [26300/35182]: Training loss 0.543614387512207, current lr 9.912903271652313e-05, contrastive loss 3.53629732131958, task loss 0.508251428604126
2025-01-05 13:18:36,837 - root - INFO - [Train] Epoch 1 [26400/35182]: Training loss 0.527617871761322, current lr 9.912761149484097e-05, contrastive loss 3.4425008296966553, task loss 0.4931928515434265
2025-01-05 13:20:17,210 - root - INFO - [Train] Epoch 1 [26500/35182]: Training loss 0.5141075253486633, current lr 9.912619027315881e-05, contrastive loss 3.4157838821411133, task loss 0.4799496829509735
2025-01-05 13:21:57,798 - root - INFO - [Train] Epoch 1 [26600/35182]: Training loss 0.5508829951286316, current lr 9.912476905147666e-05, contrastive loss 3.441931962966919, task loss 0.5164636969566345
2025-01-05 13:23:38,190 - root - INFO - [Train] Epoch 1 [26700/35182]: Training loss 0.5308609008789062, current lr 9.91233478297945e-05, contrastive loss 3.3981575965881348, task loss 0.49687930941581726
2025-01-05 13:25:18,568 - root - INFO - [Train] Epoch 1 [26800/35182]: Training loss 0.5272352695465088, current lr 9.912192660811233e-05, contrastive loss 3.5448288917541504, task loss 0.49178698658943176
2025-01-05 13:26:59,151 - root - INFO - [Train] Epoch 1 [26900/35182]: Training loss 0.6217386722564697, current lr 9.912050538643019e-05, contrastive loss 3.5778489112854004, task loss 0.5859602093696594
2025-01-05 13:28:39,528 - root - INFO - [Train] Epoch 1 [27000/35182]: Training loss 0.5266298055648804, current lr 9.911908416474802e-05, contrastive loss 3.414424180984497, task loss 0.49248555302619934
2025-01-05 13:30:20,119 - root - INFO - [Train] Epoch 1 [27100/35182]: Training loss 0.5290718078613281, current lr 9.911766294306586e-05, contrastive loss 3.444645404815674, task loss 0.49462535977363586
2025-01-05 13:32:00,480 - root - INFO - [Train] Epoch 1 [27200/35182]: Training loss 0.5963295698165894, current lr 9.91162417213837e-05, contrastive loss 3.4515457153320312, task loss 0.5618141293525696
2025-01-05 13:33:41,073 - root - INFO - [Train] Epoch 1 [27300/35182]: Training loss 0.5570144057273865, current lr 9.911482049970155e-05, contrastive loss 3.644474506378174, task loss 0.5205696821212769
2025-01-05 13:35:21,441 - root - INFO - [Train] Epoch 1 [27400/35182]: Training loss 0.5397840738296509, current lr 9.911339927801939e-05, contrastive loss 3.2555994987487793, task loss 0.5072280764579773
2025-01-05 13:37:02,041 - root - INFO - [Train] Epoch 1 [27500/35182]: Training loss 0.45004400610923767, current lr 9.911197805633723e-05, contrastive loss 3.4645934104919434, task loss 0.4153980612754822
2025-01-05 13:38:42,435 - root - INFO - [Train] Epoch 1 [27600/35182]: Training loss 0.5550028681755066, current lr 9.911055683465508e-05, contrastive loss 3.5463714599609375, task loss 0.5195391774177551
2025-01-05 13:40:23,014 - root - INFO - [Train] Epoch 1 [27700/35182]: Training loss 0.5032128691673279, current lr 9.910913561297292e-05, contrastive loss 3.457423210144043, task loss 0.46863865852355957
2025-01-05 13:42:03,399 - root - INFO - [Train] Epoch 1 [27800/35182]: Training loss 0.5702168941497803, current lr 9.910771439129076e-05, contrastive loss 3.3290390968322754, task loss 0.5369265079498291
2025-01-05 13:43:43,974 - root - INFO - [Train] Epoch 1 [27900/35182]: Training loss 0.46018198132514954, current lr 9.91062931696086e-05, contrastive loss 3.557711124420166, task loss 0.4246048629283905
2025-01-05 13:45:24,367 - root - INFO - [Train] Epoch 1 [28000/35182]: Training loss 0.46710261702537537, current lr 9.910487194792645e-05, contrastive loss 3.4107508659362793, task loss 0.43299511075019836
2025-01-05 13:47:04,951 - root - INFO - [Train] Epoch 1 [28100/35182]: Training loss 0.529452383518219, current lr 9.910345072624428e-05, contrastive loss 3.569037914276123, task loss 0.49376198649406433
2025-01-05 13:48:45,328 - root - INFO - [Train] Epoch 1 [28200/35182]: Training loss 0.5428923964500427, current lr 9.910202950456212e-05, contrastive loss 3.5376126766204834, task loss 0.5075162649154663
2025-01-05 13:50:25,908 - root - INFO - [Train] Epoch 1 [28300/35182]: Training loss 0.46630972623825073, current lr 9.910060828287997e-05, contrastive loss 3.589726209640503, task loss 0.4304124712944031
2025-01-05 13:52:06,314 - root - INFO - [Train] Epoch 1 [28400/35182]: Training loss 0.5533772706985474, current lr 9.909918706119781e-05, contrastive loss 3.4512133598327637, task loss 0.5188651084899902
2025-01-05 13:53:46,983 - root - INFO - [Train] Epoch 1 [28500/35182]: Training loss 0.4530954957008362, current lr 9.909776583951565e-05, contrastive loss 3.7251009941101074, task loss 0.41584450006484985
2025-01-05 13:55:27,350 - root - INFO - [Train] Epoch 1 [28600/35182]: Training loss 0.6280742287635803, current lr 9.90963446178335e-05, contrastive loss 3.575815200805664, task loss 0.5923160910606384
2025-01-05 13:57:07,944 - root - INFO - [Train] Epoch 1 [28700/35182]: Training loss 0.5213805437088013, current lr 9.909492339615134e-05, contrastive loss 3.3762407302856445, task loss 0.4876181483268738
2025-01-05 13:58:48,321 - root - INFO - [Train] Epoch 1 [28800/35182]: Training loss 0.6012963652610779, current lr 9.909350217446917e-05, contrastive loss 3.448556900024414, task loss 0.5668107867240906
2025-01-05 14:00:28,922 - root - INFO - [Train] Epoch 1 [28900/35182]: Training loss 0.5778329968452454, current lr 9.909208095278703e-05, contrastive loss 3.4510860443115234, task loss 0.5433221459388733
2025-01-05 14:02:09,287 - root - INFO - [Train] Epoch 1 [29000/35182]: Training loss 0.5071264505386353, current lr 9.909065973110486e-05, contrastive loss 3.433152675628662, task loss 0.47279492020606995
2025-01-05 14:03:49,878 - root - INFO - [Train] Epoch 1 [29100/35182]: Training loss 0.5505457520484924, current lr 9.90892385094227e-05, contrastive loss 3.568171262741089, task loss 0.5148640275001526
2025-01-05 14:05:30,259 - root - INFO - [Train] Epoch 1 [29200/35182]: Training loss 0.5718654990196228, current lr 9.908781728774054e-05, contrastive loss 3.526287078857422, task loss 0.5366026163101196
2025-01-05 14:07:10,844 - root - INFO - [Train] Epoch 1 [29300/35182]: Training loss 0.5084788799285889, current lr 9.908639606605839e-05, contrastive loss 3.494919776916504, task loss 0.4735296964645386
2025-01-05 14:08:51,228 - root - INFO - [Train] Epoch 1 [29400/35182]: Training loss 0.5902038216590881, current lr 9.908497484437623e-05, contrastive loss 3.4594054222106934, task loss 0.5556097626686096
2025-01-05 14:10:31,814 - root - INFO - [Train] Epoch 1 [29500/35182]: Training loss 0.48607495427131653, current lr 9.908355362269407e-05, contrastive loss 3.441434621810913, task loss 0.4516606032848358
2025-01-05 14:12:12,189 - root - INFO - [Train] Epoch 1 [29600/35182]: Training loss 0.6023061275482178, current lr 9.908213240101192e-05, contrastive loss 3.349574565887451, task loss 0.5688104033470154
2025-01-05 14:13:52,553 - root - INFO - [Train] Epoch 1 [29700/35182]: Training loss 0.5606215000152588, current lr 9.908071117932975e-05, contrastive loss 3.3716323375701904, task loss 0.5269051790237427
2025-01-05 14:15:33,148 - root - INFO - [Train] Epoch 1 [29800/35182]: Training loss 0.5533396601676941, current lr 9.90792899576476e-05, contrastive loss 3.5551815032958984, task loss 0.5177878737449646
2025-01-05 14:17:13,507 - root - INFO - [Train] Epoch 1 [29900/35182]: Training loss 0.6064252853393555, current lr 9.907786873596543e-05, contrastive loss 3.6370880603790283, task loss 0.5700544118881226
2025-01-05 14:18:54,129 - root - INFO - [Train] Epoch 1 [30000/35182]: Training loss 0.46806442737579346, current lr 9.907644751428329e-05, contrastive loss 3.3662948608398438, task loss 0.4344014823436737
2025-01-05 14:20:34,498 - root - INFO - [Train] Epoch 1 [30100/35182]: Training loss 0.5361121892929077, current lr 9.907502629260112e-05, contrastive loss 3.380331516265869, task loss 0.5023088455200195
2025-01-05 14:22:15,086 - root - INFO - [Train] Epoch 1 [30200/35182]: Training loss 0.5345515608787537, current lr 9.907360507091897e-05, contrastive loss 3.5539591312408447, task loss 0.4990119934082031
2025-01-05 14:23:55,465 - root - INFO - [Train] Epoch 1 [30300/35182]: Training loss 0.4992760419845581, current lr 9.907218384923681e-05, contrastive loss 3.4708361625671387, task loss 0.4645676910877228
2025-01-05 14:25:36,049 - root - INFO - [Train] Epoch 1 [30400/35182]: Training loss 0.5504696369171143, current lr 9.907076262755465e-05, contrastive loss 3.4917101860046387, task loss 0.5155525207519531
2025-01-05 14:27:16,432 - root - INFO - [Train] Epoch 1 [30500/35182]: Training loss 0.555006742477417, current lr 9.90693414058725e-05, contrastive loss 3.368162155151367, task loss 0.5213251113891602
2025-01-05 14:28:57,017 - root - INFO - [Train] Epoch 1 [30600/35182]: Training loss 0.5657912492752075, current lr 9.906792018419034e-05, contrastive loss 3.632861614227295, task loss 0.5294626355171204
2025-01-05 14:30:37,388 - root - INFO - [Train] Epoch 1 [30700/35182]: Training loss 0.5609031915664673, current lr 9.906649896250818e-05, contrastive loss 3.3429999351501465, task loss 0.5274732112884521
2025-01-05 14:32:17,979 - root - INFO - [Train] Epoch 1 [30800/35182]: Training loss 0.5220749974250793, current lr 9.906507774082601e-05, contrastive loss 3.597698211669922, task loss 0.4860979914665222
2025-01-05 14:33:58,359 - root - INFO - [Train] Epoch 1 [30900/35182]: Training loss 0.49486324191093445, current lr 9.906365651914387e-05, contrastive loss 3.4477782249450684, task loss 0.4603854715824127
2025-01-05 14:35:38,957 - root - INFO - [Train] Epoch 1 [31000/35182]: Training loss 0.5857133269309998, current lr 9.90622352974617e-05, contrastive loss 3.6187992095947266, task loss 0.5495253205299377
2025-01-05 14:37:19,329 - root - INFO - [Train] Epoch 1 [31100/35182]: Training loss 0.5401617288589478, current lr 9.906081407577954e-05, contrastive loss 3.321173667907715, task loss 0.5069500207901001
2025-01-05 14:38:59,922 - root - INFO - [Train] Epoch 1 [31200/35182]: Training loss 0.5000534057617188, current lr 9.905939285409739e-05, contrastive loss 3.361140727996826, task loss 0.4664420187473297
2025-01-05 14:40:40,291 - root - INFO - [Train] Epoch 1 [31300/35182]: Training loss 0.5795032978057861, current lr 9.905797163241523e-05, contrastive loss 3.465013027191162, task loss 0.544853150844574
2025-01-05 14:42:20,878 - root - INFO - [Train] Epoch 1 [31400/35182]: Training loss 0.5423606038093567, current lr 9.905655041073307e-05, contrastive loss 3.364098072052002, task loss 0.5087196230888367
2025-01-05 14:44:01,260 - root - INFO - [Train] Epoch 1 [31500/35182]: Training loss 0.5920920372009277, current lr 9.905512918905092e-05, contrastive loss 3.5327348709106445, task loss 0.5567646622657776
2025-01-05 14:45:41,854 - root - INFO - [Train] Epoch 1 [31600/35182]: Training loss 0.5478575825691223, current lr 9.905370796736876e-05, contrastive loss 3.3861069679260254, task loss 0.5139965415000916
2025-01-05 14:47:22,215 - root - INFO - [Train] Epoch 1 [31700/35182]: Training loss 0.5576441884040833, current lr 9.905228674568659e-05, contrastive loss 3.4527926445007324, task loss 0.5231162905693054
2025-01-05 14:49:02,821 - root - INFO - [Train] Epoch 1 [31800/35182]: Training loss 0.5161893963813782, current lr 9.905086552400445e-05, contrastive loss 3.399271249771118, task loss 0.4821966588497162
2025-01-05 14:50:43,189 - root - INFO - [Train] Epoch 1 [31900/35182]: Training loss 0.536188542842865, current lr 9.904944430232228e-05, contrastive loss 3.535975456237793, task loss 0.5008288025856018
2025-01-05 14:52:23,548 - root - INFO - [Train] Epoch 1 [32000/35182]: Training loss 0.5187512636184692, current lr 9.904802308064012e-05, contrastive loss 3.3691954612731934, task loss 0.48505929112434387
2025-01-05 14:54:04,172 - root - INFO - [Train] Epoch 1 [32100/35182]: Training loss 0.5141312479972839, current lr 9.904660185895796e-05, contrastive loss 3.4412877559661865, task loss 0.4797183573246002
2025-01-05 14:55:44,535 - root - INFO - [Train] Epoch 1 [32200/35182]: Training loss 0.5424017310142517, current lr 9.90451806372758e-05, contrastive loss 3.547537326812744, task loss 0.5069263577461243
2025-01-05 14:57:25,122 - root - INFO - [Train] Epoch 1 [32300/35182]: Training loss 0.5057811141014099, current lr 9.904375941559365e-05, contrastive loss 3.381816864013672, task loss 0.47196292877197266
2025-01-05 14:59:05,481 - root - INFO - [Train] Epoch 1 [32400/35182]: Training loss 0.5851569771766663, current lr 9.904233819391149e-05, contrastive loss 3.52805757522583, task loss 0.5498763918876648
2025-01-05 15:00:46,080 - root - INFO - [Train] Epoch 1 [32500/35182]: Training loss 0.5286543369293213, current lr 9.904091697222934e-05, contrastive loss 3.551168918609619, task loss 0.49314266443252563
2025-01-05 15:02:26,452 - root - INFO - [Train] Epoch 1 [32600/35182]: Training loss 0.5707890391349792, current lr 9.903949575054718e-05, contrastive loss 3.5648553371429443, task loss 0.5351405143737793
2025-01-05 15:04:07,048 - root - INFO - [Train] Epoch 1 [32700/35182]: Training loss 0.5014322400093079, current lr 9.903807452886502e-05, contrastive loss 3.306276559829712, task loss 0.4683694839477539
2025-01-05 15:05:47,420 - root - INFO - [Train] Epoch 1 [32800/35182]: Training loss 0.5909642577171326, current lr 9.903665330718285e-05, contrastive loss 3.44126033782959, task loss 0.5565516352653503
2025-01-05 15:07:28,015 - root - INFO - [Train] Epoch 1 [32900/35182]: Training loss 0.4993751347064972, current lr 9.903523208550071e-05, contrastive loss 3.3282785415649414, task loss 0.4660923480987549
2025-01-05 15:09:08,385 - root - INFO - [Train] Epoch 1 [33000/35182]: Training loss 0.5209911465644836, current lr 9.903381086381854e-05, contrastive loss 3.359710216522217, task loss 0.4873940348625183
2025-01-05 15:10:48,984 - root - INFO - [Train] Epoch 1 [33100/35182]: Training loss 0.4982546269893646, current lr 9.903238964213638e-05, contrastive loss 3.3710975646972656, task loss 0.4645436406135559
2025-01-05 15:12:29,355 - root - INFO - [Train] Epoch 1 [33200/35182]: Training loss 0.4904721677303314, current lr 9.903096842045423e-05, contrastive loss 3.326509714126587, task loss 0.4572070837020874
2025-01-05 15:14:09,960 - root - INFO - [Train] Epoch 1 [33300/35182]: Training loss 0.5547415018081665, current lr 9.902954719877207e-05, contrastive loss 3.440002918243408, task loss 0.5203414559364319
2025-01-05 15:15:50,361 - root - INFO - [Train] Epoch 1 [33400/35182]: Training loss 0.4727260172367096, current lr 9.902812597708991e-05, contrastive loss 3.297309398651123, task loss 0.4397529363632202
2025-01-05 15:17:30,929 - root - INFO - [Train] Epoch 1 [33500/35182]: Training loss 0.5013858675956726, current lr 9.902670475540776e-05, contrastive loss 3.5134944915771484, task loss 0.4662509262561798
2025-01-05 15:19:11,312 - root - INFO - [Train] Epoch 1 [33600/35182]: Training loss 0.5227606296539307, current lr 9.90252835337256e-05, contrastive loss 3.4938511848449707, task loss 0.48782214522361755
2025-01-05 15:20:51,897 - root - INFO - [Train] Epoch 1 [33700/35182]: Training loss 0.5950073599815369, current lr 9.902386231204343e-05, contrastive loss 3.393608570098877, task loss 0.5610712766647339
2025-01-05 15:22:32,279 - root - INFO - [Train] Epoch 1 [33800/35182]: Training loss 0.5409393906593323, current lr 9.902244109036129e-05, contrastive loss 3.437697649002075, task loss 0.5065624117851257
2025-01-05 15:24:12,852 - root - INFO - [Train] Epoch 1 [33900/35182]: Training loss 0.4515989422798157, current lr 9.902101986867912e-05, contrastive loss 3.4402434825897217, task loss 0.41719651222229004
2025-01-05 15:25:53,226 - root - INFO - [Train] Epoch 1 [34000/35182]: Training loss 0.493251234292984, current lr 9.901959864699696e-05, contrastive loss 3.411083698272705, task loss 0.4591403901576996
2025-01-05 15:27:33,823 - root - INFO - [Train] Epoch 1 [34100/35182]: Training loss 0.5191264152526855, current lr 9.90181774253148e-05, contrastive loss 3.3491973876953125, task loss 0.485634446144104
2025-01-05 15:29:14,191 - root - INFO - [Train] Epoch 1 [34200/35182]: Training loss 0.5201661586761475, current lr 9.901675620363265e-05, contrastive loss 3.3445065021514893, task loss 0.48672106862068176
2025-01-05 15:30:54,562 - root - INFO - [Train] Epoch 1 [34300/35182]: Training loss 0.542002260684967, current lr 9.901533498195049e-05, contrastive loss 3.5272295475006104, task loss 0.5067299604415894
2025-01-05 15:32:35,164 - root - INFO - [Train] Epoch 1 [34400/35182]: Training loss 0.5735189914703369, current lr 9.901391376026833e-05, contrastive loss 3.3015260696411133, task loss 0.540503740310669
2025-01-05 15:34:15,552 - root - INFO - [Train] Epoch 1 [34500/35182]: Training loss 0.6264333724975586, current lr 9.901249253858618e-05, contrastive loss 3.4772212505340576, task loss 0.5916611552238464
2025-01-05 15:35:56,125 - root - INFO - [Train] Epoch 1 [34600/35182]: Training loss 0.5353943109512329, current lr 9.901107131690402e-05, contrastive loss 3.3891282081604004, task loss 0.501503050327301
2025-01-05 15:37:36,514 - root - INFO - [Train] Epoch 1 [34700/35182]: Training loss 0.4896150529384613, current lr 9.900965009522186e-05, contrastive loss 3.186309576034546, task loss 0.45775195956230164
2025-01-05 15:39:17,106 - root - INFO - [Train] Epoch 1 [34800/35182]: Training loss 0.5568680763244629, current lr 9.900822887353969e-05, contrastive loss 3.4652388095855713, task loss 0.5222156643867493
2025-01-05 15:40:57,478 - root - INFO - [Train] Epoch 1 [34900/35182]: Training loss 0.5620235800743103, current lr 9.900680765185754e-05, contrastive loss 3.3648149967193604, task loss 0.5283754467964172
2025-01-05 15:42:38,065 - root - INFO - [Train] Epoch 1 [35000/35182]: Training loss 0.5474668145179749, current lr 9.900538643017538e-05, contrastive loss 3.316277027130127, task loss 0.5143040418624878
2025-01-05 15:44:18,429 - root - INFO - [Train] Epoch 1 [35100/35182]: Training loss 0.5684299468994141, current lr 9.900396520849322e-05, contrastive loss 3.309284210205078, task loss 0.5353370904922485
2025-01-05 15:45:42,714 - root - INFO - [Train] Epoch 1: Overall Training loss 0.0
2025-01-05 15:45:42,716 - root - INFO - [Time] Epoch 1: Time taken 9.82 hours
2025-01-05 15:45:47,783 - root - INFO - [Train] Epoch 2 [0/35182]: Training loss 0.5364336371421814, current lr 9.900279980671386e-05, contrastive loss 3.3774242401123047, task loss 0.5026593804359436
2025-01-05 15:47:28,151 - root - INFO - [Train] Epoch 2 [100/35182]: Training loss 0.5526028275489807, current lr 9.90013785850317e-05, contrastive loss 3.6864497661590576, task loss 0.515738308429718
2025-01-05 15:49:08,524 - root - INFO - [Train] Epoch 2 [200/35182]: Training loss 0.5480747818946838, current lr 9.899995736334954e-05, contrastive loss 3.3662660121917725, task loss 0.5144121050834656
2025-01-05 15:50:49,098 - root - INFO - [Train] Epoch 2 [300/35182]: Training loss 0.5443515181541443, current lr 9.899853614166739e-05, contrastive loss 3.5178349018096924, task loss 0.5091731548309326
2025-01-05 15:52:29,495 - root - INFO - [Train] Epoch 2 [400/35182]: Training loss 0.5289921164512634, current lr 9.899711491998522e-05, contrastive loss 3.429002046585083, task loss 0.4947021007537842
2025-01-05 15:54:10,074 - root - INFO - [Train] Epoch 2 [500/35182]: Training loss 0.4988407492637634, current lr 9.899569369830307e-05, contrastive loss 3.392547130584717, task loss 0.46491527557373047
2025-01-05 15:55:50,463 - root - INFO - [Train] Epoch 2 [600/35182]: Training loss 0.5770707726478577, current lr 9.89942724766209e-05, contrastive loss 3.3675355911254883, task loss 0.5433954000473022
2025-01-05 15:57:31,030 - root - INFO - [Train] Epoch 2 [700/35182]: Training loss 0.5225006341934204, current lr 9.899285125493875e-05, contrastive loss 3.470025062561035, task loss 0.48780035972595215
2025-01-05 15:59:11,413 - root - INFO - [Train] Epoch 2 [800/35182]: Training loss 0.5973633527755737, current lr 9.899143003325659e-05, contrastive loss 3.3157219886779785, task loss 0.5642061233520508
2025-01-05 16:00:52,016 - root - INFO - [Train] Epoch 2 [900/35182]: Training loss 0.5486114025115967, current lr 9.899000881157443e-05, contrastive loss 3.4368581771850586, task loss 0.5142428278923035
2025-01-05 16:02:32,401 - root - INFO - [Train] Epoch 2 [1000/35182]: Training loss 0.5798738598823547, current lr 9.898858758989228e-05, contrastive loss 3.470219373703003, task loss 0.5451716780662537
2025-01-05 16:04:12,966 - root - INFO - [Train] Epoch 2 [1100/35182]: Training loss 0.46990612149238586, current lr 9.898716636821012e-05, contrastive loss 3.4313745498657227, task loss 0.435592383146286
2025-01-05 16:05:53,343 - root - INFO - [Train] Epoch 2 [1200/35182]: Training loss 0.5255247354507446, current lr 9.898574514652796e-05, contrastive loss 3.1328158378601074, task loss 0.4941965937614441
2025-01-05 16:07:33,937 - root - INFO - [Train] Epoch 2 [1300/35182]: Training loss 0.5282633900642395, current lr 9.89843239248458e-05, contrastive loss 3.356773853302002, task loss 0.49469563364982605
2025-01-05 16:09:14,321 - root - INFO - [Train] Epoch 2 [1400/35182]: Training loss 0.5492329597473145, current lr 9.898290270316365e-05, contrastive loss 3.342928171157837, task loss 0.5158036947250366
2025-01-05 16:10:54,896 - root - INFO - [Train] Epoch 2 [1500/35182]: Training loss 0.5618034601211548, current lr 9.898148148148148e-05, contrastive loss 3.3383777141571045, task loss 0.5284196734428406
2025-01-05 16:12:35,281 - root - INFO - [Train] Epoch 2 [1600/35182]: Training loss 0.5407806038856506, current lr 9.898006025979932e-05, contrastive loss 3.3152883052825928, task loss 0.5076277256011963
2025-01-05 16:14:15,866 - root - INFO - [Train] Epoch 2 [1700/35182]: Training loss 0.4634827971458435, current lr 9.897863903811717e-05, contrastive loss 3.3664212226867676, task loss 0.42981860041618347
2025-01-05 16:15:56,269 - root - INFO - [Train] Epoch 2 [1800/35182]: Training loss 0.5376319885253906, current lr 9.897721781643501e-05, contrastive loss 3.4831085205078125, task loss 0.5028008818626404
2025-01-05 16:17:36,925 - root - INFO - [Train] Epoch 2 [1900/35182]: Training loss 0.5098882913589478, current lr 9.897579659475286e-05, contrastive loss 3.453829288482666, task loss 0.4753499925136566
2025-01-05 16:19:17,307 - root - INFO - [Train] Epoch 2 [2000/35182]: Training loss 0.596025288105011, current lr 9.89743753730707e-05, contrastive loss 3.228954315185547, task loss 0.5637357234954834
2025-01-05 16:20:57,913 - root - INFO - [Train] Epoch 2 [2100/35182]: Training loss 0.4616071879863739, current lr 9.897295415138854e-05, contrastive loss 3.326693058013916, task loss 0.42834025621414185
2025-01-05 16:22:38,317 - root - INFO - [Train] Epoch 2 [2200/35182]: Training loss 0.549987256526947, current lr 9.897153292970639e-05, contrastive loss 3.3237037658691406, task loss 0.5167502164840698
2025-01-05 16:24:18,874 - root - INFO - [Train] Epoch 2 [2300/35182]: Training loss 0.5148226618766785, current lr 9.897011170802423e-05, contrastive loss 3.3953468799591064, task loss 0.4808691740036011
2025-01-05 16:25:59,248 - root - INFO - [Train] Epoch 2 [2400/35182]: Training loss 0.45958101749420166, current lr 9.896869048634206e-05, contrastive loss 3.4691100120544434, task loss 0.4248899221420288
2025-01-05 16:27:39,847 - root - INFO - [Train] Epoch 2 [2500/35182]: Training loss 0.6240420937538147, current lr 9.89672692646599e-05, contrastive loss 3.2495779991149902, task loss 0.5915462970733643
2025-01-05 16:29:20,211 - root - INFO - [Train] Epoch 2 [2600/35182]: Training loss 0.5064571499824524, current lr 9.896584804297775e-05, contrastive loss 3.379197120666504, task loss 0.4726651906967163
2025-01-05 16:31:00,809 - root - INFO - [Train] Epoch 2 [2700/35182]: Training loss 0.6312736868858337, current lr 9.896442682129559e-05, contrastive loss 3.215942621231079, task loss 0.5991142392158508
2025-01-05 16:32:41,175 - root - INFO - [Train] Epoch 2 [2800/35182]: Training loss 0.49054470658302307, current lr 9.896300559961343e-05, contrastive loss 3.3347392082214355, task loss 0.45719730854034424
2025-01-05 16:34:21,540 - root - INFO - [Train] Epoch 2 [2900/35182]: Training loss 0.532535970211029, current lr 9.896158437793128e-05, contrastive loss 3.227938413619995, task loss 0.5002565979957581
2025-01-05 16:36:02,140 - root - INFO - [Train] Epoch 2 [3000/35182]: Training loss 0.5690414905548096, current lr 9.896016315624912e-05, contrastive loss 3.4708991050720215, task loss 0.5343325138092041
2025-01-05 16:37:42,503 - root - INFO - [Train] Epoch 2 [3100/35182]: Training loss 0.5415914058685303, current lr 9.895874193456696e-05, contrastive loss 3.4117531776428223, task loss 0.507473886013031
2025-01-05 16:39:23,097 - root - INFO - [Train] Epoch 2 [3200/35182]: Training loss 0.5213353037834167, current lr 9.89573207128848e-05, contrastive loss 3.4954140186309814, task loss 0.4863811433315277
2025-01-05 16:41:03,462 - root - INFO - [Train] Epoch 2 [3300/35182]: Training loss 0.5355467796325684, current lr 9.895589949120264e-05, contrastive loss 3.4775218963623047, task loss 0.5007715821266174
2025-01-05 16:42:44,071 - root - INFO - [Train] Epoch 2 [3400/35182]: Training loss 0.5414084792137146, current lr 9.895447826952049e-05, contrastive loss 3.6562139987945557, task loss 0.5048463344573975
2025-01-05 16:44:24,437 - root - INFO - [Train] Epoch 2 [3500/35182]: Training loss 0.48415258526802063, current lr 9.895305704783832e-05, contrastive loss 3.467689275741577, task loss 0.4494757056236267
2025-01-05 16:46:05,041 - root - INFO - [Train] Epoch 2 [3600/35182]: Training loss 0.4799344539642334, current lr 9.895163582615617e-05, contrastive loss 3.4686131477355957, task loss 0.4452483355998993
2025-01-05 16:47:45,434 - root - INFO - [Train] Epoch 2 [3700/35182]: Training loss 0.46746307611465454, current lr 9.895021460447401e-05, contrastive loss 3.521444320678711, task loss 0.43224862217903137
2025-01-05 16:49:26,003 - root - INFO - [Train] Epoch 2 [3800/35182]: Training loss 0.5153486728668213, current lr 9.894879338279185e-05, contrastive loss 3.5622286796569824, task loss 0.4797263741493225
2025-01-05 16:51:06,387 - root - INFO - [Train] Epoch 2 [3900/35182]: Training loss 0.5828807950019836, current lr 9.89473721611097e-05, contrastive loss 3.329664707183838, task loss 0.549584150314331
2025-01-05 16:52:46,972 - root - INFO - [Train] Epoch 2 [4000/35182]: Training loss 0.6040478348731995, current lr 9.894595093942754e-05, contrastive loss 3.3728599548339844, task loss 0.5703192353248596
2025-01-05 16:54:27,363 - root - INFO - [Train] Epoch 2 [4100/35182]: Training loss 0.576129674911499, current lr 9.894452971774538e-05, contrastive loss 3.1991991996765137, task loss 0.5441376566886902
2025-01-05 16:56:07,951 - root - INFO - [Train] Epoch 2 [4200/35182]: Training loss 0.5958062410354614, current lr 9.894310849606323e-05, contrastive loss 3.483705759048462, task loss 0.5609691739082336
2025-01-05 16:57:48,310 - root - INFO - [Train] Epoch 2 [4300/35182]: Training loss 0.5469493865966797, current lr 9.894168727438107e-05, contrastive loss 3.316333293914795, task loss 0.5137860774993896
2025-01-05 16:59:28,906 - root - INFO - [Train] Epoch 2 [4400/35182]: Training loss 0.5365628004074097, current lr 9.89402660526989e-05, contrastive loss 3.1886253356933594, task loss 0.5046765208244324
2025-01-05 17:01:09,269 - root - INFO - [Train] Epoch 2 [4500/35182]: Training loss 0.45570042729377747, current lr 9.893884483101674e-05, contrastive loss 3.194146156311035, task loss 0.42375895380973816
2025-01-05 17:02:49,877 - root - INFO - [Train] Epoch 2 [4600/35182]: Training loss 0.5001737475395203, current lr 9.893742360933459e-05, contrastive loss 3.422877788543701, task loss 0.46594497561454773
2025-01-05 17:04:30,253 - root - INFO - [Train] Epoch 2 [4700/35182]: Training loss 0.5667542815208435, current lr 9.893600238765243e-05, contrastive loss 3.391148090362549, task loss 0.5328428149223328
2025-01-05 17:06:10,841 - root - INFO - [Train] Epoch 2 [4800/35182]: Training loss 0.538604199886322, current lr 9.893458116597027e-05, contrastive loss 3.498586654663086, task loss 0.5036183595657349
2025-01-05 17:07:51,204 - root - INFO - [Train] Epoch 2 [4900/35182]: Training loss 0.49299192428588867, current lr 9.893315994428812e-05, contrastive loss 3.37186861038208, task loss 0.45927324891090393
2025-01-05 17:09:31,810 - root - INFO - [Train] Epoch 2 [5000/35182]: Training loss 0.45794782042503357, current lr 9.893173872260596e-05, contrastive loss 3.404122829437256, task loss 0.4239065945148468
2025-01-05 17:11:12,188 - root - INFO - [Train] Epoch 2 [5100/35182]: Training loss 0.575857937335968, current lr 9.89303175009238e-05, contrastive loss 3.359133243560791, task loss 0.5422666072845459
2025-01-05 17:12:52,553 - root - INFO - [Train] Epoch 2 [5200/35182]: Training loss 0.41484495997428894, current lr 9.892889627924165e-05, contrastive loss 3.3064026832580566, task loss 0.3817809224128723
2025-01-05 17:14:33,158 - root - INFO - [Train] Epoch 2 [5300/35182]: Training loss 0.5198840498924255, current lr 9.892747505755948e-05, contrastive loss 3.284233570098877, task loss 0.48704174160957336
2025-01-05 17:16:13,528 - root - INFO - [Train] Epoch 2 [5400/35182]: Training loss 0.5138453841209412, current lr 9.892605383587733e-05, contrastive loss 3.4816601276397705, task loss 0.4790287911891937
2025-01-05 17:17:54,139 - root - INFO - [Train] Epoch 2 [5500/35182]: Training loss 0.5430353879928589, current lr 9.892463261419516e-05, contrastive loss 3.369504928588867, task loss 0.5093403458595276
2025-01-05 17:19:34,502 - root - INFO - [Train] Epoch 2 [5600/35182]: Training loss 0.5767005085945129, current lr 9.8923211392513e-05, contrastive loss 3.354248285293579, task loss 0.5431580543518066
2025-01-05 17:21:15,094 - root - INFO - [Train] Epoch 2 [5700/35182]: Training loss 0.575232982635498, current lr 9.892179017083085e-05, contrastive loss 3.3489794731140137, task loss 0.5417432188987732
2025-01-05 17:22:55,472 - root - INFO - [Train] Epoch 2 [5800/35182]: Training loss 0.48294565081596375, current lr 9.892036894914869e-05, contrastive loss 3.6036040782928467, task loss 0.4469096064567566
2025-01-05 17:24:36,053 - root - INFO - [Train] Epoch 2 [5900/35182]: Training loss 0.5627336502075195, current lr 9.891894772746654e-05, contrastive loss 3.4621596336364746, task loss 0.5281120538711548
2025-01-05 17:26:16,424 - root - INFO - [Train] Epoch 2 [6000/35182]: Training loss 0.6255250573158264, current lr 9.891752650578438e-05, contrastive loss 3.4108059406280518, task loss 0.5914170145988464
2025-01-05 17:27:57,015 - root - INFO - [Train] Epoch 2 [6100/35182]: Training loss 0.4942321479320526, current lr 9.891610528410222e-05, contrastive loss 3.590305805206299, task loss 0.45832908153533936
2025-01-05 17:29:37,401 - root - INFO - [Train] Epoch 2 [6200/35182]: Training loss 0.5033919215202332, current lr 9.891468406242007e-05, contrastive loss 3.307692527770996, task loss 0.47031500935554504
2025-01-05 17:31:17,984 - root - INFO - [Train] Epoch 2 [6300/35182]: Training loss 0.5655953288078308, current lr 9.891326284073791e-05, contrastive loss 3.326965808868408, task loss 0.5323256850242615
2025-01-05 17:32:58,354 - root - INFO - [Train] Epoch 2 [6400/35182]: Training loss 0.5320423245429993, current lr 9.891184161905574e-05, contrastive loss 3.5983448028564453, task loss 0.496058851480484
2025-01-05 17:34:38,945 - root - INFO - [Train] Epoch 2 [6500/35182]: Training loss 0.5050258636474609, current lr 9.891042039737358e-05, contrastive loss 3.2788853645324707, task loss 0.4722369909286499
2025-01-05 17:36:19,312 - root - INFO - [Train] Epoch 2 [6600/35182]: Training loss 0.5536138415336609, current lr 9.890899917569143e-05, contrastive loss 3.3021857738494873, task loss 0.5205919742584229
2025-01-05 17:37:59,940 - root - INFO - [Train] Epoch 2 [6700/35182]: Training loss 0.5827157497406006, current lr 9.890757795400927e-05, contrastive loss 3.314164638519287, task loss 0.549574077129364
2025-01-05 17:39:40,338 - root - INFO - [Train] Epoch 2 [6800/35182]: Training loss 0.534776508808136, current lr 9.890615673232711e-05, contrastive loss 3.29775333404541, task loss 0.5017989873886108
2025-01-05 17:41:21,008 - root - INFO - [Train] Epoch 2 [6900/35182]: Training loss 0.5212104916572571, current lr 9.890473551064496e-05, contrastive loss 3.419504165649414, task loss 0.48701542615890503
2025-01-05 17:43:01,392 - root - INFO - [Train] Epoch 2 [7000/35182]: Training loss 0.5605031251907349, current lr 9.89033142889628e-05, contrastive loss 3.302032947540283, task loss 0.527482807636261
2025-01-05 17:44:41,964 - root - INFO - [Train] Epoch 2 [7100/35182]: Training loss 0.47504785656929016, current lr 9.890189306728064e-05, contrastive loss 3.3263440132141113, task loss 0.44178441166877747
2025-01-05 17:46:22,346 - root - INFO - [Train] Epoch 2 [7200/35182]: Training loss 0.5836084485054016, current lr 9.890047184559849e-05, contrastive loss 3.362527370452881, task loss 0.5499832034111023
2025-01-05 17:48:02,934 - root - INFO - [Train] Epoch 2 [7300/35182]: Training loss 0.5054028630256653, current lr 9.889905062391632e-05, contrastive loss 3.407545566558838, task loss 0.47132739424705505
2025-01-05 17:49:43,331 - root - INFO - [Train] Epoch 2 [7400/35182]: Training loss 0.5622571706771851, current lr 9.889762940223416e-05, contrastive loss 3.595791816711426, task loss 0.526299238204956
2025-01-05 17:51:23,897 - root - INFO - [Train] Epoch 2 [7500/35182]: Training loss 0.5354961156845093, current lr 9.8896208180552e-05, contrastive loss 3.518975257873535, task loss 0.5003063678741455
2025-01-05 17:53:04,280 - root - INFO - [Train] Epoch 2 [7600/35182]: Training loss 0.6265596151351929, current lr 9.889478695886985e-05, contrastive loss 3.296099901199341, task loss 0.5935986042022705
2025-01-05 17:54:44,861 - root - INFO - [Train] Epoch 2 [7700/35182]: Training loss 0.5799004435539246, current lr 9.889336573718769e-05, contrastive loss 3.4533791542053223, task loss 0.545366644859314
2025-01-05 17:56:25,240 - root - INFO - [Train] Epoch 2 [7800/35182]: Training loss 0.4825782775878906, current lr 9.889194451550553e-05, contrastive loss 3.589818000793457, task loss 0.44668009877204895
2025-01-05 17:58:05,825 - root - INFO - [Train] Epoch 2 [7900/35182]: Training loss 0.5293137431144714, current lr 9.889052329382338e-05, contrastive loss 3.547480583190918, task loss 0.49383896589279175
2025-01-05 17:59:46,206 - root - INFO - [Train] Epoch 2 [8000/35182]: Training loss 0.5337308049201965, current lr 9.888910207214122e-05, contrastive loss 3.37003755569458, task loss 0.5000304579734802
2025-01-05 18:01:26,585 - root - INFO - [Train] Epoch 2 [8100/35182]: Training loss 0.5002138614654541, current lr 9.888768085045906e-05, contrastive loss 3.4228017330169678, task loss 0.46598586440086365
2025-01-05 18:03:07,188 - root - INFO - [Train] Epoch 2 [8200/35182]: Training loss 0.46523764729499817, current lr 9.888625962877691e-05, contrastive loss 3.4125609397888184, task loss 0.43111205101013184
2025-01-05 18:04:47,575 - root - INFO - [Train] Epoch 2 [8300/35182]: Training loss 0.5151183009147644, current lr 9.888483840709475e-05, contrastive loss 3.4747891426086426, task loss 0.480370432138443
2025-01-05 18:06:28,128 - root - INFO - [Train] Epoch 2 [8400/35182]: Training loss 0.5282266736030579, current lr 9.888341718541258e-05, contrastive loss 3.3767471313476562, task loss 0.4944591820240021
2025-01-05 18:08:08,505 - root - INFO - [Train] Epoch 2 [8500/35182]: Training loss 0.4995913505554199, current lr 9.888199596373042e-05, contrastive loss 3.4200234413146973, task loss 0.4653911292552948
2025-01-05 18:09:49,109 - root - INFO - [Train] Epoch 2 [8600/35182]: Training loss 0.5403296947479248, current lr 9.888057474204827e-05, contrastive loss 3.2751684188842773, task loss 0.5075780153274536
2025-01-05 18:11:29,487 - root - INFO - [Train] Epoch 2 [8700/35182]: Training loss 0.5181694626808167, current lr 9.887915352036611e-05, contrastive loss 3.281954765319824, task loss 0.4853498935699463
2025-01-05 18:13:10,070 - root - INFO - [Train] Epoch 2 [8800/35182]: Training loss 0.5048201680183411, current lr 9.887773229868395e-05, contrastive loss 3.4505364894866943, task loss 0.4703148305416107
2025-01-05 18:14:50,437 - root - INFO - [Train] Epoch 2 [8900/35182]: Training loss 0.48852217197418213, current lr 9.88763110770018e-05, contrastive loss 3.5148770809173584, task loss 0.45337340235710144
2025-01-05 18:16:31,026 - root - INFO - [Train] Epoch 2 [9000/35182]: Training loss 0.5265144109725952, current lr 9.887488985531964e-05, contrastive loss 3.2701187133789062, task loss 0.4938132166862488
2025-01-05 18:18:11,401 - root - INFO - [Train] Epoch 2 [9100/35182]: Training loss 0.49330630898475647, current lr 9.887346863363748e-05, contrastive loss 3.3232264518737793, task loss 0.4600740373134613
2025-01-05 18:19:51,991 - root - INFO - [Train] Epoch 2 [9200/35182]: Training loss 0.5411711931228638, current lr 9.887204741195533e-05, contrastive loss 3.4314074516296387, task loss 0.5068570971488953
2025-01-05 18:21:32,372 - root - INFO - [Train] Epoch 2 [9300/35182]: Training loss 0.5680489540100098, current lr 9.887062619027316e-05, contrastive loss 3.3406906127929688, task loss 0.5346420407295227
2025-01-05 18:23:12,959 - root - INFO - [Train] Epoch 2 [9400/35182]: Training loss 0.5047137141227722, current lr 9.8869204968591e-05, contrastive loss 3.2854442596435547, task loss 0.47185924649238586
2025-01-05 18:24:53,350 - root - INFO - [Train] Epoch 2 [9500/35182]: Training loss 0.4982267916202545, current lr 9.886778374690884e-05, contrastive loss 3.2592062950134277, task loss 0.4656347334384918
2025-01-05 18:26:33,932 - root - INFO - [Train] Epoch 2 [9600/35182]: Training loss 0.5218642950057983, current lr 9.886636252522669e-05, contrastive loss 3.0185885429382324, task loss 0.4916784167289734
2025-01-05 18:28:14,323 - root - INFO - [Train] Epoch 2 [9700/35182]: Training loss 0.5312612652778625, current lr 9.886494130354453e-05, contrastive loss 3.536496162414551, task loss 0.4958963096141815
2025-01-05 18:29:54,901 - root - INFO - [Train] Epoch 2 [9800/35182]: Training loss 0.5003464818000793, current lr 9.886352008186237e-05, contrastive loss 3.2595558166503906, task loss 0.4677509367465973
2025-01-05 18:31:35,291 - root - INFO - [Train] Epoch 2 [9900/35182]: Training loss 0.5411853194236755, current lr 9.886209886018022e-05, contrastive loss 3.301616907119751, task loss 0.5081691741943359
2025-01-05 18:33:15,866 - root - INFO - [Train] Epoch 2 [10000/35182]: Training loss 0.44274166226387024, current lr 9.886067763849806e-05, contrastive loss 3.3958163261413574, task loss 0.408783495426178
2025-01-05 18:34:56,255 - root - INFO - [Train] Epoch 2 [10100/35182]: Training loss 0.47871556878089905, current lr 9.88592564168159e-05, contrastive loss 3.4175167083740234, task loss 0.444540411233902
2025-01-05 18:36:36,838 - root - INFO - [Train] Epoch 2 [10200/35182]: Training loss 0.580981433391571, current lr 9.885783519513373e-05, contrastive loss 3.2748517990112305, task loss 0.548232913017273
2025-01-05 18:38:17,216 - root - INFO - [Train] Epoch 2 [10300/35182]: Training loss 0.5107583403587341, current lr 9.885641397345158e-05, contrastive loss 3.1614346504211426, task loss 0.47914397716522217
2025-01-05 18:39:57,795 - root - INFO - [Train] Epoch 2 [10400/35182]: Training loss 0.5292537808418274, current lr 9.885499275176942e-05, contrastive loss 3.4537429809570312, task loss 0.4947163462638855
2025-01-05 18:41:38,180 - root - INFO - [Train] Epoch 2 [10500/35182]: Training loss 0.5444719791412354, current lr 9.885357153008726e-05, contrastive loss 3.487826347351074, task loss 0.5095937252044678
2025-01-05 18:43:18,543 - root - INFO - [Train] Epoch 2 [10600/35182]: Training loss 0.5977287292480469, current lr 9.885215030840511e-05, contrastive loss 3.321176052093506, task loss 0.5645169615745544
2025-01-05 18:44:59,128 - root - INFO - [Train] Epoch 2 [10700/35182]: Training loss 0.5758158564567566, current lr 9.885072908672295e-05, contrastive loss 3.314297914505005, task loss 0.542672872543335
2025-01-05 18:46:39,492 - root - INFO - [Train] Epoch 2 [10800/35182]: Training loss 0.5528770685195923, current lr 9.88493078650408e-05, contrastive loss 3.2478041648864746, task loss 0.5203990340232849
2025-01-05 18:48:20,105 - root - INFO - [Train] Epoch 2 [10900/35182]: Training loss 0.5751377940177917, current lr 9.884788664335864e-05, contrastive loss 3.3647518157958984, task loss 0.5414902567863464
2025-01-05 18:50:00,486 - root - INFO - [Train] Epoch 2 [11000/35182]: Training loss 0.5227054357528687, current lr 9.884646542167648e-05, contrastive loss 3.5050318241119385, task loss 0.4876551330089569
2025-01-05 18:51:41,079 - root - INFO - [Train] Epoch 2 [11100/35182]: Training loss 0.4836951792240143, current lr 9.884504419999432e-05, contrastive loss 3.4784350395202637, task loss 0.44891083240509033
2025-01-05 18:53:21,459 - root - INFO - [Train] Epoch 2 [11200/35182]: Training loss 0.5488516092300415, current lr 9.884362297831217e-05, contrastive loss 3.366173028945923, task loss 0.5151898860931396
2025-01-05 18:55:02,051 - root - INFO - [Train] Epoch 2 [11300/35182]: Training loss 0.5081079006195068, current lr 9.884220175663e-05, contrastive loss 3.445096015930176, task loss 0.47365695238113403
2025-01-05 18:56:42,463 - root - INFO - [Train] Epoch 2 [11400/35182]: Training loss 0.49140360951423645, current lr 9.884078053494784e-05, contrastive loss 3.2170562744140625, task loss 0.45923304557800293
2025-01-05 18:58:23,115 - root - INFO - [Train] Epoch 2 [11500/35182]: Training loss 0.5072047114372253, current lr 9.883935931326568e-05, contrastive loss 3.408599376678467, task loss 0.47311869263648987
2025-01-05 19:00:03,492 - root - INFO - [Train] Epoch 2 [11600/35182]: Training loss 0.5229451656341553, current lr 9.883793809158353e-05, contrastive loss 3.164295196533203, task loss 0.4913021922111511
2025-01-05 19:01:44,089 - root - INFO - [Train] Epoch 2 [11700/35182]: Training loss 0.5749087333679199, current lr 9.883651686990137e-05, contrastive loss 3.3419106006622314, task loss 0.5414896011352539
2025-01-05 19:03:24,467 - root - INFO - [Train] Epoch 2 [11800/35182]: Training loss 0.5628290176391602, current lr 9.883509564821922e-05, contrastive loss 3.3249921798706055, task loss 0.5295791029930115
2025-01-05 19:05:05,043 - root - INFO - [Train] Epoch 2 [11900/35182]: Training loss 0.4952770471572876, current lr 9.883367442653706e-05, contrastive loss 3.4152393341064453, task loss 0.4611246585845947
2025-01-05 19:06:45,398 - root - INFO - [Train] Epoch 2 [12000/35182]: Training loss 0.5217002630233765, current lr 9.88322532048549e-05, contrastive loss 3.376589775085449, task loss 0.4879343509674072
2025-01-05 19:08:26,005 - root - INFO - [Train] Epoch 2 [12100/35182]: Training loss 0.5373719334602356, current lr 9.883083198317275e-05, contrastive loss 3.4167253971099854, task loss 0.5032047033309937
2025-01-05 19:10:06,377 - root - INFO - [Train] Epoch 2 [12200/35182]: Training loss 0.5097761154174805, current lr 9.882941076149058e-05, contrastive loss 3.5377984046936035, task loss 0.47439810633659363
2025-01-05 19:11:46,981 - root - INFO - [Train] Epoch 2 [12300/35182]: Training loss 0.5672029256820679, current lr 9.882798953980842e-05, contrastive loss 3.2661309242248535, task loss 0.5345416069030762
2025-01-05 19:13:27,373 - root - INFO - [Train] Epoch 2 [12400/35182]: Training loss 0.5081889629364014, current lr 9.882656831812626e-05, contrastive loss 3.4587442874908447, task loss 0.4736015200614929
2025-01-05 19:15:07,944 - root - INFO - [Train] Epoch 2 [12500/35182]: Training loss 0.47427839040756226, current lr 9.88251470964441e-05, contrastive loss 3.147860050201416, task loss 0.44279980659484863
2025-01-05 19:16:48,327 - root - INFO - [Train] Epoch 2 [12600/35182]: Training loss 0.48721134662628174, current lr 9.882372587476195e-05, contrastive loss 3.3774752616882324, task loss 0.45343658328056335
2025-01-05 19:18:28,910 - root - INFO - [Train] Epoch 2 [12700/35182]: Training loss 0.5314239263534546, current lr 9.882230465307979e-05, contrastive loss 3.139791965484619, task loss 0.5000259876251221
2025-01-05 19:20:09,282 - root - INFO - [Train] Epoch 2 [12800/35182]: Training loss 0.6200048327445984, current lr 9.882088343139764e-05, contrastive loss 3.4907095432281494, task loss 0.5850977301597595
2025-01-05 19:21:49,876 - root - INFO - [Train] Epoch 2 [12900/35182]: Training loss 0.5467967391014099, current lr 9.881946220971548e-05, contrastive loss 3.2018685340881348, task loss 0.5147780776023865
2025-01-05 19:23:30,270 - root - INFO - [Train] Epoch 2 [13000/35182]: Training loss 0.6313999891281128, current lr 9.881804098803332e-05, contrastive loss 3.361269950866699, task loss 0.5977872610092163
2025-01-05 19:25:10,847 - root - INFO - [Train] Epoch 2 [13100/35182]: Training loss 0.5268652439117432, current lr 9.881661976635117e-05, contrastive loss 3.2910358905792236, task loss 0.4939548969268799
2025-01-05 19:26:51,223 - root - INFO - [Train] Epoch 2 [13200/35182]: Training loss 0.4507942199707031, current lr 9.881519854466901e-05, contrastive loss 3.313838005065918, task loss 0.4176558256149292
2025-01-05 19:28:31,812 - root - INFO - [Train] Epoch 2 [13300/35182]: Training loss 0.6232320070266724, current lr 9.881377732298684e-05, contrastive loss 3.435370922088623, task loss 0.5888782739639282
2025-01-05 19:30:12,195 - root - INFO - [Train] Epoch 2 [13400/35182]: Training loss 0.48421069979667664, current lr 9.881235610130468e-05, contrastive loss 3.5374183654785156, task loss 0.4488365054130554
2025-01-05 19:31:52,563 - root - INFO - [Train] Epoch 2 [13500/35182]: Training loss 0.5604696869850159, current lr 9.881093487962253e-05, contrastive loss 3.263849973678589, task loss 0.5278311967849731
2025-01-05 19:33:33,149 - root - INFO - [Train] Epoch 2 [13600/35182]: Training loss 0.5698126554489136, current lr 9.880951365794037e-05, contrastive loss 3.505629539489746, task loss 0.5347563624382019
2025-01-05 19:35:13,528 - root - INFO - [Train] Epoch 2 [13700/35182]: Training loss 0.5145682692527771, current lr 9.880809243625821e-05, contrastive loss 3.352972984313965, task loss 0.48103854060173035
2025-01-05 19:36:54,116 - root - INFO - [Train] Epoch 2 [13800/35182]: Training loss 0.4776492118835449, current lr 9.880667121457606e-05, contrastive loss 3.3398547172546387, task loss 0.4442506730556488
2025-01-05 19:38:34,490 - root - INFO - [Train] Epoch 2 [13900/35182]: Training loss 0.5546683669090271, current lr 9.88052499928939e-05, contrastive loss 3.316087245941162, task loss 0.5215075016021729
2025-01-05 19:40:15,109 - root - INFO - [Train] Epoch 2 [14000/35182]: Training loss 0.5714616775512695, current lr 9.880382877121174e-05, contrastive loss 3.5479202270507812, task loss 0.5359824895858765
2025-01-05 19:41:55,484 - root - INFO - [Train] Epoch 2 [14100/35182]: Training loss 0.5470878481864929, current lr 9.880240754952959e-05, contrastive loss 3.1706295013427734, task loss 0.5153815746307373
2025-01-05 19:43:36,056 - root - INFO - [Train] Epoch 2 [14200/35182]: Training loss 0.5410787463188171, current lr 9.880098632784742e-05, contrastive loss 3.2726893424987793, task loss 0.5083518624305725
2025-01-05 19:45:16,428 - root - INFO - [Train] Epoch 2 [14300/35182]: Training loss 0.4995388984680176, current lr 9.879956510616526e-05, contrastive loss 3.6070213317871094, task loss 0.46346867084503174
2025-01-05 19:46:57,017 - root - INFO - [Train] Epoch 2 [14400/35182]: Training loss 0.5685028433799744, current lr 9.87981438844831e-05, contrastive loss 3.2905097007751465, task loss 0.5355977416038513
2025-01-05 19:48:37,403 - root - INFO - [Train] Epoch 2 [14500/35182]: Training loss 0.5352034568786621, current lr 9.879672266280095e-05, contrastive loss 3.414505958557129, task loss 0.5010583996772766
2025-01-05 19:50:17,978 - root - INFO - [Train] Epoch 2 [14600/35182]: Training loss 0.5079343914985657, current lr 9.879530144111879e-05, contrastive loss 3.5141985416412354, task loss 0.472792387008667
2025-01-05 19:51:58,353 - root - INFO - [Train] Epoch 2 [14700/35182]: Training loss 0.46451428532600403, current lr 9.879388021943663e-05, contrastive loss 3.314220428466797, task loss 0.4313720762729645
2025-01-05 19:53:38,960 - root - INFO - [Train] Epoch 2 [14800/35182]: Training loss 0.6104323863983154, current lr 9.879245899775448e-05, contrastive loss 3.248929500579834, task loss 0.5779430866241455
2025-01-05 19:55:19,332 - root - INFO - [Train] Epoch 2 [14900/35182]: Training loss 0.6390340328216553, current lr 9.879103777607232e-05, contrastive loss 3.520819902420044, task loss 0.6038258075714111
2025-01-05 19:56:59,924 - root - INFO - [Train] Epoch 2 [15000/35182]: Training loss 0.48482590913772583, current lr 9.878961655439016e-05, contrastive loss 3.324185848236084, task loss 0.4515840411186218
2025-01-05 19:58:40,303 - root - INFO - [Train] Epoch 2 [15100/35182]: Training loss 0.5277652740478516, current lr 9.8788195332708e-05, contrastive loss 3.263472557067871, task loss 0.4951305389404297
2025-01-05 20:00:20,882 - root - INFO - [Train] Epoch 2 [15200/35182]: Training loss 0.5355470180511475, current lr 9.878677411102584e-05, contrastive loss 3.3995189666748047, task loss 0.5015518069267273
2025-01-05 20:02:01,267 - root - INFO - [Train] Epoch 2 [15300/35182]: Training loss 0.5341058373451233, current lr 9.878535288934368e-05, contrastive loss 3.398110866546631, task loss 0.5001247525215149
2025-01-05 20:03:41,848 - root - INFO - [Train] Epoch 2 [15400/35182]: Training loss 0.5258515477180481, current lr 9.878393166766152e-05, contrastive loss 3.3144893646240234, task loss 0.49270665645599365
2025-01-05 20:05:22,233 - root - INFO - [Train] Epoch 2 [15500/35182]: Training loss 0.5342203378677368, current lr 9.878251044597937e-05, contrastive loss 3.310476064682007, task loss 0.5011155605316162
2025-01-05 20:07:02,814 - root - INFO - [Train] Epoch 2 [15600/35182]: Training loss 0.47780555486679077, current lr 9.878108922429721e-05, contrastive loss 3.216765880584717, task loss 0.44563788175582886
2025-01-05 20:08:43,203 - root - INFO - [Train] Epoch 2 [15700/35182]: Training loss 0.5740264058113098, current lr 9.877966800261505e-05, contrastive loss 3.285332202911377, task loss 0.5411731004714966
2025-01-05 20:10:23,572 - root - INFO - [Train] Epoch 2 [15800/35182]: Training loss 0.5479627251625061, current lr 9.87782467809329e-05, contrastive loss 3.3398663997650146, task loss 0.514564037322998
2025-01-05 20:12:04,169 - root - INFO - [Train] Epoch 2 [15900/35182]: Training loss 0.5527052283287048, current lr 9.877682555925074e-05, contrastive loss 3.248256206512451, task loss 0.5202226638793945
2025-01-05 20:13:44,561 - root - INFO - [Train] Epoch 2 [16000/35182]: Training loss 0.5139045119285583, current lr 9.877540433756858e-05, contrastive loss 3.3299782276153564, task loss 0.48060470819473267
2025-01-05 20:15:25,126 - root - INFO - [Train] Epoch 2 [16100/35182]: Training loss 0.5308430790901184, current lr 9.877398311588643e-05, contrastive loss 3.2822537422180176, task loss 0.4980205297470093
2025-01-05 20:17:05,510 - root - INFO - [Train] Epoch 2 [16200/35182]: Training loss 0.5231223106384277, current lr 9.877256189420426e-05, contrastive loss 3.5514495372772217, task loss 0.48760783672332764
2025-01-05 20:18:46,099 - root - INFO - [Train] Epoch 2 [16300/35182]: Training loss 0.5263968706130981, current lr 9.87711406725221e-05, contrastive loss 3.2815070152282715, task loss 0.4935818016529083
2025-01-05 20:20:26,488 - root - INFO - [Train] Epoch 2 [16400/35182]: Training loss 0.549872636795044, current lr 9.876971945083994e-05, contrastive loss 3.4640493392944336, task loss 0.5152321457862854
2025-01-05 20:22:07,090 - root - INFO - [Train] Epoch 2 [16500/35182]: Training loss 0.49168771505355835, current lr 9.876829822915779e-05, contrastive loss 3.354943037033081, task loss 0.45813828706741333
2025-01-05 20:23:47,472 - root - INFO - [Train] Epoch 2 [16600/35182]: Training loss 0.5080705285072327, current lr 9.876687700747563e-05, contrastive loss 3.3866610527038574, task loss 0.4742039442062378
2025-01-05 20:25:28,127 - root - INFO - [Train] Epoch 2 [16700/35182]: Training loss 0.5753567814826965, current lr 9.876545578579347e-05, contrastive loss 3.487656354904175, task loss 0.5404801964759827
2025-01-05 20:27:08,486 - root - INFO - [Train] Epoch 2 [16800/35182]: Training loss 0.507351279258728, current lr 9.876403456411132e-05, contrastive loss 3.303474187850952, task loss 0.4743165671825409
2025-01-05 20:28:49,102 - root - INFO - [Train] Epoch 2 [16900/35182]: Training loss 0.4490574300289154, current lr 9.876261334242916e-05, contrastive loss 3.160886764526367, task loss 0.4174485504627228
2025-01-05 20:30:29,491 - root - INFO - [Train] Epoch 2 [17000/35182]: Training loss 0.5341224074363708, current lr 9.8761192120747e-05, contrastive loss 3.2636678218841553, task loss 0.5014857053756714
2025-01-05 20:32:10,073 - root - INFO - [Train] Epoch 2 [17100/35182]: Training loss 0.5733510851860046, current lr 9.875977089906485e-05, contrastive loss 3.302747964859009, task loss 0.5403236150741577
2025-01-05 20:33:50,442 - root - INFO - [Train] Epoch 2 [17200/35182]: Training loss 0.5536748170852661, current lr 9.875834967738268e-05, contrastive loss 3.3215019702911377, task loss 0.520459771156311
2025-01-05 20:35:31,026 - root - INFO - [Train] Epoch 2 [17300/35182]: Training loss 0.5556395053863525, current lr 9.875692845570052e-05, contrastive loss 3.384460210800171, task loss 0.5217949151992798
2025-01-05 20:37:11,416 - root - INFO - [Train] Epoch 2 [17400/35182]: Training loss 0.5533192157745361, current lr 9.875550723401836e-05, contrastive loss 3.2504923343658447, task loss 0.5208142995834351
2025-01-05 20:38:51,999 - root - INFO - [Train] Epoch 2 [17500/35182]: Training loss 0.5063778162002563, current lr 9.875408601233621e-05, contrastive loss 3.36446475982666, task loss 0.47273319959640503
2025-01-05 20:40:32,374 - root - INFO - [Train] Epoch 2 [17600/35182]: Training loss 0.504966676235199, current lr 9.875266479065405e-05, contrastive loss 3.246509552001953, task loss 0.47250157594680786
2025-01-05 20:42:12,971 - root - INFO - [Train] Epoch 2 [17700/35182]: Training loss 0.5471764206886292, current lr 9.87512435689719e-05, contrastive loss 3.224996328353882, task loss 0.5149264335632324
2025-01-05 20:43:53,364 - root - INFO - [Train] Epoch 2 [17800/35182]: Training loss 0.5766800045967102, current lr 9.874982234728974e-05, contrastive loss 3.357339382171631, task loss 0.5431066155433655
2025-01-05 20:45:33,939 - root - INFO - [Train] Epoch 2 [17900/35182]: Training loss 0.5133073925971985, current lr 9.874840112560758e-05, contrastive loss 3.4914298057556152, task loss 0.4783931076526642
2025-01-05 20:47:14,305 - root - INFO - [Train] Epoch 2 [18000/35182]: Training loss 0.574465274810791, current lr 9.874697990392542e-05, contrastive loss 3.2729101181030273, task loss 0.5417361855506897
2025-01-05 20:48:54,894 - root - INFO - [Train] Epoch 2 [18100/35182]: Training loss 0.5605095624923706, current lr 9.874555868224327e-05, contrastive loss 3.117321729660034, task loss 0.5293363332748413
2025-01-05 20:50:35,258 - root - INFO - [Train] Epoch 2 [18200/35182]: Training loss 0.5114938616752625, current lr 9.87441374605611e-05, contrastive loss 3.328014612197876, task loss 0.47821372747421265
2025-01-05 20:52:15,868 - root - INFO - [Train] Epoch 2 [18300/35182]: Training loss 0.5599281191825867, current lr 9.874271623887894e-05, contrastive loss 3.2510714530944824, task loss 0.5274174213409424
2025-01-05 20:53:56,232 - root - INFO - [Train] Epoch 2 [18400/35182]: Training loss 0.5103211998939514, current lr 9.874129501719678e-05, contrastive loss 3.296881914138794, task loss 0.4773523807525635
2025-01-05 20:55:36,832 - root - INFO - [Train] Epoch 2 [18500/35182]: Training loss 0.5247595310211182, current lr 9.873987379551463e-05, contrastive loss 3.2389321327209473, task loss 0.4923701882362366
2025-01-05 20:57:17,213 - root - INFO - [Train] Epoch 2 [18600/35182]: Training loss 0.48517370223999023, current lr 9.873845257383247e-05, contrastive loss 3.354778289794922, task loss 0.45162591338157654
2025-01-05 20:58:57,800 - root - INFO - [Train] Epoch 2 [18700/35182]: Training loss 0.5691791772842407, current lr 9.873703135215031e-05, contrastive loss 3.293295383453369, task loss 0.5362462401390076
2025-01-05 21:00:38,167 - root - INFO - [Train] Epoch 2 [18800/35182]: Training loss 0.6012247204780579, current lr 9.873561013046816e-05, contrastive loss 3.3746700286865234, task loss 0.5674780011177063
2025-01-05 21:02:18,530 - root - INFO - [Train] Epoch 2 [18900/35182]: Training loss 0.5208318829536438, current lr 9.8734188908786e-05, contrastive loss 3.348640203475952, task loss 0.48734545707702637
2025-01-05 21:03:59,158 - root - INFO - [Train] Epoch 2 [19000/35182]: Training loss 0.5150012969970703, current lr 9.873276768710384e-05, contrastive loss 3.420790433883667, task loss 0.48079341650009155
2025-01-05 21:05:39,556 - root - INFO - [Train] Epoch 2 [19100/35182]: Training loss 0.5487239956855774, current lr 9.873134646542169e-05, contrastive loss 3.5253851413726807, task loss 0.5134701728820801
2025-01-05 21:07:20,109 - root - INFO - [Train] Epoch 2 [19200/35182]: Training loss 0.5058498382568359, current lr 9.872992524373952e-05, contrastive loss 3.524102210998535, task loss 0.47060880064964294
2025-01-05 21:09:00,493 - root - INFO - [Train] Epoch 2 [19300/35182]: Training loss 0.5138104557991028, current lr 9.872850402205736e-05, contrastive loss 3.27030086517334, task loss 0.4811074435710907
2025-01-05 21:10:41,076 - root - INFO - [Train] Epoch 2 [19400/35182]: Training loss 0.5817097425460815, current lr 9.87270828003752e-05, contrastive loss 3.2953295707702637, task loss 0.5487564206123352
2025-01-05 21:12:21,459 - root - INFO - [Train] Epoch 2 [19500/35182]: Training loss 0.4787188470363617, current lr 9.872566157869305e-05, contrastive loss 3.159358501434326, task loss 0.44712525606155396
2025-01-05 21:14:02,048 - root - INFO - [Train] Epoch 2 [19600/35182]: Training loss 0.5130851864814758, current lr 9.872424035701089e-05, contrastive loss 3.3346810340881348, task loss 0.47973835468292236
2025-01-05 21:15:42,412 - root - INFO - [Train] Epoch 2 [19700/35182]: Training loss 0.4856961965560913, current lr 9.872281913532873e-05, contrastive loss 3.2647342681884766, task loss 0.45304885506629944
2025-01-05 21:17:22,995 - root - INFO - [Train] Epoch 2 [19800/35182]: Training loss 0.5126021504402161, current lr 9.872139791364658e-05, contrastive loss 3.3549323081970215, task loss 0.4790528118610382
2025-01-05 21:19:03,377 - root - INFO - [Train] Epoch 2 [19900/35182]: Training loss 0.5885862112045288, current lr 9.871997669196442e-05, contrastive loss 3.209177017211914, task loss 0.556494414806366
2025-01-05 21:20:43,976 - root - INFO - [Train] Epoch 2 [20000/35182]: Training loss 0.5549383163452148, current lr 9.871855547028226e-05, contrastive loss 3.2791097164154053, task loss 0.5221472382545471
2025-01-05 21:22:24,349 - root - INFO - [Train] Epoch 2 [20100/35182]: Training loss 0.5415327548980713, current lr 9.87171342486001e-05, contrastive loss 3.328976631164551, task loss 0.5082429647445679
2025-01-05 21:24:04,945 - root - INFO - [Train] Epoch 2 [20200/35182]: Training loss 0.5644544959068298, current lr 9.871571302691794e-05, contrastive loss 3.4563302993774414, task loss 0.5298911929130554
2025-01-05 21:25:45,319 - root - INFO - [Train] Epoch 2 [20300/35182]: Training loss 0.5751023888587952, current lr 9.871429180523578e-05, contrastive loss 3.2671709060668945, task loss 0.5424306988716125
2025-01-05 21:27:25,900 - root - INFO - [Train] Epoch 2 [20400/35182]: Training loss 0.5527133941650391, current lr 9.871287058355362e-05, contrastive loss 3.6219751834869385, task loss 0.5164936184883118
2025-01-05 21:29:06,293 - root - INFO - [Train] Epoch 2 [20500/35182]: Training loss 0.5535188913345337, current lr 9.871144936187147e-05, contrastive loss 3.2771849632263184, task loss 0.5207470655441284
2025-01-05 21:30:46,869 - root - INFO - [Train] Epoch 2 [20600/35182]: Training loss 0.590535044670105, current lr 9.871002814018931e-05, contrastive loss 3.2444682121276855, task loss 0.5580903887748718
2025-01-05 21:32:27,242 - root - INFO - [Train] Epoch 2 [20700/35182]: Training loss 0.5398395657539368, current lr 9.870860691850715e-05, contrastive loss 3.2730226516723633, task loss 0.5071093440055847
2025-01-05 21:34:07,837 - root - INFO - [Train] Epoch 2 [20800/35182]: Training loss 0.5109266638755798, current lr 9.8707185696825e-05, contrastive loss 3.2725441455841064, task loss 0.4782012104988098
2025-01-05 21:35:48,209 - root - INFO - [Train] Epoch 2 [20900/35182]: Training loss 0.5621389746665955, current lr 9.870576447514284e-05, contrastive loss 3.3027100563049316, task loss 0.5291118621826172
2025-01-05 21:37:28,803 - root - INFO - [Train] Epoch 2 [21000/35182]: Training loss 0.5473893284797668, current lr 9.870434325346068e-05, contrastive loss 3.4201135635375977, task loss 0.5131881833076477
2025-01-05 21:39:09,180 - root - INFO - [Train] Epoch 2 [21100/35182]: Training loss 0.5535269379615784, current lr 9.870292203177853e-05, contrastive loss 3.166865825653076, task loss 0.521858274936676
2025-01-05 21:40:49,545 - root - INFO - [Train] Epoch 2 [21200/35182]: Training loss 0.4833901524543762, current lr 9.870150081009636e-05, contrastive loss 3.345890522003174, task loss 0.44993123412132263
2025-01-05 21:42:30,150 - root - INFO - [Train] Epoch 2 [21300/35182]: Training loss 0.5612554550170898, current lr 9.87000795884142e-05, contrastive loss 3.1755082607269287, task loss 0.5295003652572632
2025-01-05 21:44:10,523 - root - INFO - [Train] Epoch 2 [21400/35182]: Training loss 0.5274838805198669, current lr 9.869865836673204e-05, contrastive loss 3.3862857818603516, task loss 0.49362102150917053
2025-01-05 21:45:51,128 - root - INFO - [Train] Epoch 2 [21500/35182]: Training loss 0.5479121804237366, current lr 9.869723714504989e-05, contrastive loss 3.229459047317505, task loss 0.5156176090240479
2025-01-05 21:47:31,507 - root - INFO - [Train] Epoch 2 [21600/35182]: Training loss 0.5030171275138855, current lr 9.869581592336773e-05, contrastive loss 3.286184787750244, task loss 0.470155268907547
2025-01-05 21:49:12,087 - root - INFO - [Train] Epoch 2 [21700/35182]: Training loss 0.5632362365722656, current lr 9.869439470168557e-05, contrastive loss 3.1965951919555664, task loss 0.5312702655792236
2025-01-05 21:50:52,479 - root - INFO - [Train] Epoch 2 [21800/35182]: Training loss 0.5205843448638916, current lr 9.869297348000342e-05, contrastive loss 3.4367289543151855, task loss 0.48621705174446106
2025-01-05 21:52:33,050 - root - INFO - [Train] Epoch 2 [21900/35182]: Training loss 0.646468997001648, current lr 9.869155225832126e-05, contrastive loss 3.086632251739502, task loss 0.6156026721000671
2025-01-05 21:54:13,430 - root - INFO - [Train] Epoch 2 [22000/35182]: Training loss 0.5110558867454529, current lr 9.86901310366391e-05, contrastive loss 3.0807714462280273, task loss 0.48024818301200867
2025-01-05 21:55:54,018 - root - INFO - [Train] Epoch 2 [22100/35182]: Training loss 0.5255643725395203, current lr 9.868870981495693e-05, contrastive loss 3.1565067768096924, task loss 0.49399933218955994
2025-01-05 21:57:34,402 - root - INFO - [Train] Epoch 2 [22200/35182]: Training loss 0.5074634552001953, current lr 9.868728859327478e-05, contrastive loss 3.5431180000305176, task loss 0.4720322787761688
2025-01-05 21:59:14,979 - root - INFO - [Train] Epoch 2 [22300/35182]: Training loss 0.5326094031333923, current lr 9.868586737159262e-05, contrastive loss 3.161649703979492, task loss 0.5009928941726685
2025-01-05 22:00:55,368 - root - INFO - [Train] Epoch 2 [22400/35182]: Training loss 0.5063709616661072, current lr 9.868444614991047e-05, contrastive loss 3.3658881187438965, task loss 0.47271209955215454
2025-01-05 22:02:35,945 - root - INFO - [Train] Epoch 2 [22500/35182]: Training loss 0.5488312840461731, current lr 9.868302492822831e-05, contrastive loss 3.3039135932922363, task loss 0.5157921314239502
2025-01-05 22:04:16,326 - root - INFO - [Train] Epoch 2 [22600/35182]: Training loss 0.5095585584640503, current lr 9.868160370654615e-05, contrastive loss 3.4100303649902344, task loss 0.4754582643508911
2025-01-05 22:05:56,908 - root - INFO - [Train] Epoch 2 [22700/35182]: Training loss 0.5530197024345398, current lr 9.8680182484864e-05, contrastive loss 3.247072696685791, task loss 0.5205489993095398
2025-01-05 22:07:37,301 - root - INFO - [Train] Epoch 2 [22800/35182]: Training loss 0.5540404319763184, current lr 9.867876126318184e-05, contrastive loss 3.352956533432007, task loss 0.5205108523368835
2025-01-05 22:09:17,876 - root - INFO - [Train] Epoch 2 [22900/35182]: Training loss 0.5042814612388611, current lr 9.867734004149968e-05, contrastive loss 3.2761387825012207, task loss 0.4715200662612915
2025-01-05 22:10:58,260 - root - INFO - [Train] Epoch 2 [23000/35182]: Training loss 0.4680608808994293, current lr 9.867591881981751e-05, contrastive loss 3.323909282684326, task loss 0.4348217844963074
2025-01-05 22:12:38,837 - root - INFO - [Train] Epoch 2 [23100/35182]: Training loss 0.609358012676239, current lr 9.867449759813537e-05, contrastive loss 3.3012642860412598, task loss 0.5763453841209412
2025-01-05 22:14:19,214 - root - INFO - [Train] Epoch 2 [23200/35182]: Training loss 0.5117132663726807, current lr 9.86730763764532e-05, contrastive loss 3.4051594734191895, task loss 0.477661669254303
2025-01-05 22:15:59,804 - root - INFO - [Train] Epoch 2 [23300/35182]: Training loss 0.47673270106315613, current lr 9.867165515477104e-05, contrastive loss 3.217400312423706, task loss 0.444558709859848
2025-01-05 22:17:40,194 - root - INFO - [Train] Epoch 2 [23400/35182]: Training loss 0.5262349247932434, current lr 9.867023393308889e-05, contrastive loss 3.0803723335266113, task loss 0.49543118476867676
2025-01-05 22:19:20,581 - root - INFO - [Train] Epoch 2 [23500/35182]: Training loss 0.5312007069587708, current lr 9.866881271140673e-05, contrastive loss 3.0668225288391113, task loss 0.5005325078964233
2025-01-05 22:21:01,154 - root - INFO - [Train] Epoch 2 [23600/35182]: Training loss 0.49120691418647766, current lr 9.866739148972457e-05, contrastive loss 3.150758743286133, task loss 0.4596993327140808
2025-01-05 22:22:41,537 - root - INFO - [Train] Epoch 2 [23700/35182]: Training loss 0.5372193455696106, current lr 9.866597026804242e-05, contrastive loss 3.0952634811401367, task loss 0.506266713142395
2025-01-05 22:24:22,114 - root - INFO - [Train] Epoch 2 [23800/35182]: Training loss 0.4981338381767273, current lr 9.866454904636026e-05, contrastive loss 3.1827425956726074, task loss 0.4663064181804657
2025-01-05 22:26:02,483 - root - INFO - [Train] Epoch 2 [23900/35182]: Training loss 0.49957939982414246, current lr 9.86631278246781e-05, contrastive loss 3.2396111488342285, task loss 0.46718329191207886
2025-01-05 22:27:43,083 - root - INFO - [Train] Epoch 2 [24000/35182]: Training loss 0.5731793642044067, current lr 9.866170660299595e-05, contrastive loss 3.0670132637023926, task loss 0.5425092577934265
2025-01-05 22:29:23,454 - root - INFO - [Train] Epoch 2 [24100/35182]: Training loss 0.5470637679100037, current lr 9.866028538131378e-05, contrastive loss 3.24033260345459, task loss 0.5146604180335999
2025-01-05 22:31:04,062 - root - INFO - [Train] Epoch 2 [24200/35182]: Training loss 0.528458297252655, current lr 9.865886415963162e-05, contrastive loss 3.3686439990997314, task loss 0.4947718679904938
2025-01-05 22:32:44,452 - root - INFO - [Train] Epoch 2 [24300/35182]: Training loss 0.5882877111434937, current lr 9.865744293794946e-05, contrastive loss 3.373619556427002, task loss 0.5545515418052673
2025-01-05 22:34:25,013 - root - INFO - [Train] Epoch 2 [24400/35182]: Training loss 0.5093626976013184, current lr 9.86560217162673e-05, contrastive loss 3.265780448913574, task loss 0.476704865694046
2025-01-05 22:36:05,395 - root - INFO - [Train] Epoch 2 [24500/35182]: Training loss 0.5277647376060486, current lr 9.865460049458515e-05, contrastive loss 3.160719394683838, task loss 0.49615752696990967
2025-01-05 22:37:45,977 - root - INFO - [Train] Epoch 2 [24600/35182]: Training loss 0.48743462562561035, current lr 9.865317927290299e-05, contrastive loss 3.208550453186035, task loss 0.4553491175174713
2025-01-05 22:39:26,353 - root - INFO - [Train] Epoch 2 [24700/35182]: Training loss 0.6333786845207214, current lr 9.865175805122084e-05, contrastive loss 3.382427215576172, task loss 0.5995544195175171
2025-01-05 22:41:06,949 - root - INFO - [Train] Epoch 2 [24800/35182]: Training loss 0.5588206648826599, current lr 9.865033682953868e-05, contrastive loss 3.113668441772461, task loss 0.5276839733123779
2025-01-05 22:42:47,322 - root - INFO - [Train] Epoch 2 [24900/35182]: Training loss 0.5088053345680237, current lr 9.864891560785652e-05, contrastive loss 3.13962984085083, task loss 0.4774090647697449
2025-01-05 22:44:27,911 - root - INFO - [Train] Epoch 2 [25000/35182]: Training loss 0.5688095688819885, current lr 9.864749438617435e-05, contrastive loss 3.2522971630096436, task loss 0.5362865924835205
2025-01-05 22:46:08,293 - root - INFO - [Train] Epoch 2 [25100/35182]: Training loss 0.5502597093582153, current lr 9.864607316449221e-05, contrastive loss 3.3759729862213135, task loss 0.5164999961853027
2025-01-05 22:47:48,884 - root - INFO - [Train] Epoch 2 [25200/35182]: Training loss 0.5405664443969727, current lr 9.864465194281004e-05, contrastive loss 3.243042469024658, task loss 0.5081360340118408
2025-01-05 22:49:29,266 - root - INFO - [Train] Epoch 2 [25300/35182]: Training loss 0.47794750332832336, current lr 9.864323072112788e-05, contrastive loss 3.221942901611328, task loss 0.445728063583374
2025-01-05 22:51:09,847 - root - INFO - [Train] Epoch 2 [25400/35182]: Training loss 0.49774298071861267, current lr 9.864180949944573e-05, contrastive loss 3.2262964248657227, task loss 0.4654800295829773
2025-01-05 22:52:50,232 - root - INFO - [Train] Epoch 2 [25500/35182]: Training loss 0.6032859683036804, current lr 9.864038827776357e-05, contrastive loss 3.234454870223999, task loss 0.5709414482116699
2025-01-05 22:54:30,814 - root - INFO - [Train] Epoch 2 [25600/35182]: Training loss 0.5111783742904663, current lr 9.863896705608141e-05, contrastive loss 3.1558051109313965, task loss 0.4796203374862671
2025-01-05 22:56:11,230 - root - INFO - [Train] Epoch 2 [25700/35182]: Training loss 0.5671666264533997, current lr 9.863754583439926e-05, contrastive loss 3.198805332183838, task loss 0.5351786017417908
2025-01-05 22:57:51,888 - root - INFO - [Train] Epoch 2 [25800/35182]: Training loss 0.4911413788795471, current lr 9.86361246127171e-05, contrastive loss 3.334118366241455, task loss 0.4578002095222473
2025-01-05 22:59:32,276 - root - INFO - [Train] Epoch 2 [25900/35182]: Training loss 0.46066752076148987, current lr 9.863470339103494e-05, contrastive loss 3.3174920082092285, task loss 0.42749258875846863
2025-01-05 23:01:12,857 - root - INFO - [Train] Epoch 2 [26000/35182]: Training loss 0.5975193381309509, current lr 9.863328216935279e-05, contrastive loss 3.2122554779052734, task loss 0.565396785736084
2025-01-05 23:02:53,243 - root - INFO - [Train] Epoch 2 [26100/35182]: Training loss 0.498363196849823, current lr 9.863186094767062e-05, contrastive loss 3.2343924045562744, task loss 0.46601927280426025
2025-01-05 23:04:33,832 - root - INFO - [Train] Epoch 2 [26200/35182]: Training loss 0.46869826316833496, current lr 9.863043972598846e-05, contrastive loss 3.2264742851257324, task loss 0.4364335238933563
2025-01-05 23:06:14,214 - root - INFO - [Train] Epoch 2 [26300/35182]: Training loss 0.5439319014549255, current lr 9.86290185043063e-05, contrastive loss 3.447049140930176, task loss 0.5094614028930664
2025-01-05 23:07:54,597 - root - INFO - [Train] Epoch 2 [26400/35182]: Training loss 0.5239850878715515, current lr 9.862759728262415e-05, contrastive loss 3.16774320602417, task loss 0.4923076331615448
2025-01-05 23:09:35,183 - root - INFO - [Train] Epoch 2 [26500/35182]: Training loss 0.5140181183815002, current lr 9.862617606094199e-05, contrastive loss 3.2449679374694824, task loss 0.48156842589378357
2025-01-05 23:11:15,561 - root - INFO - [Train] Epoch 2 [26600/35182]: Training loss 0.5499627590179443, current lr 9.862475483925983e-05, contrastive loss 3.314260482788086, task loss 0.5168201327323914
2025-01-05 23:12:56,144 - root - INFO - [Train] Epoch 2 [26700/35182]: Training loss 0.5260441303253174, current lr 9.862333361757768e-05, contrastive loss 3.157876968383789, task loss 0.4944653809070587
2025-01-05 23:14:36,829 - root - INFO - [Train] Epoch 2 [26800/35182]: Training loss 0.5293943285942078, current lr 9.862191239589552e-05, contrastive loss 3.441856622695923, task loss 0.4949757754802704
2025-01-05 23:16:17,225 - root - INFO - [Train] Epoch 2 [26900/35182]: Training loss 0.6176819205284119, current lr 9.862049117421336e-05, contrastive loss 3.381531238555908, task loss 0.5838665962219238
2025-01-05 23:17:57,800 - root - INFO - [Train] Epoch 2 [27000/35182]: Training loss 0.525326669216156, current lr 9.861906995253119e-05, contrastive loss 3.1907846927642822, task loss 0.49341881275177
2025-01-05 23:19:38,188 - root - INFO - [Train] Epoch 2 [27100/35182]: Training loss 0.527988851070404, current lr 9.861764873084905e-05, contrastive loss 3.2590863704681396, task loss 0.49539801478385925
2025-01-05 23:21:18,562 - root - INFO - [Train] Epoch 2 [27200/35182]: Training loss 0.593574047088623, current lr 9.861622750916688e-05, contrastive loss 3.1887784004211426, task loss 0.5616862773895264
2025-01-05 23:22:59,152 - root - INFO - [Train] Epoch 2 [27300/35182]: Training loss 0.5555550456047058, current lr 9.861480628748472e-05, contrastive loss 3.4387760162353516, task loss 0.5211672782897949
2025-01-05 23:24:39,538 - root - INFO - [Train] Epoch 2 [27400/35182]: Training loss 0.5379934310913086, current lr 9.861338506580257e-05, contrastive loss 3.192288398742676, task loss 0.5060705542564392
2025-01-05 23:26:20,110 - root - INFO - [Train] Epoch 2 [27500/35182]: Training loss 0.4462684094905853, current lr 9.861196384412041e-05, contrastive loss 3.139252185821533, task loss 0.41487589478492737
2025-01-05 23:28:00,490 - root - INFO - [Train] Epoch 2 [27600/35182]: Training loss 0.5540584921836853, current lr 9.861054262243825e-05, contrastive loss 3.2545342445373535, task loss 0.5215131640434265
2025-01-05 23:29:41,083 - root - INFO - [Train] Epoch 2 [27700/35182]: Training loss 0.5010412931442261, current lr 9.86091214007561e-05, contrastive loss 3.243061065673828, task loss 0.4686106741428375
2025-01-05 23:31:21,472 - root - INFO - [Train] Epoch 2 [27800/35182]: Training loss 0.566172182559967, current lr 9.860770017907394e-05, contrastive loss 3.2318906784057617, task loss 0.53385329246521
2025-01-05 23:33:02,051 - root - INFO - [Train] Epoch 2 [27900/35182]: Training loss 0.4569402039051056, current lr 9.860627895739177e-05, contrastive loss 3.1707751750946045, task loss 0.4252324402332306
2025-01-05 23:34:42,436 - root - INFO - [Train] Epoch 2 [28000/35182]: Training loss 0.46443018317222595, current lr 9.860485773570963e-05, contrastive loss 3.285959482192993, task loss 0.4315705895423889
2025-01-05 23:36:23,014 - root - INFO - [Train] Epoch 2 [28100/35182]: Training loss 0.527898371219635, current lr 9.860343651402746e-05, contrastive loss 3.37392520904541, task loss 0.49415913224220276
2025-01-05 23:38:03,392 - root - INFO - [Train] Epoch 2 [28200/35182]: Training loss 0.5376026630401611, current lr 9.86020152923453e-05, contrastive loss 3.176711082458496, task loss 0.5058355331420898
2025-01-05 23:39:43,983 - root - INFO - [Train] Epoch 2 [28300/35182]: Training loss 0.46258649230003357, current lr 9.860059407066314e-05, contrastive loss 3.3691580295562744, task loss 0.42889490723609924
2025-01-05 23:41:24,365 - root - INFO - [Train] Epoch 2 [28400/35182]: Training loss 0.5535293221473694, current lr 9.859917284898099e-05, contrastive loss 3.213792324066162, task loss 0.5213913917541504
2025-01-05 23:43:04,961 - root - INFO - [Train] Epoch 2 [28500/35182]: Training loss 0.44899898767471313, current lr 9.859775162729883e-05, contrastive loss 3.4322619438171387, task loss 0.41467636823654175
2025-01-05 23:44:45,353 - root - INFO - [Train] Epoch 2 [28600/35182]: Training loss 0.6257010698318481, current lr 9.859633040561667e-05, contrastive loss 3.2916502952575684, task loss 0.592784583568573
2025-01-05 23:46:26,020 - root - INFO - [Train] Epoch 2 [28700/35182]: Training loss 0.5184469223022461, current lr 9.859490918393452e-05, contrastive loss 3.2375454902648926, task loss 0.48607149720191956
2025-01-05 23:48:06,387 - root - INFO - [Train] Epoch 2 [28800/35182]: Training loss 0.5992596745491028, current lr 9.859348796225236e-05, contrastive loss 3.3295745849609375, task loss 0.5659639239311218
2025-01-05 23:49:46,993 - root - INFO - [Train] Epoch 2 [28900/35182]: Training loss 0.5730819702148438, current lr 9.85920667405702e-05, contrastive loss 3.197126865386963, task loss 0.5411106944084167
2025-01-05 23:51:27,378 - root - INFO - [Train] Epoch 2 [29000/35182]: Training loss 0.502788245677948, current lr 9.859064551888803e-05, contrastive loss 3.188753366470337, task loss 0.4709007143974304
2025-01-05 23:53:07,950 - root - INFO - [Train] Epoch 2 [29100/35182]: Training loss 0.5462281107902527, current lr 9.858922429720589e-05, contrastive loss 3.298086166381836, task loss 0.5132472515106201
2025-01-05 23:54:48,329 - root - INFO - [Train] Epoch 2 [29200/35182]: Training loss 0.5687350630760193, current lr 9.858780307552372e-05, contrastive loss 3.41479229927063, task loss 0.5345871448516846
2025-01-05 23:56:28,918 - root - INFO - [Train] Epoch 2 [29300/35182]: Training loss 0.5083267688751221, current lr 9.858638185384156e-05, contrastive loss 3.440211534500122, task loss 0.4739246666431427
2025-01-05 23:58:09,304 - root - INFO - [Train] Epoch 2 [29400/35182]: Training loss 0.5891126394271851, current lr 9.858496063215941e-05, contrastive loss 3.3906967639923096, task loss 0.5552056431770325
2025-01-05 23:59:49,878 - root - INFO - [Train] Epoch 2 [29500/35182]: Training loss 0.4826623797416687, current lr 9.858353941047725e-05, contrastive loss 3.2173752784729004, task loss 0.4504886269569397
2025-01-06 00:01:30,257 - root - INFO - [Train] Epoch 2 [29600/35182]: Training loss 0.6024925708770752, current lr 9.85821181887951e-05, contrastive loss 3.1755423545837402, task loss 0.5707371234893799
2025-01-06 00:03:10,848 - root - INFO - [Train] Epoch 2 [29700/35182]: Training loss 0.5605193972587585, current lr 9.858069696711294e-05, contrastive loss 3.137392997741699, task loss 0.5291454792022705
2025-01-06 00:04:51,231 - root - INFO - [Train] Epoch 2 [29800/35182]: Training loss 0.5461053848266602, current lr 9.857927574543078e-05, contrastive loss 3.103426694869995, task loss 0.5150710940361023
2025-01-06 00:06:31,820 - root - INFO - [Train] Epoch 2 [29900/35182]: Training loss 0.6018611788749695, current lr 9.857785452374861e-05, contrastive loss 3.2751407623291016, task loss 0.5691097974777222
2025-01-06 00:08:12,194 - root - INFO - [Train] Epoch 2 [30000/35182]: Training loss 0.4643992483615875, current lr 9.857643330206647e-05, contrastive loss 3.199131488800049, task loss 0.432407945394516
2025-01-06 00:09:52,562 - root - INFO - [Train] Epoch 2 [30100/35182]: Training loss 0.5321688055992126, current lr 9.85750120803843e-05, contrastive loss 3.165485382080078, task loss 0.5005139708518982
2025-01-06 00:11:33,146 - root - INFO - [Train] Epoch 2 [30200/35182]: Training loss 0.5317676663398743, current lr 9.857359085870214e-05, contrastive loss 3.154385805130005, task loss 0.5002238154411316
2025-01-06 00:13:13,509 - root - INFO - [Train] Epoch 2 [30300/35182]: Training loss 0.49603089690208435, current lr 9.857216963701998e-05, contrastive loss 3.1401994228363037, task loss 0.4646289050579071
2025-01-06 00:14:54,111 - root - INFO - [Train] Epoch 2 [30400/35182]: Training loss 0.5451011061668396, current lr 9.857074841533783e-05, contrastive loss 3.1793251037597656, task loss 0.5133078694343567
2025-01-06 00:16:34,487 - root - INFO - [Train] Epoch 2 [30500/35182]: Training loss 0.5514960885047913, current lr 9.856932719365567e-05, contrastive loss 3.144085168838501, task loss 0.5200552344322205
2025-01-06 00:18:15,080 - root - INFO - [Train] Epoch 2 [30600/35182]: Training loss 0.56194669008255, current lr 9.856790597197351e-05, contrastive loss 3.33992862701416, task loss 0.5285474061965942
2025-01-06 00:19:55,450 - root - INFO - [Train] Epoch 2 [30700/35182]: Training loss 0.5599058866500854, current lr 9.856648475029136e-05, contrastive loss 2.986997604370117, task loss 0.5300359129905701
2025-01-06 00:21:36,046 - root - INFO - [Train] Epoch 2 [30800/35182]: Training loss 0.518240213394165, current lr 9.856506352860919e-05, contrastive loss 3.245065689086914, task loss 0.48578953742980957
2025-01-06 00:23:16,420 - root - INFO - [Train] Epoch 2 [30900/35182]: Training loss 0.4917620122432709, current lr 9.856364230692704e-05, contrastive loss 3.1014134883880615, task loss 0.4607478678226471
2025-01-06 00:24:57,020 - root - INFO - [Train] Epoch 2 [31000/35182]: Training loss 0.5818765759468079, current lr 9.856222108524487e-05, contrastive loss 3.3862404823303223, task loss 0.5480141639709473
2025-01-06 00:26:37,396 - root - INFO - [Train] Epoch 2 [31100/35182]: Training loss 0.5415791869163513, current lr 9.856079986356273e-05, contrastive loss 3.260298728942871, task loss 0.5089762210845947
2025-01-06 00:28:17,985 - root - INFO - [Train] Epoch 2 [31200/35182]: Training loss 0.49928727746009827, current lr 9.855937864188056e-05, contrastive loss 3.2615296840667725, task loss 0.46667197346687317
2025-01-06 00:29:58,363 - root - INFO - [Train] Epoch 2 [31300/35182]: Training loss 0.57658451795578, current lr 9.85579574201984e-05, contrastive loss 3.210247039794922, task loss 0.5444820523262024
2025-01-06 00:31:38,956 - root - INFO - [Train] Epoch 2 [31400/35182]: Training loss 0.5414426922798157, current lr 9.855653619851625e-05, contrastive loss 3.245102643966675, task loss 0.5089916586875916
2025-01-06 00:33:19,338 - root - INFO - [Train] Epoch 2 [31500/35182]: Training loss 0.5893868207931519, current lr 9.855511497683409e-05, contrastive loss 3.3613295555114746, task loss 0.5557735562324524
2025-01-06 00:34:59,922 - root - INFO - [Train] Epoch 2 [31600/35182]: Training loss 0.5461512804031372, current lr 9.855369375515193e-05, contrastive loss 3.090569496154785, task loss 0.5152456164360046
2025-01-06 00:36:40,301 - root - INFO - [Train] Epoch 2 [31700/35182]: Training loss 0.5547972917556763, current lr 9.855227253346978e-05, contrastive loss 3.2955427169799805, task loss 0.5218418836593628
2025-01-06 00:38:20,883 - root - INFO - [Train] Epoch 2 [31800/35182]: Training loss 0.5144329071044922, current lr 9.855085131178762e-05, contrastive loss 3.3455734252929688, task loss 0.4809771776199341
2025-01-06 00:40:01,278 - root - INFO - [Train] Epoch 2 [31900/35182]: Training loss 0.5340379476547241, current lr 9.854943009010545e-05, contrastive loss 3.302549362182617, task loss 0.5010124444961548
2025-01-06 00:41:41,851 - root - INFO - [Train] Epoch 2 [32000/35182]: Training loss 0.5197287201881409, current lr 9.854800886842331e-05, contrastive loss 3.1815643310546875, task loss 0.4879131019115448
2025-01-06 00:43:22,241 - root - INFO - [Train] Epoch 2 [32100/35182]: Training loss 0.5105808973312378, current lr 9.854658764674114e-05, contrastive loss 3.15356183052063, task loss 0.4790453016757965
2025-01-06 00:45:02,819 - root - INFO - [Train] Epoch 2 [32200/35182]: Training loss 0.5399444103240967, current lr 9.854516642505898e-05, contrastive loss 3.187042713165283, task loss 0.5080739855766296
2025-01-06 00:46:43,194 - root - INFO - [Train] Epoch 2 [32300/35182]: Training loss 0.5036458969116211, current lr 9.854374520337683e-05, contrastive loss 3.175485610961914, task loss 0.47189101576805115
2025-01-06 00:48:23,555 - root - INFO - [Train] Epoch 2 [32400/35182]: Training loss 0.5803001523017883, current lr 9.854232398169467e-05, contrastive loss 3.1784095764160156, task loss 0.548516035079956
2025-01-06 00:50:04,156 - root - INFO - [Train] Epoch 2 [32500/35182]: Training loss 0.526458203792572, current lr 9.854090276001251e-05, contrastive loss 3.20469069480896, task loss 0.49441131949424744
2025-01-06 00:51:44,538 - root - INFO - [Train] Epoch 2 [32600/35182]: Training loss 0.5650139451026917, current lr 9.853948153833036e-05, contrastive loss 3.181201457977295, task loss 0.5332019329071045
2025-01-06 00:53:25,148 - root - INFO - [Train] Epoch 2 [32700/35182]: Training loss 0.4959738552570343, current lr 9.85380603166482e-05, contrastive loss 3.033363103866577, task loss 0.46564021706581116
2025-01-06 00:55:05,820 - root - INFO - [Train] Epoch 2 [32800/35182]: Training loss 0.5899583697319031, current lr 9.853663909496603e-05, contrastive loss 3.3682265281677246, task loss 0.5562760829925537
2025-01-06 00:56:46,196 - root - INFO - [Train] Epoch 2 [32900/35182]: Training loss 0.4987122416496277, current lr 9.853521787328389e-05, contrastive loss 3.1995222568511963, task loss 0.46671703457832336
2025-01-06 00:58:26,590 - root - INFO - [Train] Epoch 2 [33000/35182]: Training loss 0.5178711414337158, current lr 9.853379665160172e-05, contrastive loss 3.213137149810791, task loss 0.4857397973537445
2025-01-06 01:00:07,180 - root - INFO - [Train] Epoch 2 [33100/35182]: Training loss 0.4985244572162628, current lr 9.853237542991957e-05, contrastive loss 3.244597911834717, task loss 0.4660784900188446
2025-01-06 01:01:47,574 - root - INFO - [Train] Epoch 2 [33200/35182]: Training loss 0.48931920528411865, current lr 9.85309542082374e-05, contrastive loss 3.074666976928711, task loss 0.45857253670692444
2025-01-06 01:03:28,161 - root - INFO - [Train] Epoch 2 [33300/35182]: Training loss 0.5524476766586304, current lr 9.852953298655525e-05, contrastive loss 3.117650032043457, task loss 0.5212711691856384
2025-01-06 01:05:08,830 - root - INFO - [Train] Epoch 2 [33400/35182]: Training loss 0.4699517488479614, current lr 9.852811176487309e-05, contrastive loss 3.1566314697265625, task loss 0.4383854269981384
2025-01-06 01:06:49,223 - root - INFO - [Train] Epoch 2 [33500/35182]: Training loss 0.4995317757129669, current lr 9.852669054319093e-05, contrastive loss 3.3298683166503906, task loss 0.46623310446739197
2025-01-06 01:08:29,807 - root - INFO - [Train] Epoch 2 [33600/35182]: Training loss 0.5216626524925232, current lr 9.852526932150878e-05, contrastive loss 3.385526418685913, task loss 0.48780736327171326
2025-01-06 01:10:10,192 - root - INFO - [Train] Epoch 2 [33700/35182]: Training loss 0.5936911106109619, current lr 9.852384809982662e-05, contrastive loss 3.072197437286377, task loss 0.5629691481590271
2025-01-06 01:11:50,589 - root - INFO - [Train] Epoch 2 [33800/35182]: Training loss 0.5388640761375427, current lr 9.852242687814446e-05, contrastive loss 3.2302165031433105, task loss 0.5065619349479675
2025-01-06 01:13:31,156 - root - INFO - [Train] Epoch 2 [33900/35182]: Training loss 0.451097309589386, current lr 9.852100565646229e-05, contrastive loss 3.1797685623168945, task loss 0.4192996323108673
2025-01-06 01:15:11,545 - root - INFO - [Train] Epoch 2 [34000/35182]: Training loss 0.4934048652648926, current lr 9.851958443478015e-05, contrastive loss 3.4105334281921387, task loss 0.4592995345592499
2025-01-06 01:16:52,127 - root - INFO - [Train] Epoch 2 [34100/35182]: Training loss 0.5160316824913025, current lr 9.851816321309798e-05, contrastive loss 3.09389066696167, task loss 0.48509275913238525
2025-01-06 01:18:32,505 - root - INFO - [Train] Epoch 2 [34200/35182]: Training loss 0.5196540355682373, current lr 9.851674199141582e-05, contrastive loss 3.2241063117980957, task loss 0.4874129593372345
2025-01-06 01:20:13,082 - root - INFO - [Train] Epoch 2 [34300/35182]: Training loss 0.5387037396430969, current lr 9.851532076973367e-05, contrastive loss 3.261868476867676, task loss 0.5060850381851196
2025-01-06 01:21:53,459 - root - INFO - [Train] Epoch 2 [34400/35182]: Training loss 0.5728000402450562, current lr 9.851389954805151e-05, contrastive loss 3.0840187072753906, task loss 0.5419598817825317
2025-01-06 01:23:34,062 - root - INFO - [Train] Epoch 2 [34500/35182]: Training loss 0.6221698522567749, current lr 9.851247832636935e-05, contrastive loss 3.249549150466919, task loss 0.5896743535995483
2025-01-06 01:25:14,426 - root - INFO - [Train] Epoch 2 [34600/35182]: Training loss 0.5317656993865967, current lr 9.85110571046872e-05, contrastive loss 3.1660923957824707, task loss 0.5001047849655151
2025-01-06 01:26:55,006 - root - INFO - [Train] Epoch 2 [34700/35182]: Training loss 0.4907187223434448, current lr 9.850963588300504e-05, contrastive loss 3.098757743835449, task loss 0.4597311317920685
2025-01-06 01:28:35,389 - root - INFO - [Train] Epoch 2 [34800/35182]: Training loss 0.5519545078277588, current lr 9.850821466132287e-05, contrastive loss 3.0784316062927246, task loss 0.5211701989173889
2025-01-06 01:30:15,984 - root - INFO - [Train] Epoch 2 [34900/35182]: Training loss 0.559278666973114, current lr 9.850679343964073e-05, contrastive loss 3.0512266159057617, task loss 0.528766393661499
2025-01-06 01:31:56,379 - root - INFO - [Train] Epoch 2 [35000/35182]: Training loss 0.5443204045295715, current lr 9.850537221795856e-05, contrastive loss 3.0136396884918213, task loss 0.5141839981079102
2025-01-06 01:33:36,959 - root - INFO - [Train] Epoch 2 [35100/35182]: Training loss 0.5674331784248352, current lr 9.850395099627641e-05, contrastive loss 3.1862988471984863, task loss 0.5355702042579651
2025-01-06 01:35:00,936 - root - INFO - [Train] Epoch 2: Overall Training loss 0.0
2025-01-06 01:35:00,938 - root - INFO - [Time] Epoch 2: Time taken 9.82 hours
2025-01-06 01:35:06,058 - root - INFO - [Train] Epoch 3 [0/35182]: Training loss 0.5331371426582336, current lr 9.850278559449703e-05, contrastive loss 3.2594118118286133, task loss 0.5005429983139038
2025-01-06 01:36:46,444 - root - INFO - [Train] Epoch 3 [100/35182]: Training loss 0.546542227268219, current lr 9.850136437281488e-05, contrastive loss 3.229403495788574, task loss 0.5142481923103333
2025-01-06 01:38:27,028 - root - INFO - [Train] Epoch 3 [200/35182]: Training loss 0.5486176013946533, current lr 9.849994315113272e-05, contrastive loss 3.3254570960998535, task loss 0.5153630375862122
2025-01-06 01:40:07,423 - root - INFO - [Train] Epoch 3 [300/35182]: Training loss 0.5409283638000488, current lr 9.849852192945056e-05, contrastive loss 3.3012852668762207, task loss 0.5079154968261719
2025-01-06 01:41:47,983 - root - INFO - [Train] Epoch 3 [400/35182]: Training loss 0.5242036581039429, current lr 9.84971007077684e-05, contrastive loss 3.1867542266845703, task loss 0.49233609437942505
2025-01-06 01:43:28,355 - root - INFO - [Train] Epoch 3 [500/35182]: Training loss 0.49654242396354675, current lr 9.849567948608625e-05, contrastive loss 3.213517189025879, task loss 0.4644072651863098
2025-01-06 01:45:08,942 - root - INFO - [Train] Epoch 3 [600/35182]: Training loss 0.574167013168335, current lr 9.849425826440408e-05, contrastive loss 3.2400262355804443, task loss 0.5417667627334595
2025-01-06 01:46:49,306 - root - INFO - [Train] Epoch 3 [700/35182]: Training loss 0.5190244317054749, current lr 9.849283704272192e-05, contrastive loss 3.1759495735168457, task loss 0.4872649312019348
2025-01-06 01:48:29,915 - root - INFO - [Train] Epoch 3 [800/35182]: Training loss 0.5951291918754578, current lr 9.849141582103977e-05, contrastive loss 3.0726914405822754, task loss 0.5644022822380066
2025-01-06 01:50:10,281 - root - INFO - [Train] Epoch 3 [900/35182]: Training loss 0.5456728339195251, current lr 9.848999459935761e-05, contrastive loss 3.243070125579834, task loss 0.5132421255111694
2025-01-06 01:51:50,880 - root - INFO - [Train] Epoch 3 [1000/35182]: Training loss 0.5767838358879089, current lr 9.848857337767545e-05, contrastive loss 3.187537670135498, task loss 0.5449084639549255
2025-01-06 01:53:31,256 - root - INFO - [Train] Epoch 3 [1100/35182]: Training loss 0.46929728984832764, current lr 9.84871521559933e-05, contrastive loss 3.144568681716919, task loss 0.43785160779953003
2025-01-06 01:55:11,851 - root - INFO - [Train] Epoch 3 [1200/35182]: Training loss 0.525722324848175, current lr 9.848573093431114e-05, contrastive loss 2.9903600215911865, task loss 0.49581870436668396
2025-01-06 01:56:52,239 - root - INFO - [Train] Epoch 3 [1300/35182]: Training loss 0.5263599753379822, current lr 9.848430971262898e-05, contrastive loss 3.2336440086364746, task loss 0.49402356147766113
2025-01-06 01:58:32,820 - root - INFO - [Train] Epoch 3 [1400/35182]: Training loss 0.5457691550254822, current lr 9.848288849094683e-05, contrastive loss 3.11043643951416, task loss 0.5146647691726685
2025-01-06 02:00:13,193 - root - INFO - [Train] Epoch 3 [1500/35182]: Training loss 0.5616108179092407, current lr 9.848146726926466e-05, contrastive loss 3.163508415222168, task loss 0.5299757122993469
2025-01-06 02:01:53,573 - root - INFO - [Train] Epoch 3 [1600/35182]: Training loss 0.5385779738426208, current lr 9.848004604758251e-05, contrastive loss 3.153202772140503, task loss 0.5070459246635437
2025-01-06 02:03:34,166 - root - INFO - [Train] Epoch 3 [1700/35182]: Training loss 0.461740642786026, current lr 9.847862482590034e-05, contrastive loss 3.1298487186431885, task loss 0.4304421544075012
2025-01-06 02:05:14,570 - root - INFO - [Train] Epoch 3 [1800/35182]: Training loss 0.5332104563713074, current lr 9.847720360421819e-05, contrastive loss 3.1851205825805664, task loss 0.501359224319458
2025-01-06 02:06:55,134 - root - INFO - [Train] Epoch 3 [1900/35182]: Training loss 0.5064459443092346, current lr 9.847578238253603e-05, contrastive loss 3.2807505130767822, task loss 0.4736384451389313
2025-01-06 02:08:35,510 - root - INFO - [Train] Epoch 3 [2000/35182]: Training loss 0.5945556163787842, current lr 9.847436116085387e-05, contrastive loss 3.015742063522339, task loss 0.5643981695175171
2025-01-06 02:10:16,095 - root - INFO - [Train] Epoch 3 [2100/35182]: Training loss 0.46017688512802124, current lr 9.847293993917172e-05, contrastive loss 3.220273494720459, task loss 0.4279741644859314
2025-01-06 02:11:56,466 - root - INFO - [Train] Epoch 3 [2200/35182]: Training loss 0.5446035861968994, current lr 9.847151871748956e-05, contrastive loss 3.0523338317871094, task loss 0.5140802264213562
2025-01-06 02:13:37,052 - root - INFO - [Train] Epoch 3 [2300/35182]: Training loss 0.5095842480659485, current lr 9.84700974958074e-05, contrastive loss 3.1633896827697754, task loss 0.4779503345489502
2025-01-06 02:15:17,423 - root - INFO - [Train] Epoch 3 [2400/35182]: Training loss 0.45543691515922546, current lr 9.846867627412523e-05, contrastive loss 3.0804972648620605, task loss 0.4246319532394409
2025-01-06 02:16:58,021 - root - INFO - [Train] Epoch 3 [2500/35182]: Training loss 0.6234618425369263, current lr 9.846725505244309e-05, contrastive loss 3.1100053787231445, task loss 0.5923618078231812
2025-01-06 02:18:38,394 - root - INFO - [Train] Epoch 3 [2600/35182]: Training loss 0.5064171552658081, current lr 9.846583383076092e-05, contrastive loss 3.2674496173858643, task loss 0.47374266386032104
2025-01-06 02:20:19,002 - root - INFO - [Train] Epoch 3 [2700/35182]: Training loss 0.6298039555549622, current lr 9.846441260907876e-05, contrastive loss 3.1301605701446533, task loss 0.5985023379325867
2025-01-06 02:21:59,380 - root - INFO - [Train] Epoch 3 [2800/35182]: Training loss 0.4889167547225952, current lr 9.846299138739661e-05, contrastive loss 3.0173771381378174, task loss 0.45874297618865967
2025-01-06 02:23:39,946 - root - INFO - [Train] Epoch 3 [2900/35182]: Training loss 0.5313407778739929, current lr 9.846157016571445e-05, contrastive loss 3.0316991806030273, task loss 0.5010237693786621
2025-01-06 02:25:20,313 - root - INFO - [Train] Epoch 3 [3000/35182]: Training loss 0.5657380819320679, current lr 9.84601489440323e-05, contrastive loss 3.1416220664978027, task loss 0.5343218445777893
2025-01-06 02:27:00,927 - root - INFO - [Train] Epoch 3 [3100/35182]: Training loss 0.5388723611831665, current lr 9.845872772235014e-05, contrastive loss 3.2486085891723633, task loss 0.5063862800598145
2025-01-06 02:28:41,312 - root - INFO - [Train] Epoch 3 [3200/35182]: Training loss 0.517149806022644, current lr 9.845730650066798e-05, contrastive loss 3.21445894241333, task loss 0.4850052297115326
2025-01-06 02:30:21,886 - root - INFO - [Train] Epoch 3 [3300/35182]: Training loss 0.5318962335586548, current lr 9.845588527898581e-05, contrastive loss 3.073712110519409, task loss 0.501159131526947
2025-01-06 02:32:02,264 - root - INFO - [Train] Epoch 3 [3400/35182]: Training loss 0.5398293137550354, current lr 9.845446405730367e-05, contrastive loss 3.2188689708709717, task loss 0.5076406002044678
2025-01-06 02:33:42,856 - root - INFO - [Train] Epoch 3 [3500/35182]: Training loss 0.48181164264678955, current lr 9.84530428356215e-05, contrastive loss 3.090996742248535, task loss 0.45090168714523315
2025-01-06 02:35:23,240 - root - INFO - [Train] Epoch 3 [3600/35182]: Training loss 0.47796425223350525, current lr 9.845162161393936e-05, contrastive loss 3.2254638671875, task loss 0.44570961594581604
2025-01-06 02:37:03,819 - root - INFO - [Train] Epoch 3 [3700/35182]: Training loss 0.46420377492904663, current lr 9.845020039225718e-05, contrastive loss 3.3387773036956787, task loss 0.43081599473953247
2025-01-06 02:38:44,196 - root - INFO - [Train] Epoch 3 [3800/35182]: Training loss 0.5117740035057068, current lr 9.844877917057503e-05, contrastive loss 3.100499153137207, task loss 0.4807690382003784
2025-01-06 02:40:24,577 - root - INFO - [Train] Epoch 3 [3900/35182]: Training loss 0.5816323161125183, current lr 9.844735794889287e-05, contrastive loss 3.257552146911621, task loss 0.5490567684173584
2025-01-06 02:42:05,162 - root - INFO - [Train] Epoch 3 [4000/35182]: Training loss 0.6007242798805237, current lr 9.844593672721072e-05, contrastive loss 3.0201611518859863, task loss 0.570522665977478
2025-01-06 02:43:45,546 - root - INFO - [Train] Epoch 3 [4100/35182]: Training loss 0.5780894160270691, current lr 9.844451550552856e-05, contrastive loss 3.099277973175049, task loss 0.5470966100692749
2025-01-06 02:45:26,124 - root - INFO - [Train] Epoch 3 [4200/35182]: Training loss 0.5928963422775269, current lr 9.84430942838464e-05, contrastive loss 3.0859315395355225, task loss 0.5620370507240295
2025-01-06 02:47:06,496 - root - INFO - [Train] Epoch 3 [4300/35182]: Training loss 0.5453625321388245, current lr 9.844167306216425e-05, contrastive loss 3.1413064002990723, task loss 0.513949453830719
2025-01-06 02:48:47,084 - root - INFO - [Train] Epoch 3 [4400/35182]: Training loss 0.5378676056861877, current lr 9.844025184048208e-05, contrastive loss 2.9741129875183105, task loss 0.5081264972686768
2025-01-06 02:50:27,467 - root - INFO - [Train] Epoch 3 [4500/35182]: Training loss 0.45439305901527405, current lr 9.843883061879993e-05, contrastive loss 3.0448997020721436, task loss 0.42394405603408813
2025-01-06 02:52:08,055 - root - INFO - [Train] Epoch 3 [4600/35182]: Training loss 0.5001726746559143, current lr 9.843740939711776e-05, contrastive loss 3.330099582672119, task loss 0.4668716788291931
2025-01-06 02:53:48,423 - root - INFO - [Train] Epoch 3 [4700/35182]: Training loss 0.5671730041503906, current lr 9.84359881754356e-05, contrastive loss 3.198153018951416, task loss 0.5351914763450623
2025-01-06 02:55:29,027 - root - INFO - [Train] Epoch 3 [4800/35182]: Training loss 0.533821165561676, current lr 9.843456695375345e-05, contrastive loss 3.1738030910491943, task loss 0.5020831227302551
2025-01-06 02:57:09,411 - root - INFO - [Train] Epoch 3 [4900/35182]: Training loss 0.4922429621219635, current lr 9.843314573207129e-05, contrastive loss 3.170635223388672, task loss 0.4605365991592407
2025-01-06 02:58:49,980 - root - INFO - [Train] Epoch 3 [5000/35182]: Training loss 0.45662081241607666, current lr 9.843172451038914e-05, contrastive loss 3.115565776824951, task loss 0.4254651665687561
2025-01-06 03:00:30,344 - root - INFO - [Train] Epoch 3 [5100/35182]: Training loss 0.5727277994155884, current lr 9.843030328870698e-05, contrastive loss 3.117124557495117, task loss 0.5415565371513367
2025-01-06 03:02:10,962 - root - INFO - [Train] Epoch 3 [5200/35182]: Training loss 0.41485604643821716, current lr 9.842888206702482e-05, contrastive loss 3.243569850921631, task loss 0.3824203610420227
2025-01-06 03:03:51,347 - root - INFO - [Train] Epoch 3 [5300/35182]: Training loss 0.5180384516716003, current lr 9.842746084534265e-05, contrastive loss 3.098583221435547, task loss 0.4870526194572449
2025-01-06 03:05:31,924 - root - INFO - [Train] Epoch 3 [5400/35182]: Training loss 0.5097426772117615, current lr 9.842603962366051e-05, contrastive loss 3.110213279724121, task loss 0.47864052653312683
2025-01-06 03:07:12,290 - root - INFO - [Train] Epoch 3 [5500/35182]: Training loss 0.5426010489463806, current lr 9.842461840197834e-05, contrastive loss 3.2309064865112305, task loss 0.5102919936180115
2025-01-06 03:08:52,886 - root - INFO - [Train] Epoch 3 [5600/35182]: Training loss 0.5721178650856018, current lr 9.84231971802962e-05, contrastive loss 3.0963358879089355, task loss 0.5411545038223267
2025-01-06 03:10:33,250 - root - INFO - [Train] Epoch 3 [5700/35182]: Training loss 0.5748066306114197, current lr 9.842177595861403e-05, contrastive loss 3.1379566192626953, task loss 0.543427050113678
2025-01-06 03:12:13,851 - root - INFO - [Train] Epoch 3 [5800/35182]: Training loss 0.4787585139274597, current lr 9.842035473693187e-05, contrastive loss 3.280515193939209, task loss 0.445953369140625
2025-01-06 03:13:54,224 - root - INFO - [Train] Epoch 3 [5900/35182]: Training loss 0.5609079003334045, current lr 9.841893351524971e-05, contrastive loss 3.419949531555176, task loss 0.5267084240913391
2025-01-06 03:15:34,823 - root - INFO - [Train] Epoch 3 [6000/35182]: Training loss 0.6225571036338806, current lr 9.841751229356756e-05, contrastive loss 3.185009717941284, task loss 0.590707004070282
2025-01-06 03:17:15,201 - root - INFO - [Train] Epoch 3 [6100/35182]: Training loss 0.49281755089759827, current lr 9.84160910718854e-05, contrastive loss 3.2486414909362793, task loss 0.46033114194869995
2025-01-06 03:18:55,571 - root - INFO - [Train] Epoch 3 [6200/35182]: Training loss 0.5029544830322266, current lr 9.841466985020324e-05, contrastive loss 3.1347007751464844, task loss 0.471607506275177
2025-01-06 03:20:36,173 - root - INFO - [Train] Epoch 3 [6300/35182]: Training loss 0.5664559006690979, current lr 9.841324862852109e-05, contrastive loss 3.2328410148620605, task loss 0.5341274738311768
2025-01-06 03:22:16,545 - root - INFO - [Train] Epoch 3 [6400/35182]: Training loss 0.527044951915741, current lr 9.841182740683892e-05, contrastive loss 3.170034408569336, task loss 0.4953446090221405
2025-01-06 03:23:57,136 - root - INFO - [Train] Epoch 3 [6500/35182]: Training loss 0.5034574270248413, current lr 9.841040618515677e-05, contrastive loss 3.1251869201660156, task loss 0.47220557928085327
2025-01-06 03:25:37,511 - root - INFO - [Train] Epoch 3 [6600/35182]: Training loss 0.5522668957710266, current lr 9.84089849634746e-05, contrastive loss 3.0752696990966797, task loss 0.5215141773223877
2025-01-06 03:27:18,088 - root - INFO - [Train] Epoch 3 [6700/35182]: Training loss 0.5811729431152344, current lr 9.840756374179245e-05, contrastive loss 3.1052441596984863, task loss 0.5501205325126648
2025-01-06 03:28:58,464 - root - INFO - [Train] Epoch 3 [6800/35182]: Training loss 0.5318782925605774, current lr 9.840614252011029e-05, contrastive loss 3.025815486907959, task loss 0.5016201138496399
2025-01-06 03:30:39,056 - root - INFO - [Train] Epoch 3 [6900/35182]: Training loss 0.5174311399459839, current lr 9.840472129842813e-05, contrastive loss 3.1877894401550293, task loss 0.48555323481559753
2025-01-06 03:32:19,444 - root - INFO - [Train] Epoch 3 [7000/35182]: Training loss 0.557741105556488, current lr 9.840330007674598e-05, contrastive loss 3.055091381072998, task loss 0.5271902084350586
2025-01-06 03:34:00,027 - root - INFO - [Train] Epoch 3 [7100/35182]: Training loss 0.47293001413345337, current lr 9.840187885506382e-05, contrastive loss 3.049463987350464, task loss 0.4424353837966919
2025-01-06 03:35:40,403 - root - INFO - [Train] Epoch 3 [7200/35182]: Training loss 0.5800229907035828, current lr 9.840045763338166e-05, contrastive loss 3.08646297454834, task loss 0.5491583347320557
2025-01-06 03:37:21,001 - root - INFO - [Train] Epoch 3 [7300/35182]: Training loss 0.5016928315162659, current lr 9.839903641169949e-05, contrastive loss 3.1288580894470215, task loss 0.4704042673110962
2025-01-06 03:39:01,385 - root - INFO - [Train] Epoch 3 [7400/35182]: Training loss 0.5606706142425537, current lr 9.839761519001735e-05, contrastive loss 3.1545262336730957, task loss 0.5291253328323364
2025-01-06 03:40:41,953 - root - INFO - [Train] Epoch 3 [7500/35182]: Training loss 0.5334680676460266, current lr 9.839619396833518e-05, contrastive loss 3.1212029457092285, task loss 0.5022560358047485
2025-01-06 03:42:22,308 - root - INFO - [Train] Epoch 3 [7600/35182]: Training loss 0.6233859658241272, current lr 9.839477274665304e-05, contrastive loss 3.0533039569854736, task loss 0.5928529500961304
2025-01-06 03:44:02,922 - root - INFO - [Train] Epoch 3 [7700/35182]: Training loss 0.5776280760765076, current lr 9.839335152497087e-05, contrastive loss 3.233734607696533, task loss 0.5452907085418701
2025-01-06 03:45:43,285 - root - INFO - [Train] Epoch 3 [7800/35182]: Training loss 0.4782981872558594, current lr 9.839193030328871e-05, contrastive loss 3.1598455905914307, task loss 0.44669973850250244
2025-01-06 03:47:23,893 - root - INFO - [Train] Epoch 3 [7900/35182]: Training loss 0.5251144170761108, current lr 9.839050908160655e-05, contrastive loss 3.2275407314300537, task loss 0.4928390383720398
2025-01-06 03:49:04,267 - root - INFO - [Train] Epoch 3 [8000/35182]: Training loss 0.5308515429496765, current lr 9.83890878599244e-05, contrastive loss 3.1818995475769043, task loss 0.49903255701065063
2025-01-06 03:50:44,855 - root - INFO - [Train] Epoch 3 [8100/35182]: Training loss 0.49739307165145874, current lr 9.838766663824224e-05, contrastive loss 3.093456745147705, task loss 0.4664584994316101
2025-01-06 03:52:25,238 - root - INFO - [Train] Epoch 3 [8200/35182]: Training loss 0.46443507075309753, current lr 9.838624541656007e-05, contrastive loss 3.1979565620422363, task loss 0.43245550990104675
2025-01-06 03:54:05,832 - root - INFO - [Train] Epoch 3 [8300/35182]: Training loss 0.5115563869476318, current lr 9.838482419487793e-05, contrastive loss 3.1573309898376465, task loss 0.47998306155204773
2025-01-06 03:55:46,227 - root - INFO - [Train] Epoch 3 [8400/35182]: Training loss 0.5253613591194153, current lr 9.838340297319576e-05, contrastive loss 3.1970605850219727, task loss 0.4933907389640808
2025-01-06 03:57:26,606 - root - INFO - [Train] Epoch 3 [8500/35182]: Training loss 0.49550721049308777, current lr 9.838198175151361e-05, contrastive loss 2.995609760284424, task loss 0.46555110812187195
2025-01-06 03:59:07,179 - root - INFO - [Train] Epoch 3 [8600/35182]: Training loss 0.5386390089988708, current lr 9.838056052983144e-05, contrastive loss 3.147796392440796, task loss 0.507161021232605
2025-01-06 04:00:47,568 - root - INFO - [Train] Epoch 3 [8700/35182]: Training loss 0.5165438652038574, current lr 9.837913930814929e-05, contrastive loss 3.076603412628174, task loss 0.4857778549194336
2025-01-06 04:02:28,145 - root - INFO - [Train] Epoch 3 [8800/35182]: Training loss 0.5034274458885193, current lr 9.837771808646713e-05, contrastive loss 3.1648828983306885, task loss 0.47177860140800476
2025-01-06 04:04:08,531 - root - INFO - [Train] Epoch 3 [8900/35182]: Training loss 0.48473653197288513, current lr 9.837629686478497e-05, contrastive loss 3.174792766571045, task loss 0.4529885947704315
2025-01-06 04:05:49,114 - root - INFO - [Train] Epoch 3 [9000/35182]: Training loss 0.5237011313438416, current lr 9.837487564310282e-05, contrastive loss 3.040595054626465, task loss 0.49329516291618347
2025-01-06 04:07:29,489 - root - INFO - [Train] Epoch 3 [9100/35182]: Training loss 0.492379754781723, current lr 9.837345442142066e-05, contrastive loss 3.039855480194092, task loss 0.4619812071323395
2025-01-06 04:09:10,091 - root - INFO - [Train] Epoch 3 [9200/35182]: Training loss 0.5404614210128784, current lr 9.83720331997385e-05, contrastive loss 3.2376585006713867, task loss 0.5080848336219788
2025-01-06 04:10:50,472 - root - INFO - [Train] Epoch 3 [9300/35182]: Training loss 0.565412163734436, current lr 9.837061197805633e-05, contrastive loss 3.0233402252197266, task loss 0.5351787805557251
2025-01-06 04:12:31,038 - root - INFO - [Train] Epoch 3 [9400/35182]: Training loss 0.5049033761024475, current lr 9.836919075637419e-05, contrastive loss 3.2141847610473633, task loss 0.4727615416049957
2025-01-06 04:14:11,412 - root - INFO - [Train] Epoch 3 [9500/35182]: Training loss 0.4987916350364685, current lr 9.836776953469202e-05, contrastive loss 3.217813014984131, task loss 0.4666135013103485
2025-01-06 04:15:52,002 - root - INFO - [Train] Epoch 3 [9600/35182]: Training loss 0.5243438482284546, current lr 9.836634831300988e-05, contrastive loss 2.978473663330078, task loss 0.494559109210968
2025-01-06 04:17:32,383 - root - INFO - [Train] Epoch 3 [9700/35182]: Training loss 0.5245262384414673, current lr 9.836492709132771e-05, contrastive loss 3.1604256629943848, task loss 0.49292197823524475
2025-01-06 04:19:12,971 - root - INFO - [Train] Epoch 3 [9800/35182]: Training loss 0.49683284759521484, current lr 9.836350586964555e-05, contrastive loss 3.0467827320098877, task loss 0.4663650095462799
2025-01-06 04:20:53,350 - root - INFO - [Train] Epoch 3 [9900/35182]: Training loss 0.5379031300544739, current lr 9.83620846479634e-05, contrastive loss 3.1331820487976074, task loss 0.5065712928771973
2025-01-06 04:22:33,936 - root - INFO - [Train] Epoch 3 [10000/35182]: Training loss 0.4412417411804199, current lr 9.836066342628124e-05, contrastive loss 3.2445812225341797, task loss 0.40879592299461365
2025-01-06 04:24:14,314 - root - INFO - [Train] Epoch 3 [10100/35182]: Training loss 0.47666171193122864, current lr 9.835924220459908e-05, contrastive loss 3.119554281234741, task loss 0.44546616077423096
2025-01-06 04:25:54,896 - root - INFO - [Train] Epoch 3 [10200/35182]: Training loss 0.5805212259292603, current lr 9.835782098291691e-05, contrastive loss 3.0216968059539795, task loss 0.5503042340278625
2025-01-06 04:27:35,284 - root - INFO - [Train] Epoch 3 [10300/35182]: Training loss 0.5103609561920166, current lr 9.835639976123477e-05, contrastive loss 3.128117799758911, task loss 0.4790797531604767
2025-01-06 04:29:15,866 - root - INFO - [Train] Epoch 3 [10400/35182]: Training loss 0.5245761871337891, current lr 9.83549785395526e-05, contrastive loss 3.2584965229034424, task loss 0.49199122190475464
2025-01-06 04:30:56,254 - root - INFO - [Train] Epoch 3 [10500/35182]: Training loss 0.5414575338363647, current lr 9.835355731787045e-05, contrastive loss 3.3482117652893066, task loss 0.5079753994941711
2025-01-06 04:32:36,844 - root - INFO - [Train] Epoch 3 [10600/35182]: Training loss 0.5936317443847656, current lr 9.835213609618828e-05, contrastive loss 3.0391664505004883, task loss 0.5632400512695312
2025-01-06 04:34:17,209 - root - INFO - [Train] Epoch 3 [10700/35182]: Training loss 0.5736547112464905, current lr 9.835071487450613e-05, contrastive loss 3.0917651653289795, task loss 0.5427370667457581
2025-01-06 04:35:57,793 - root - INFO - [Train] Epoch 3 [10800/35182]: Training loss 0.5490621328353882, current lr 9.834929365282397e-05, contrastive loss 2.9664483070373535, task loss 0.5193976759910583
2025-01-06 04:37:38,166 - root - INFO - [Train] Epoch 3 [10900/35182]: Training loss 0.577235758304596, current lr 9.834787243114181e-05, contrastive loss 3.297489643096924, task loss 0.5442608594894409
2025-01-06 04:39:18,535 - root - INFO - [Train] Epoch 3 [11000/35182]: Training loss 0.516342282295227, current lr 9.834645120945966e-05, contrastive loss 3.0626964569091797, task loss 0.4857152998447418
2025-01-06 04:40:59,131 - root - INFO - [Train] Epoch 3 [11100/35182]: Training loss 0.4818328022956848, current lr 9.834502998777749e-05, contrastive loss 3.398202896118164, task loss 0.44785076379776
2025-01-06 04:42:39,502 - root - INFO - [Train] Epoch 3 [11200/35182]: Training loss 0.5437856316566467, current lr 9.834360876609534e-05, contrastive loss 3.0235843658447266, task loss 0.5135498046875
2025-01-06 04:44:20,099 - root - INFO - [Train] Epoch 3 [11300/35182]: Training loss 0.5053901672363281, current lr 9.834218754441317e-05, contrastive loss 3.241138458251953, task loss 0.47297877073287964
2025-01-06 04:46:00,474 - root - INFO - [Train] Epoch 3 [11400/35182]: Training loss 0.4902036190032959, current lr 9.834076632273103e-05, contrastive loss 3.0336313247680664, task loss 0.45986729860305786
2025-01-06 04:47:41,073 - root - INFO - [Train] Epoch 3 [11500/35182]: Training loss 0.506683349609375, current lr 9.833934510104886e-05, contrastive loss 3.1814937591552734, task loss 0.47486841678619385
2025-01-06 04:49:21,434 - root - INFO - [Train] Epoch 3 [11600/35182]: Training loss 0.5225988030433655, current lr 9.833792387936672e-05, contrastive loss 2.924111843109131, task loss 0.49335768818855286
2025-01-06 04:51:02,026 - root - INFO - [Train] Epoch 3 [11700/35182]: Training loss 0.5761097073554993, current lr 9.833650265768455e-05, contrastive loss 3.1592254638671875, task loss 0.544517457485199
2025-01-06 04:52:42,388 - root - INFO - [Train] Epoch 3 [11800/35182]: Training loss 0.5609481930732727, current lr 9.833508143600239e-05, contrastive loss 3.208683490753174, task loss 0.5288613438606262
2025-01-06 04:54:22,993 - root - INFO - [Train] Epoch 3 [11900/35182]: Training loss 0.49183860421180725, current lr 9.833366021432023e-05, contrastive loss 3.1506125926971436, task loss 0.4603324830532074
2025-01-06 04:56:03,375 - root - INFO - [Train] Epoch 3 [12000/35182]: Training loss 0.5172222852706909, current lr 9.833223899263808e-05, contrastive loss 3.1013271808624268, task loss 0.4862090051174164
2025-01-06 04:57:43,966 - root - INFO - [Train] Epoch 3 [12100/35182]: Training loss 0.5300486087799072, current lr 9.833081777095592e-05, contrastive loss 3.06545090675354, task loss 0.49939411878585815
2025-01-06 04:59:24,352 - root - INFO - [Train] Epoch 3 [12200/35182]: Training loss 0.5026772022247314, current lr 9.832939654927375e-05, contrastive loss 3.0026817321777344, task loss 0.472650408744812
2025-01-06 05:01:04,941 - root - INFO - [Train] Epoch 3 [12300/35182]: Training loss 0.5657294988632202, current lr 9.832797532759161e-05, contrastive loss 3.226977825164795, task loss 0.5334597229957581
2025-01-06 05:02:45,318 - root - INFO - [Train] Epoch 3 [12400/35182]: Training loss 0.5036492347717285, current lr 9.832655410590944e-05, contrastive loss 3.0090548992156982, task loss 0.4735586941242218
2025-01-06 05:04:25,903 - root - INFO - [Train] Epoch 3 [12500/35182]: Training loss 0.47280800342559814, current lr 9.83251328842273e-05, contrastive loss 2.8630199432373047, task loss 0.4441778063774109
2025-01-06 05:06:06,299 - root - INFO - [Train] Epoch 3 [12600/35182]: Training loss 0.48492950201034546, current lr 9.832371166254512e-05, contrastive loss 3.2768306732177734, task loss 0.45216119289398193
2025-01-06 05:07:46,870 - root - INFO - [Train] Epoch 3 [12700/35182]: Training loss 0.5304057598114014, current lr 9.832229044086297e-05, contrastive loss 2.917267084121704, task loss 0.5012331008911133
2025-01-06 05:09:27,249 - root - INFO - [Train] Epoch 3 [12800/35182]: Training loss 0.6177653074264526, current lr 9.832086921918081e-05, contrastive loss 3.2027316093444824, task loss 0.5857380032539368
2025-01-06 05:11:07,834 - root - INFO - [Train] Epoch 3 [12900/35182]: Training loss 0.5418024063110352, current lr 9.831944799749865e-05, contrastive loss 2.8450074195861816, task loss 0.5133523344993591
2025-01-06 05:12:48,209 - root - INFO - [Train] Epoch 3 [13000/35182]: Training loss 0.6286582946777344, current lr 9.83180267758165e-05, contrastive loss 3.085819721221924, task loss 0.597800076007843
2025-01-06 05:14:28,799 - root - INFO - [Train] Epoch 3 [13100/35182]: Training loss 0.5247095227241516, current lr 9.831660555413433e-05, contrastive loss 3.113825798034668, task loss 0.49357128143310547
2025-01-06 05:16:09,168 - root - INFO - [Train] Epoch 3 [13200/35182]: Training loss 0.45009639859199524, current lr 9.831518433245218e-05, contrastive loss 3.0533876419067383, task loss 0.41956251859664917
2025-01-06 05:17:49,545 - root - INFO - [Train] Epoch 3 [13300/35182]: Training loss 0.6202148795127869, current lr 9.831376311077001e-05, contrastive loss 3.2036750316619873, task loss 0.5881781578063965
2025-01-06 05:19:30,150 - root - INFO - [Train] Epoch 3 [13400/35182]: Training loss 0.4837241768836975, current lr 9.831234188908787e-05, contrastive loss 3.269136428833008, task loss 0.451032817363739
2025-01-06 05:21:10,516 - root - INFO - [Train] Epoch 3 [13500/35182]: Training loss 0.5564522743225098, current lr 9.83109206674057e-05, contrastive loss 2.9087557792663574, task loss 0.5273647308349609
2025-01-06 05:22:51,118 - root - INFO - [Train] Epoch 3 [13600/35182]: Training loss 0.5639283061027527, current lr 9.830949944572356e-05, contrastive loss 3.082568645477295, task loss 0.5331026315689087
2025-01-06 05:24:31,498 - root - INFO - [Train] Epoch 3 [13700/35182]: Training loss 0.5111839175224304, current lr 9.830807822404139e-05, contrastive loss 3.074287176132202, task loss 0.48044106364250183
2025-01-06 05:26:12,079 - root - INFO - [Train] Epoch 3 [13800/35182]: Training loss 0.47467610239982605, current lr 9.830665700235923e-05, contrastive loss 3.1802573204040527, task loss 0.4428735375404358
2025-01-06 05:27:52,464 - root - INFO - [Train] Epoch 3 [13900/35182]: Training loss 0.5494240522384644, current lr 9.830523578067707e-05, contrastive loss 2.9253041744232178, task loss 0.5201709866523743
2025-01-06 05:29:33,044 - root - INFO - [Train] Epoch 3 [14000/35182]: Training loss 0.5658077001571655, current lr 9.830381455899492e-05, contrastive loss 3.1793572902679443, task loss 0.534014105796814
2025-01-06 05:31:13,414 - root - INFO - [Train] Epoch 3 [14100/35182]: Training loss 0.5452810525894165, current lr 9.830239333731276e-05, contrastive loss 3.0659420490264893, task loss 0.5146216154098511
2025-01-06 05:32:54,003 - root - INFO - [Train] Epoch 3 [14200/35182]: Training loss 0.5384684801101685, current lr 9.830097211563059e-05, contrastive loss 3.141885280609131, task loss 0.5070496201515198
2025-01-06 05:34:34,372 - root - INFO - [Train] Epoch 3 [14300/35182]: Training loss 0.496773898601532, current lr 9.829955089394845e-05, contrastive loss 3.3001110553741455, task loss 0.4637727737426758
2025-01-06 05:36:14,974 - root - INFO - [Train] Epoch 3 [14400/35182]: Training loss 0.5659712553024292, current lr 9.829812967226628e-05, contrastive loss 3.126551866531372, task loss 0.5347057580947876
2025-01-06 05:37:55,358 - root - INFO - [Train] Epoch 3 [14500/35182]: Training loss 0.5315746665000916, current lr 9.829670845058414e-05, contrastive loss 3.2891287803649902, task loss 0.498683363199234
2025-01-06 05:39:35,943 - root - INFO - [Train] Epoch 3 [14600/35182]: Training loss 0.5074450373649597, current lr 9.829528722890197e-05, contrastive loss 3.3445992469787598, task loss 0.4739990532398224
2025-01-06 05:41:16,325 - root - INFO - [Train] Epoch 3 [14700/35182]: Training loss 0.4635249972343445, current lr 9.829386600721981e-05, contrastive loss 3.1555233001708984, task loss 0.4319697618484497
2025-01-06 05:42:56,905 - root - INFO - [Train] Epoch 3 [14800/35182]: Training loss 0.6064794063568115, current lr 9.829244478553765e-05, contrastive loss 3.191929340362549, task loss 0.5745601058006287
2025-01-06 05:44:37,296 - root - INFO - [Train] Epoch 3 [14900/35182]: Training loss 0.6369866132736206, current lr 9.82910235638555e-05, contrastive loss 3.221683979034424, task loss 0.604769766330719
2025-01-06 05:46:17,872 - root - INFO - [Train] Epoch 3 [15000/35182]: Training loss 0.4813369810581207, current lr 9.828960234217334e-05, contrastive loss 3.111739158630371, task loss 0.45021960139274597
2025-01-06 05:47:58,268 - root - INFO - [Train] Epoch 3 [15100/35182]: Training loss 0.5268638730049133, current lr 9.828818112049117e-05, contrastive loss 3.196160078048706, task loss 0.49490225315093994
2025-01-06 05:49:38,842 - root - INFO - [Train] Epoch 3 [15200/35182]: Training loss 0.5321894884109497, current lr 9.828675989880903e-05, contrastive loss 3.134274482727051, task loss 0.5008467435836792
2025-01-06 05:51:19,213 - root - INFO - [Train] Epoch 3 [15300/35182]: Training loss 0.5325971841812134, current lr 9.828533867712686e-05, contrastive loss 3.1409504413604736, task loss 0.5011876821517944
2025-01-06 05:52:59,808 - root - INFO - [Train] Epoch 3 [15400/35182]: Training loss 0.525456428527832, current lr 9.828391745544471e-05, contrastive loss 3.1009459495544434, task loss 0.4944469928741455
2025-01-06 05:54:40,192 - root - INFO - [Train] Epoch 3 [15500/35182]: Training loss 0.5311874151229858, current lr 9.828249623376254e-05, contrastive loss 3.233208179473877, task loss 0.4988553524017334
2025-01-06 05:56:20,581 - root - INFO - [Train] Epoch 3 [15600/35182]: Training loss 0.47695374488830566, current lr 9.82810750120804e-05, contrastive loss 3.2300753593444824, task loss 0.4446530044078827
2025-01-06 05:58:01,152 - root - INFO - [Train] Epoch 3 [15700/35182]: Training loss 0.5724348425865173, current lr 9.827965379039823e-05, contrastive loss 3.1198768615722656, task loss 0.5412361025810242
2025-01-06 05:59:41,533 - root - INFO - [Train] Epoch 3 [15800/35182]: Training loss 0.5458303093910217, current lr 9.827823256871607e-05, contrastive loss 3.2581400871276855, task loss 0.5132489204406738
2025-01-06 06:01:22,120 - root - INFO - [Train] Epoch 3 [15900/35182]: Training loss 0.553706169128418, current lr 9.827681134703392e-05, contrastive loss 3.214909076690674, task loss 0.521557092666626
2025-01-06 06:03:02,514 - root - INFO - [Train] Epoch 3 [16000/35182]: Training loss 0.5111169815063477, current lr 9.827539012535175e-05, contrastive loss 3.20943021774292, task loss 0.47902268171310425
2025-01-06 06:04:43,080 - root - INFO - [Train] Epoch 3 [16100/35182]: Training loss 0.5317040085792542, current lr 9.82739689036696e-05, contrastive loss 3.141465425491333, task loss 0.5002893805503845
2025-01-06 06:06:23,447 - root - INFO - [Train] Epoch 3 [16200/35182]: Training loss 0.5186625719070435, current lr 9.827254768198743e-05, contrastive loss 3.312105894088745, task loss 0.4855414927005768
2025-01-06 06:08:04,044 - root - INFO - [Train] Epoch 3 [16300/35182]: Training loss 0.5241331458091736, current lr 9.827112646030529e-05, contrastive loss 3.1143600940704346, task loss 0.49298954010009766
2025-01-06 06:09:44,420 - root - INFO - [Train] Epoch 3 [16400/35182]: Training loss 0.5441789031028748, current lr 9.826970523862312e-05, contrastive loss 3.1672630310058594, task loss 0.5125062465667725
2025-01-06 06:11:25,018 - root - INFO - [Train] Epoch 3 [16500/35182]: Training loss 0.48962894082069397, current lr 9.826828401694098e-05, contrastive loss 3.238720655441284, task loss 0.4572417438030243
2025-01-06 06:13:05,396 - root - INFO - [Train] Epoch 3 [16600/35182]: Training loss 0.5062113404273987, current lr 9.82668627952588e-05, contrastive loss 3.136666774749756, task loss 0.47484466433525085
2025-01-06 06:14:45,988 - root - INFO - [Train] Epoch 3 [16700/35182]: Training loss 0.5704213380813599, current lr 9.826544157357665e-05, contrastive loss 3.126477003097534, task loss 0.5391565561294556
2025-01-06 06:16:26,384 - root - INFO - [Train] Epoch 3 [16800/35182]: Training loss 0.5052200555801392, current lr 9.826402035189449e-05, contrastive loss 3.062145948410034, task loss 0.47459861636161804
2025-01-06 06:18:07,059 - root - INFO - [Train] Epoch 3 [16900/35182]: Training loss 0.44873401522636414, current lr 9.826259913021234e-05, contrastive loss 3.0795223712921143, task loss 0.41793879866600037
2025-01-06 06:19:47,430 - root - INFO - [Train] Epoch 3 [17000/35182]: Training loss 0.531889021396637, current lr 9.826117790853018e-05, contrastive loss 2.9903206825256348, task loss 0.5019857883453369
2025-01-06 06:21:28,025 - root - INFO - [Train] Epoch 3 [17100/35182]: Training loss 0.5718123316764832, current lr 9.825975668684801e-05, contrastive loss 3.06034517288208, task loss 0.5412088632583618
2025-01-06 06:23:08,408 - root - INFO - [Train] Epoch 3 [17200/35182]: Training loss 0.5513233542442322, current lr 9.825833546516587e-05, contrastive loss 3.003784418106079, task loss 0.5212855339050293
2025-01-06 06:24:48,988 - root - INFO - [Train] Epoch 3 [17300/35182]: Training loss 0.5533647537231445, current lr 9.82569142434837e-05, contrastive loss 3.051069498062134, task loss 0.5228540897369385
2025-01-06 06:26:29,357 - root - INFO - [Train] Epoch 3 [17400/35182]: Training loss 0.5535411238670349, current lr 9.825549302180155e-05, contrastive loss 3.1729142665863037, task loss 0.5218119621276855
2025-01-06 06:28:09,954 - root - INFO - [Train] Epoch 3 [17500/35182]: Training loss 0.5020421147346497, current lr 9.825407180011938e-05, contrastive loss 3.142904281616211, task loss 0.4706130921840668
2025-01-06 09:12:52,688 - root - INFO - [Valid] Evaluation Results: HR@5:0.0449,NDCG@5:0.0262,HR@10:0.0929,NDCG@10:0.0416,HR@20:0.1877,NDCG@20:0.0652
2025-01-06 09:12:53,563 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-06 09:13:03,822 - root - INFO - [Train] Epoch 3 [17600/35182]: Training loss 0.5030770897865295, current lr 9.825265057843724e-05, contrastive loss 3.0326390266418457, task loss 0.4727506935596466
2025-01-06 09:14:44,205 - root - INFO - [Train] Epoch 3 [17700/35182]: Training loss 0.5466146469116211, current lr 9.825122935675507e-05, contrastive loss 3.176544189453125, task loss 0.5148491859436035
2025-01-06 09:16:24,594 - root - INFO - [Train] Epoch 3 [17800/35182]: Training loss 0.5762887001037598, current lr 9.824980813507291e-05, contrastive loss 3.341609001159668, task loss 0.5428726077079773
2025-01-06 09:18:05,189 - root - INFO - [Train] Epoch 3 [17900/35182]: Training loss 0.5100365281105042, current lr 9.824838691339076e-05, contrastive loss 3.2431557178497314, task loss 0.4776049852371216
2025-01-06 09:19:45,864 - root - INFO - [Train] Epoch 3 [18000/35182]: Training loss 0.5728773474693298, current lr 9.824696569170859e-05, contrastive loss 3.244670867919922, task loss 0.5404306650161743
2025-01-06 09:21:26,264 - root - INFO - [Train] Epoch 3 [18100/35182]: Training loss 0.5565192103385925, current lr 9.824554447002644e-05, contrastive loss 2.947704553604126, task loss 0.5270421504974365
2025-01-06 09:23:06,835 - root - INFO - [Train] Epoch 3 [18200/35182]: Training loss 0.5117495059967041, current lr 9.824412324834427e-05, contrastive loss 3.042100429534912, task loss 0.4813285171985626
2025-01-06 09:24:47,236 - root - INFO - [Train] Epoch 3 [18300/35182]: Training loss 0.5586118102073669, current lr 9.824270202666213e-05, contrastive loss 3.160550594329834, task loss 0.5270063281059265
2025-01-06 09:26:27,923 - root - INFO - [Train] Epoch 3 [18400/35182]: Training loss 0.5131186246871948, current lr 9.824128080497996e-05, contrastive loss 3.3387646675109863, task loss 0.4797309935092926
2025-01-06 09:28:08,328 - root - INFO - [Train] Epoch 3 [18500/35182]: Training loss 0.5241602063179016, current lr 9.823985958329782e-05, contrastive loss 3.06156063079834, task loss 0.4935445785522461
2025-01-06 09:29:49,001 - root - INFO - [Train] Epoch 3 [18600/35182]: Training loss 0.4802539050579071, current lr 9.823843836161565e-05, contrastive loss 3.075554132461548, task loss 0.44949835538864136
2025-01-06 09:31:29,400 - root - INFO - [Train] Epoch 3 [18700/35182]: Training loss 0.5668861269950867, current lr 9.823701713993349e-05, contrastive loss 3.2011499404907227, task loss 0.5348746180534363
2025-01-06 09:33:10,076 - root - INFO - [Train] Epoch 3 [18800/35182]: Training loss 0.6000310778617859, current lr 9.823559591825133e-05, contrastive loss 3.229037284851074, task loss 0.5677406787872314
2025-01-06 09:34:50,455 - root - INFO - [Train] Epoch 3 [18900/35182]: Training loss 0.5179490447044373, current lr 9.823417469656918e-05, contrastive loss 3.187239170074463, task loss 0.4860766530036926
2025-01-06 09:36:31,053 - root - INFO - [Train] Epoch 3 [19000/35182]: Training loss 0.5095163583755493, current lr 9.823275347488702e-05, contrastive loss 3.149371385574341, task loss 0.47802266478538513
2025-01-06 09:38:11,438 - root - INFO - [Train] Epoch 3 [19100/35182]: Training loss 0.5468606948852539, current lr 9.823133225320485e-05, contrastive loss 3.235506534576416, task loss 0.5145056247711182
2025-01-06 09:39:52,010 - root - INFO - [Train] Epoch 3 [19200/35182]: Training loss 0.5048651695251465, current lr 9.822991103152271e-05, contrastive loss 3.152141571044922, task loss 0.47334375977516174
2025-01-06 09:41:32,407 - root - INFO - [Train] Epoch 3 [19300/35182]: Training loss 0.5106769800186157, current lr 9.822848980984054e-05, contrastive loss 3.067542791366577, task loss 0.4800015687942505
2025-01-06 09:43:12,977 - root - INFO - [Train] Epoch 3 [19400/35182]: Training loss 0.5786727070808411, current lr 9.82270685881584e-05, contrastive loss 3.1519858837127686, task loss 0.5471528768539429
2025-01-06 09:44:53,362 - root - INFO - [Train] Epoch 3 [19500/35182]: Training loss 0.4785284399986267, current lr 9.822564736647622e-05, contrastive loss 3.085570812225342, task loss 0.4476727247238159
2025-01-06 09:46:33,944 - root - INFO - [Train] Epoch 3 [19600/35182]: Training loss 0.5105604529380798, current lr 9.822422614479408e-05, contrastive loss 3.147441864013672, task loss 0.4790860414505005
2025-01-06 09:48:14,333 - root - INFO - [Train] Epoch 3 [19700/35182]: Training loss 0.48689353466033936, current lr 9.822280492311191e-05, contrastive loss 3.262716770172119, task loss 0.45426636934280396
2025-01-06 09:49:54,908 - root - INFO - [Train] Epoch 3 [19800/35182]: Training loss 0.5144132971763611, current lr 9.822138370142975e-05, contrastive loss 3.4782145023345947, task loss 0.47963112592697144
2025-01-06 09:51:35,302 - root - INFO - [Train] Epoch 3 [19900/35182]: Training loss 0.5863127708435059, current lr 9.82199624797476e-05, contrastive loss 3.1340157985687256, task loss 0.5549725890159607
2025-01-06 09:53:15,969 - root - INFO - [Train] Epoch 3 [20000/35182]: Training loss 0.5518393516540527, current lr 9.821854125806543e-05, contrastive loss 3.086534261703491, task loss 0.5209739804267883
2025-01-06 09:54:56,355 - root - INFO - [Train] Epoch 3 [20100/35182]: Training loss 0.5394914746284485, current lr 9.821712003638328e-05, contrastive loss 3.3171517848968506, task loss 0.5063199400901794
2025-01-06 09:56:36,959 - root - INFO - [Train] Epoch 3 [20200/35182]: Training loss 0.5607575178146362, current lr 9.821569881470111e-05, contrastive loss 3.3086233139038086, task loss 0.5276712775230408
2025-01-06 09:58:17,347 - root - INFO - [Train] Epoch 3 [20300/35182]: Training loss 0.575901448726654, current lr 9.821427759301897e-05, contrastive loss 3.1767005920410156, task loss 0.5441344380378723
2025-01-06 09:59:58,035 - root - INFO - [Train] Epoch 3 [20400/35182]: Training loss 0.5481489300727844, current lr 9.82128563713368e-05, contrastive loss 3.396904945373535, task loss 0.5141798853874207
2025-01-06 10:01:38,445 - root - INFO - [Train] Epoch 3 [20500/35182]: Training loss 0.5538095831871033, current lr 9.821143514965466e-05, contrastive loss 3.3521580696105957, task loss 0.5202879905700684
2025-01-06 10:03:19,117 - root - INFO - [Train] Epoch 3 [20600/35182]: Training loss 0.5900660753250122, current lr 9.821001392797249e-05, contrastive loss 3.2367067337036133, task loss 0.5576990246772766
2025-01-06 10:04:59,793 - root - INFO - [Train] Epoch 3 [20700/35182]: Training loss 0.5384615659713745, current lr 9.820859270629033e-05, contrastive loss 3.139908790588379, task loss 0.5070624947547913
2025-01-06 10:06:40,187 - root - INFO - [Train] Epoch 3 [20800/35182]: Training loss 0.5106920599937439, current lr 9.820717148460817e-05, contrastive loss 3.165574550628662, task loss 0.4790363013744354
2025-01-06 10:08:20,582 - root - INFO - [Train] Epoch 3 [20900/35182]: Training loss 0.5598701238632202, current lr 9.820575026292602e-05, contrastive loss 3.32468843460083, task loss 0.5266232490539551
2025-01-06 10:10:01,187 - root - INFO - [Train] Epoch 3 [21000/35182]: Training loss 0.5482409000396729, current lr 9.820432904124386e-05, contrastive loss 3.3290679454803467, task loss 0.5149502158164978
2025-01-06 10:11:41,859 - root - INFO - [Train] Epoch 3 [21100/35182]: Training loss 0.5525680184364319, current lr 9.820290781956169e-05, contrastive loss 3.061131477355957, task loss 0.5219566822052002
2025-01-06 10:13:22,255 - root - INFO - [Train] Epoch 3 [21200/35182]: Training loss 0.4847044050693512, current lr 9.820148659787955e-05, contrastive loss 3.3517980575561523, task loss 0.4511864185333252
2025-01-06 10:15:02,935 - root - INFO - [Train] Epoch 3 [21300/35182]: Training loss 0.560535192489624, current lr 9.820006537619738e-05, contrastive loss 3.097439765930176, task loss 0.5295608043670654
2025-01-06 10:16:43,325 - root - INFO - [Train] Epoch 3 [21400/35182]: Training loss 0.5248609185218811, current lr 9.819864415451523e-05, contrastive loss 3.205455780029297, task loss 0.4928063452243805
2025-01-06 10:18:23,907 - root - INFO - [Train] Epoch 3 [21500/35182]: Training loss 0.5437228679656982, current lr 9.819722293283306e-05, contrastive loss 3.008452892303467, task loss 0.5136383175849915
2025-01-06 10:20:04,313 - root - INFO - [Train] Epoch 3 [21600/35182]: Training loss 0.5033581852912903, current lr 9.819580171115092e-05, contrastive loss 3.1745786666870117, task loss 0.4716123938560486
2025-01-06 10:21:44,991 - root - INFO - [Train] Epoch 3 [21700/35182]: Training loss 0.5566927790641785, current lr 9.819438048946875e-05, contrastive loss 2.9957261085510254, task loss 0.5267355442047119
2025-01-06 10:23:25,399 - root - INFO - [Train] Epoch 3 [21800/35182]: Training loss 0.5178665518760681, current lr 9.81929592677866e-05, contrastive loss 3.071497917175293, task loss 0.4871515929698944
2025-01-06 10:25:06,085 - root - INFO - [Train] Epoch 3 [21900/35182]: Training loss 0.6483932137489319, current lr 9.819153804610444e-05, contrastive loss 3.0972936153411865, task loss 0.6174202561378479
2025-01-06 10:26:46,487 - root - INFO - [Train] Epoch 3 [22000/35182]: Training loss 0.5115112066268921, current lr 9.819011682442227e-05, contrastive loss 3.0576000213623047, task loss 0.4809351861476898
2025-01-06 10:28:27,154 - root - INFO - [Train] Epoch 3 [22100/35182]: Training loss 0.5259584784507751, current lr 9.818869560274012e-05, contrastive loss 3.0800318717956543, task loss 0.4951581656932831
2025-01-06 10:30:07,839 - root - INFO - [Train] Epoch 3 [22200/35182]: Training loss 0.5069310069084167, current lr 9.818727438105795e-05, contrastive loss 3.320375919342041, task loss 0.4737272262573242
2025-01-06 10:31:48,225 - root - INFO - [Train] Epoch 3 [22300/35182]: Training loss 0.5328881144523621, current lr 9.818585315937581e-05, contrastive loss 3.2505273818969727, task loss 0.5003828406333923
2025-01-06 10:33:28,803 - root - INFO - [Train] Epoch 3 [22400/35182]: Training loss 0.5057443976402283, current lr 9.818443193769364e-05, contrastive loss 3.3283066749572754, task loss 0.4724613130092621
2025-01-06 10:35:09,205 - root - INFO - [Train] Epoch 3 [22500/35182]: Training loss 0.5452767610549927, current lr 9.81830107160115e-05, contrastive loss 3.1332387924194336, task loss 0.5139443874359131
2025-01-06 10:36:49,883 - root - INFO - [Train] Epoch 3 [22600/35182]: Training loss 0.5074421763420105, current lr 9.818158949432933e-05, contrastive loss 3.396602153778076, task loss 0.4734761416912079
2025-01-06 10:38:30,284 - root - INFO - [Train] Epoch 3 [22700/35182]: Training loss 0.552387535572052, current lr 9.818016827264717e-05, contrastive loss 3.1274542808532715, task loss 0.5211129784584045
2025-01-06 10:40:10,977 - root - INFO - [Train] Epoch 3 [22800/35182]: Training loss 0.5529378652572632, current lr 9.817874705096501e-05, contrastive loss 3.019733428955078, task loss 0.5227405428886414
2025-01-06 10:41:51,372 - root - INFO - [Train] Epoch 3 [22900/35182]: Training loss 0.5016716122627258, current lr 9.817732582928286e-05, contrastive loss 3.050567150115967, task loss 0.4711659252643585
2025-01-06 10:43:32,076 - root - INFO - [Train] Epoch 3 [23000/35182]: Training loss 0.46394726634025574, current lr 9.81759046076007e-05, contrastive loss 3.216769218444824, task loss 0.43177956342697144
2025-01-06 10:45:12,491 - root - INFO - [Train] Epoch 3 [23100/35182]: Training loss 0.6036322116851807, current lr 9.817448338591853e-05, contrastive loss 3.081327438354492, task loss 0.57281893491745
2025-01-06 10:46:53,144 - root - INFO - [Train] Epoch 3 [23200/35182]: Training loss 0.5106961131095886, current lr 9.817306216423639e-05, contrastive loss 3.161677837371826, task loss 0.47907933592796326
2025-01-06 10:48:33,834 - root - INFO - [Train] Epoch 3 [23300/35182]: Training loss 0.4726755917072296, current lr 9.817164094255422e-05, contrastive loss 2.971501588821411, task loss 0.44296059012413025
2025-01-06 10:50:14,248 - root - INFO - [Train] Epoch 3 [23400/35182]: Training loss 0.5255827307701111, current lr 9.817021972087207e-05, contrastive loss 3.1064553260803223, task loss 0.4945181906223297
2025-01-06 10:51:54,910 - root - INFO - [Train] Epoch 3 [23500/35182]: Training loss 0.53132164478302, current lr 9.81687984991899e-05, contrastive loss 3.151369094848633, task loss 0.4998079240322113
2025-01-06 10:53:35,323 - root - INFO - [Train] Epoch 3 [23600/35182]: Training loss 0.4898548126220703, current lr 9.816737727750776e-05, contrastive loss 3.136902332305908, task loss 0.45848578214645386
2025-01-06 10:55:16,004 - root - INFO - [Train] Epoch 3 [23700/35182]: Training loss 0.538985013961792, current lr 9.816595605582559e-05, contrastive loss 3.090989589691162, task loss 0.5080751180648804
2025-01-06 10:56:56,409 - root - INFO - [Train] Epoch 3 [23800/35182]: Training loss 0.49571293592453003, current lr 9.816453483414343e-05, contrastive loss 3.0971879959106445, task loss 0.464741051197052
2025-01-06 10:58:37,084 - root - INFO - [Train] Epoch 3 [23900/35182]: Training loss 0.49780723452568054, current lr 9.816311361246128e-05, contrastive loss 3.1118524074554443, task loss 0.46668872237205505
2025-01-06 11:00:17,496 - root - INFO - [Train] Epoch 3 [24000/35182]: Training loss 0.5721449851989746, current lr 9.816169239077911e-05, contrastive loss 2.968352794647217, task loss 0.5424614548683167
2025-01-06 11:01:58,173 - root - INFO - [Train] Epoch 3 [24100/35182]: Training loss 0.5467643737792969, current lr 9.816027116909697e-05, contrastive loss 3.1009721755981445, task loss 0.5157546401023865
2025-01-06 11:03:38,843 - root - INFO - [Train] Epoch 3 [24200/35182]: Training loss 0.5269788503646851, current lr 9.81588499474148e-05, contrastive loss 3.26676607131958, task loss 0.49431121349334717
2025-01-06 11:05:19,254 - root - INFO - [Train] Epoch 3 [24300/35182]: Training loss 0.5835682153701782, current lr 9.815742872573265e-05, contrastive loss 3.1059107780456543, task loss 0.5525091290473938
2025-01-06 11:06:59,927 - root - INFO - [Train] Epoch 3 [24400/35182]: Training loss 0.5105950236320496, current lr 9.815600750405048e-05, contrastive loss 3.197803497314453, task loss 0.47861701250076294
2025-01-06 11:08:40,318 - root - INFO - [Train] Epoch 3 [24500/35182]: Training loss 0.5230399966239929, current lr 9.815458628236834e-05, contrastive loss 2.971710681915283, task loss 0.49332287907600403
2025-01-06 11:10:21,014 - root - INFO - [Train] Epoch 3 [24600/35182]: Training loss 0.48270678520202637, current lr 9.815316506068617e-05, contrastive loss 3.038221836090088, task loss 0.4523245692253113
2025-01-06 11:12:01,423 - root - INFO - [Train] Epoch 3 [24700/35182]: Training loss 0.6285771131515503, current lr 9.815174383900401e-05, contrastive loss 3.029148817062378, task loss 0.5982856154441833
2025-01-06 11:13:42,106 - root - INFO - [Train] Epoch 3 [24800/35182]: Training loss 0.5589459538459778, current lr 9.815032261732186e-05, contrastive loss 3.203983783721924, task loss 0.5269061326980591
2025-01-06 11:15:22,524 - root - INFO - [Train] Epoch 3 [24900/35182]: Training loss 0.5099310278892517, current lr 9.81489013956397e-05, contrastive loss 3.1176505088806152, task loss 0.47875455021858215
2025-01-06 11:17:03,181 - root - INFO - [Train] Epoch 3 [25000/35182]: Training loss 0.5645537972450256, current lr 9.814748017395754e-05, contrastive loss 3.0538253784179688, task loss 0.5340155363082886
2025-01-06 11:18:43,863 - root - INFO - [Train] Epoch 3 [25100/35182]: Training loss 0.5457623600959778, current lr 9.814605895227537e-05, contrastive loss 3.043769359588623, task loss 0.5153246521949768
2025-01-06 11:20:24,270 - root - INFO - [Train] Epoch 3 [25200/35182]: Training loss 0.5389395952224731, current lr 9.814463773059323e-05, contrastive loss 3.0984435081481934, task loss 0.5079551339149475
2025-01-06 11:22:04,953 - root - INFO - [Train] Epoch 3 [25300/35182]: Training loss 0.4752761125564575, current lr 9.814321650891106e-05, contrastive loss 3.0373129844665527, task loss 0.4449029862880707
2025-01-06 11:23:45,358 - root - INFO - [Train] Epoch 3 [25400/35182]: Training loss 0.4980139136314392, current lr 9.814179528722892e-05, contrastive loss 3.197615146636963, task loss 0.4660377502441406
2025-01-06 11:25:26,023 - root - INFO - [Train] Epoch 3 [25500/35182]: Training loss 0.6014797687530518, current lr 9.814037406554675e-05, contrastive loss 3.1656885147094727, task loss 0.5698229074478149
2025-01-06 11:27:06,419 - root - INFO - [Train] Epoch 3 [25600/35182]: Training loss 0.5104706287384033, current lr 9.813895284386459e-05, contrastive loss 3.168179988861084, task loss 0.478788822889328
2025-01-06 11:28:47,096 - root - INFO - [Train] Epoch 3 [25700/35182]: Training loss 0.5646019577980042, current lr 9.813753162218243e-05, contrastive loss 3.040778636932373, task loss 0.5341941714286804
2025-01-06 11:30:27,483 - root - INFO - [Train] Epoch 3 [25800/35182]: Training loss 0.4894734025001526, current lr 9.813611040050028e-05, contrastive loss 3.1337952613830566, task loss 0.4581354558467865
2025-01-06 11:32:08,081 - root - INFO - [Train] Epoch 3 [25900/35182]: Training loss 0.4606451988220215, current lr 9.813468917881812e-05, contrastive loss 3.193302631378174, task loss 0.4287121593952179
2025-01-06 11:33:48,469 - root - INFO - [Train] Epoch 3 [26000/35182]: Training loss 0.5969727039337158, current lr 9.813326795713595e-05, contrastive loss 3.1031651496887207, task loss 0.5659410357475281
2025-01-06 11:35:29,149 - root - INFO - [Train] Epoch 3 [26100/35182]: Training loss 0.49821940064430237, current lr 9.81318467354538e-05, contrastive loss 3.23760986328125, task loss 0.4658432900905609
2025-01-06 11:37:09,825 - root - INFO - [Train] Epoch 3 [26200/35182]: Training loss 0.46590369939804077, current lr 9.813042551377164e-05, contrastive loss 3.1461994647979736, task loss 0.4344417154788971
2025-01-06 11:38:50,228 - root - INFO - [Train] Epoch 3 [26300/35182]: Training loss 0.5415378212928772, current lr 9.812900429208949e-05, contrastive loss 3.336371898651123, task loss 0.5081741213798523
2025-01-06 11:40:30,816 - root - INFO - [Train] Epoch 3 [26400/35182]: Training loss 0.5251407027244568, current lr 9.812758307040732e-05, contrastive loss 3.1321606636047363, task loss 0.49381911754608154
2025-01-06 11:42:11,215 - root - INFO - [Train] Epoch 3 [26500/35182]: Training loss 0.5118849277496338, current lr 9.812616184872518e-05, contrastive loss 3.104567050933838, task loss 0.4808392822742462
2025-01-06 11:43:51,893 - root - INFO - [Train] Epoch 3 [26600/35182]: Training loss 0.5491482615470886, current lr 9.812474062704301e-05, contrastive loss 3.1790246963500977, task loss 0.5173580050468445
2025-01-06 11:45:32,294 - root - INFO - [Train] Epoch 3 [26700/35182]: Training loss 0.5281345844268799, current lr 9.812331940536085e-05, contrastive loss 3.3340656757354736, task loss 0.49479392170906067
2025-01-06 11:47:12,965 - root - INFO - [Train] Epoch 3 [26800/35182]: Training loss 0.525189995765686, current lr 9.81218981836787e-05, contrastive loss 3.268787384033203, task loss 0.4925020933151245
2025-01-06 11:48:53,367 - root - INFO - [Train] Epoch 3 [26900/35182]: Training loss 0.6148453950881958, current lr 9.812047696199654e-05, contrastive loss 3.0547056198120117, task loss 0.584298312664032
2025-01-06 11:50:33,963 - root - INFO - [Train] Epoch 3 [27000/35182]: Training loss 0.5225902795791626, current lr 9.811905574031438e-05, contrastive loss 3.0026283264160156, task loss 0.49256402254104614
2025-01-06 11:52:14,355 - root - INFO - [Train] Epoch 3 [27100/35182]: Training loss 0.5267378091812134, current lr 9.811763451863221e-05, contrastive loss 3.1444859504699707, task loss 0.4952929615974426
2025-01-06 11:53:55,015 - root - INFO - [Train] Epoch 3 [27200/35182]: Training loss 0.592150092124939, current lr 9.811621329695007e-05, contrastive loss 3.1275758743286133, task loss 0.560874342918396
2025-01-06 11:55:35,424 - root - INFO - [Train] Epoch 3 [27300/35182]: Training loss 0.5546714663505554, current lr 9.81147920752679e-05, contrastive loss 3.265324831008911, task loss 0.5220181941986084
2025-01-06 11:57:16,110 - root - INFO - [Train] Epoch 3 [27400/35182]: Training loss 0.5375487804412842, current lr 9.811337085358576e-05, contrastive loss 3.172187328338623, task loss 0.5058268904685974
2025-01-06 11:58:56,512 - root - INFO - [Train] Epoch 3 [27500/35182]: Training loss 0.4451364576816559, current lr 9.811194963190359e-05, contrastive loss 2.9976673126220703, task loss 0.41515979170799255
2025-01-06 12:00:37,204 - root - INFO - [Train] Epoch 3 [27600/35182]: Training loss 0.5532538294792175, current lr 9.811052841022143e-05, contrastive loss 3.1852478981018066, task loss 0.5214013457298279
2025-01-06 12:02:17,879 - root - INFO - [Train] Epoch 3 [27700/35182]: Training loss 0.4985170066356659, current lr 9.810910718853927e-05, contrastive loss 3.1577205657958984, task loss 0.4669398069381714
2025-01-06 12:03:58,293 - root - INFO - [Train] Epoch 3 [27800/35182]: Training loss 0.5661555528640747, current lr 9.810768596685712e-05, contrastive loss 3.086191177368164, task loss 0.5352936387062073
2025-01-06 12:05:38,964 - root - INFO - [Train] Epoch 3 [27900/35182]: Training loss 0.45494237542152405, current lr 9.810626474517496e-05, contrastive loss 3.1273934841156006, task loss 0.42366844415664673
2025-01-06 12:07:19,346 - root - INFO - [Train] Epoch 3 [28000/35182]: Training loss 0.4600623846054077, current lr 9.810484352349279e-05, contrastive loss 2.8980207443237305, task loss 0.43108218908309937
2025-01-06 12:08:59,934 - root - INFO - [Train] Epoch 3 [28100/35182]: Training loss 0.5256449580192566, current lr 9.810342230181065e-05, contrastive loss 3.2497124671936035, task loss 0.4931478202342987
2025-01-06 12:10:40,341 - root - INFO - [Train] Epoch 3 [28200/35182]: Training loss 0.5400988459587097, current lr 9.810200108012848e-05, contrastive loss 3.2858633995056152, task loss 0.5072402358055115
2025-01-06 12:12:21,024 - root - INFO - [Train] Epoch 3 [28300/35182]: Training loss 0.4625799059867859, current lr 9.810057985844633e-05, contrastive loss 3.2805233001708984, task loss 0.429774671792984
2025-01-06 12:14:01,427 - root - INFO - [Train] Epoch 3 [28400/35182]: Training loss 0.5520537495613098, current lr 9.809915863676416e-05, contrastive loss 3.159698486328125, task loss 0.5204567909240723
2025-01-06 12:15:42,095 - root - INFO - [Train] Epoch 3 [28500/35182]: Training loss 0.44765108823776245, current lr 9.809773741508202e-05, contrastive loss 3.248286247253418, task loss 0.41516822576522827
2025-01-06 12:17:22,490 - root - INFO - [Train] Epoch 3 [28600/35182]: Training loss 0.6243219971656799, current lr 9.809631619339985e-05, contrastive loss 3.223968982696533, task loss 0.5920823216438293
2025-01-06 12:19:03,183 - root - INFO - [Train] Epoch 3 [28700/35182]: Training loss 0.5198571085929871, current lr 9.809489497171769e-05, contrastive loss 3.2363243103027344, task loss 0.4874938726425171
2025-01-06 12:20:43,862 - root - INFO - [Train] Epoch 3 [28800/35182]: Training loss 0.598923921585083, current lr 9.809347375003554e-05, contrastive loss 3.224830389022827, task loss 0.56667560338974
2025-01-06 12:22:24,258 - root - INFO - [Train] Epoch 3 [28900/35182]: Training loss 0.5729010701179504, current lr 9.809205252835337e-05, contrastive loss 3.1965386867523193, task loss 0.5409356951713562
2025-01-06 12:24:04,957 - root - INFO - [Train] Epoch 3 [29000/35182]: Training loss 0.5049683451652527, current lr 9.809063130667122e-05, contrastive loss 3.086954116821289, task loss 0.474098801612854
2025-01-06 12:25:45,352 - root - INFO - [Train] Epoch 3 [29100/35182]: Training loss 0.5451675653457642, current lr 9.808921008498905e-05, contrastive loss 3.1020493507385254, task loss 0.5141471028327942
2025-01-06 12:27:26,032 - root - INFO - [Train] Epoch 3 [29200/35182]: Training loss 0.5682209730148315, current lr 9.808778886330691e-05, contrastive loss 3.2331862449645996, task loss 0.5358890891075134
2025-01-06 12:29:06,444 - root - INFO - [Train] Epoch 3 [29300/35182]: Training loss 0.5051424503326416, current lr 9.808636764162474e-05, contrastive loss 3.1116840839385986, task loss 0.4740256369113922
2025-01-06 12:30:47,121 - root - INFO - [Train] Epoch 3 [29400/35182]: Training loss 0.5885756611824036, current lr 9.80849464199426e-05, contrastive loss 3.260753870010376, task loss 0.5559681057929993
2025-01-06 12:32:27,797 - root - INFO - [Train] Epoch 3 [29500/35182]: Training loss 0.4812602698802948, current lr 9.808352519826043e-05, contrastive loss 2.9768948554992676, task loss 0.4514913260936737
2025-01-06 12:34:08,210 - root - INFO - [Train] Epoch 3 [29600/35182]: Training loss 0.6024513244628906, current lr 9.808210397657827e-05, contrastive loss 3.016713857650757, task loss 0.5722841620445251
2025-01-06 12:35:48,880 - root - INFO - [Train] Epoch 3 [29700/35182]: Training loss 0.5582687258720398, current lr 9.808068275489611e-05, contrastive loss 2.986250400543213, task loss 0.5284062027931213
2025-01-06 12:37:29,293 - root - INFO - [Train] Epoch 3 [29800/35182]: Training loss 0.5478111505508423, current lr 9.807926153321396e-05, contrastive loss 3.0073373317718506, task loss 0.5177378058433533
2025-01-06 12:39:09,968 - root - INFO - [Train] Epoch 3 [29900/35182]: Training loss 0.6024215817451477, current lr 9.80778403115318e-05, contrastive loss 3.119826316833496, task loss 0.5712233185768127
2025-01-06 12:40:50,380 - root - INFO - [Train] Epoch 3 [30000/35182]: Training loss 0.46458959579467773, current lr 9.807641908984963e-05, contrastive loss 3.036926746368408, task loss 0.4342203140258789
2025-01-06 12:42:31,060 - root - INFO - [Train] Epoch 3 [30100/35182]: Training loss 0.5300199389457703, current lr 9.807499786816749e-05, contrastive loss 3.073537826538086, task loss 0.499284565448761
2025-01-06 12:44:11,467 - root - INFO - [Train] Epoch 3 [30200/35182]: Training loss 0.5325402617454529, current lr 9.807357664648532e-05, contrastive loss 3.1022777557373047, task loss 0.5015174746513367
2025-01-06 12:45:52,139 - root - INFO - [Train] Epoch 3 [30300/35182]: Training loss 0.49716272950172424, current lr 9.807215542480317e-05, contrastive loss 3.1464409828186035, task loss 0.46569833159446716
2025-01-06 12:47:32,820 - root - INFO - [Train] Epoch 3 [30400/35182]: Training loss 0.543316662311554, current lr 9.8070734203121e-05, contrastive loss 2.9880142211914062, task loss 0.513436496257782
2025-01-06 12:49:13,224 - root - INFO - [Train] Epoch 3 [30500/35182]: Training loss 0.5528386235237122, current lr 9.806931298143886e-05, contrastive loss 3.13631534576416, task loss 0.5214754939079285
2025-01-06 12:50:53,906 - root - INFO - [Train] Epoch 3 [30600/35182]: Training loss 0.5602662563323975, current lr 9.806789175975669e-05, contrastive loss 3.148224353790283, task loss 0.5287840366363525
2025-01-06 12:52:34,327 - root - INFO - [Train] Epoch 3 [30700/35182]: Training loss 0.5557816028594971, current lr 9.806647053807453e-05, contrastive loss 2.9235620498657227, task loss 0.5265460014343262
2025-01-06 12:54:14,998 - root - INFO - [Train] Epoch 3 [30800/35182]: Training loss 0.5147490501403809, current lr 9.806504931639238e-05, contrastive loss 3.080684185028076, task loss 0.4839421808719635
2025-01-06 12:55:55,410 - root - INFO - [Train] Epoch 3 [30900/35182]: Training loss 0.49252283573150635, current lr 9.806362809471021e-05, contrastive loss 3.284018039703369, task loss 0.4596826434135437
2025-01-06 12:57:36,098 - root - INFO - [Train] Epoch 3 [31000/35182]: Training loss 0.5795155763626099, current lr 9.806220687302806e-05, contrastive loss 3.11570405960083, task loss 0.5483585596084595
2025-01-06 12:59:16,503 - root - INFO - [Train] Epoch 3 [31100/35182]: Training loss 0.5377879738807678, current lr 9.80607856513459e-05, contrastive loss 3.1490612030029297, task loss 0.5062973499298096
2025-01-06 13:00:57,153 - root - INFO - [Train] Epoch 3 [31200/35182]: Training loss 0.4949449896812439, current lr 9.805936442966375e-05, contrastive loss 3.0935566425323486, task loss 0.46400943398475647
2025-01-06 13:02:37,821 - root - INFO - [Train] Epoch 3 [31300/35182]: Training loss 0.5724867582321167, current lr 9.805794320798158e-05, contrastive loss 3.0235540866851807, task loss 0.5422512292861938
2025-01-06 13:04:18,229 - root - INFO - [Train] Epoch 3 [31400/35182]: Training loss 0.5389454960823059, current lr 9.805652198629944e-05, contrastive loss 3.131680965423584, task loss 0.5076286792755127
2025-01-06 13:05:58,922 - root - INFO - [Train] Epoch 3 [31500/35182]: Training loss 0.5895112752914429, current lr 9.805510076461727e-05, contrastive loss 3.301252841949463, task loss 0.5564987659454346
2025-01-06 13:07:39,326 - root - INFO - [Train] Epoch 3 [31600/35182]: Training loss 0.5446969270706177, current lr 9.805367954293511e-05, contrastive loss 2.9868998527526855, task loss 0.5148279070854187
2025-01-06 13:09:20,012 - root - INFO - [Train] Epoch 3 [31700/35182]: Training loss 0.5519639253616333, current lr 9.805225832125295e-05, contrastive loss 3.049156665802002, task loss 0.5214723348617554
2025-01-06 13:11:00,420 - root - INFO - [Train] Epoch 3 [31800/35182]: Training loss 0.5096071362495422, current lr 9.80508370995708e-05, contrastive loss 3.048590898513794, task loss 0.47912123799324036
2025-01-06 13:12:41,088 - root - INFO - [Train] Epoch 3 [31900/35182]: Training loss 0.5324677228927612, current lr 9.804941587788864e-05, contrastive loss 3.1415047645568848, task loss 0.5010526776313782
2025-01-06 13:14:21,485 - root - INFO - [Train] Epoch 3 [32000/35182]: Training loss 0.5181470513343811, current lr 9.804799465620647e-05, contrastive loss 3.0002989768981934, task loss 0.48814406991004944
2025-01-06 13:16:02,171 - root - INFO - [Train] Epoch 3 [32100/35182]: Training loss 0.5095537900924683, current lr 9.804657343452433e-05, contrastive loss 3.0049262046813965, task loss 0.4795045554637909
2025-01-06 13:17:42,860 - root - INFO - [Train] Epoch 3 [32200/35182]: Training loss 0.539710283279419, current lr 9.804515221284216e-05, contrastive loss 3.165518283843994, task loss 0.5080550909042358
2025-01-06 13:19:23,255 - root - INFO - [Train] Epoch 3 [32300/35182]: Training loss 0.5029473304748535, current lr 9.804373099116001e-05, contrastive loss 2.9716506004333496, task loss 0.4732308089733124
2025-01-06 13:21:03,947 - root - INFO - [Train] Epoch 3 [32400/35182]: Training loss 0.582443118095398, current lr 9.804230976947784e-05, contrastive loss 3.2095823287963867, task loss 0.5503472685813904
2025-01-06 13:22:44,371 - root - INFO - [Train] Epoch 3 [32500/35182]: Training loss 0.5248107314109802, current lr 9.80408885477957e-05, contrastive loss 3.1400225162506104, task loss 0.49341052770614624
2025-01-06 13:24:25,042 - root - INFO - [Train] Epoch 3 [32600/35182]: Training loss 0.5644863247871399, current lr 9.803946732611353e-05, contrastive loss 3.0445075035095215, task loss 0.5340412259101868
2025-01-06 13:26:05,440 - root - INFO - [Train] Epoch 3 [32700/35182]: Training loss 0.49573132395744324, current lr 9.803804610443137e-05, contrastive loss 2.9486184120178223, task loss 0.4662451446056366
2025-01-06 13:27:46,114 - root - INFO - [Train] Epoch 3 [32800/35182]: Training loss 0.586587131023407, current lr 9.803662488274922e-05, contrastive loss 3.1080470085144043, task loss 0.5555066466331482
2025-01-06 13:29:26,500 - root - INFO - [Train] Epoch 3 [32900/35182]: Training loss 0.4998934864997864, current lr 9.803520366106705e-05, contrastive loss 3.118189573287964, task loss 0.46871158480644226
2025-01-06 13:31:07,194 - root - INFO - [Train] Epoch 3 [33000/35182]: Training loss 0.5170252919197083, current lr 9.80337824393849e-05, contrastive loss 2.9740099906921387, task loss 0.48728516697883606
2025-01-06 13:32:47,876 - root - INFO - [Train] Epoch 3 [33100/35182]: Training loss 0.4968048632144928, current lr 9.803236121770273e-05, contrastive loss 3.097322940826416, task loss 0.4658316373825073
2025-01-06 13:34:28,286 - root - INFO - [Train] Epoch 3 [33200/35182]: Training loss 0.4888573884963989, current lr 9.803093999602059e-05, contrastive loss 3.101341485977173, task loss 0.45784395933151245
2025-01-06 13:36:08,959 - root - INFO - [Train] Epoch 3 [33300/35182]: Training loss 0.5522658824920654, current lr 9.802951877433842e-05, contrastive loss 3.1537084579467773, task loss 0.5207288265228271
2025-01-06 13:37:49,353 - root - INFO - [Train] Epoch 3 [33400/35182]: Training loss 0.46973636746406555, current lr 9.802809755265628e-05, contrastive loss 3.066455364227295, task loss 0.43907180428504944
2025-01-06 13:39:30,053 - root - INFO - [Train] Epoch 3 [33500/35182]: Training loss 0.49700766801834106, current lr 9.802667633097411e-05, contrastive loss 3.236072063446045, task loss 0.46464693546295166
2025-01-06 13:41:10,454 - root - INFO - [Train] Epoch 3 [33600/35182]: Training loss 0.5203344225883484, current lr 9.802525510929195e-05, contrastive loss 3.279477596282959, task loss 0.4875396490097046
2025-01-06 13:42:51,129 - root - INFO - [Train] Epoch 3 [33700/35182]: Training loss 0.592068076133728, current lr 9.80238338876098e-05, contrastive loss 2.968323230743408, task loss 0.562384843826294
2025-01-06 13:44:31,818 - root - INFO - [Train] Epoch 3 [33800/35182]: Training loss 0.5390340089797974, current lr 9.802241266592764e-05, contrastive loss 3.186316728591919, task loss 0.5071708559989929
2025-01-06 13:46:12,212 - root - INFO - [Train] Epoch 3 [33900/35182]: Training loss 0.4487368166446686, current lr 9.802099144424548e-05, contrastive loss 2.9710564613342285, task loss 0.419026255607605
2025-01-06 13:47:52,898 - root - INFO - [Train] Epoch 3 [34000/35182]: Training loss 0.4902161657810211, current lr 9.801957022256331e-05, contrastive loss 3.0599966049194336, task loss 0.45961621403694153
2025-01-06 13:49:33,287 - root - INFO - [Train] Epoch 3 [34100/35182]: Training loss 0.515683114528656, current lr 9.801814900088117e-05, contrastive loss 3.106013774871826, task loss 0.484622985124588
2025-01-06 13:51:13,867 - root - INFO - [Train] Epoch 3 [34200/35182]: Training loss 0.517650306224823, current lr 9.8016727779199e-05, contrastive loss 3.1453640460968018, task loss 0.4861966669559479
2025-01-06 13:52:54,269 - root - INFO - [Train] Epoch 3 [34300/35182]: Training loss 0.5398151874542236, current lr 9.801530655751686e-05, contrastive loss 3.3014392852783203, task loss 0.5068007707595825
2025-01-06 13:54:34,942 - root - INFO - [Train] Epoch 3 [34400/35182]: Training loss 0.5723177194595337, current lr 9.801388533583468e-05, contrastive loss 3.0809054374694824, task loss 0.541508674621582
2025-01-06 13:56:15,333 - root - INFO - [Train] Epoch 3 [34500/35182]: Training loss 0.6217598915100098, current lr 9.801246411415254e-05, contrastive loss 3.0505080223083496, task loss 0.5912548303604126
2025-01-06 13:57:56,020 - root - INFO - [Train] Epoch 3 [34600/35182]: Training loss 0.5318911075592041, current lr 9.801104289247037e-05, contrastive loss 3.085099220275879, task loss 0.5010401010513306
2025-01-06 13:59:36,414 - root - INFO - [Train] Epoch 3 [34700/35182]: Training loss 0.48667317628860474, current lr 9.800962167078822e-05, contrastive loss 2.898865222930908, task loss 0.4576845169067383
2025-01-06 14:01:17,004 - root - INFO - [Train] Epoch 3 [34800/35182]: Training loss 0.5528531670570374, current lr 9.800820044910606e-05, contrastive loss 3.225594997406006, task loss 0.5205972194671631
2025-01-06 14:02:57,398 - root - INFO - [Train] Epoch 3 [34900/35182]: Training loss 0.5582540035247803, current lr 9.800677922742389e-05, contrastive loss 2.980595588684082, task loss 0.5284480452537537
2025-01-06 14:04:38,087 - root - INFO - [Train] Epoch 3 [35000/35182]: Training loss 0.5449443459510803, current lr 9.800535800574175e-05, contrastive loss 2.9985265731811523, task loss 0.5149590969085693
2025-01-06 14:06:18,483 - root - INFO - [Train] Epoch 3 [35100/35182]: Training loss 0.5649871826171875, current lr 9.800393678405958e-05, contrastive loss 3.0294241905212402, task loss 0.5346929430961609
2025-01-06 16:52:02,516 - root - INFO - [Valid] Evaluation Results: HR@5:0.0419,NDCG@5:0.0244,HR@10:0.0880,NDCG@10:0.0392,HR@20:0.1885,NDCG@20:0.0642
2025-01-06 16:52:05,556 - root - INFO - [Train] Epoch 3: Overall Training loss 0.0
2025-01-06 16:52:05,557 - root - INFO - [Time] Epoch 3: Time taken 15.28 hours
2025-01-06 16:52:10,799 - root - INFO - [Train] Epoch 4 [0/35182]: Training loss 0.5330952405929565, current lr 9.800277138228021e-05, contrastive loss 3.1493430137634277, task loss 0.5016018152236938
2025-01-06 16:53:51,209 - root - INFO - [Train] Epoch 4 [100/35182]: Training loss 0.5466273427009583, current lr 9.800135016059807e-05, contrastive loss 3.218038558959961, task loss 0.5144469738006592
2025-01-06 16:55:31,819 - root - INFO - [Train] Epoch 4 [200/35182]: Training loss 0.543752908706665, current lr 9.79999289389159e-05, contrastive loss 3.066258668899536, task loss 0.5130903124809265
2025-01-06 16:57:12,215 - root - INFO - [Train] Epoch 4 [300/35182]: Training loss 0.5383211970329285, current lr 9.799850771723374e-05, contrastive loss 3.068485736846924, task loss 0.5076363682746887
2025-01-06 16:58:52,895 - root - INFO - [Train] Epoch 4 [400/35182]: Training loss 0.5271400213241577, current lr 9.799708649555158e-05, contrastive loss 3.2158172130584717, task loss 0.4949818551540375
2025-01-06 17:00:33,294 - root - INFO - [Train] Epoch 4 [500/35182]: Training loss 0.4959331452846527, current lr 9.799566527386941e-05, contrastive loss 3.1751742362976074, task loss 0.46418139338493347
2025-01-06 17:02:13,966 - root - INFO - [Train] Epoch 4 [600/35182]: Training loss 0.5743083953857422, current lr 9.799424405218727e-05, contrastive loss 3.2466464042663574, task loss 0.5418419241905212
2025-01-06 17:03:54,379 - root - INFO - [Train] Epoch 4 [700/35182]: Training loss 0.5174305438995361, current lr 9.79928228305051e-05, contrastive loss 3.0193710327148438, task loss 0.4872368574142456
2025-01-06 17:05:35,077 - root - INFO - [Train] Epoch 4 [800/35182]: Training loss 0.5941985845565796, current lr 9.799140160882296e-05, contrastive loss 3.095522165298462, task loss 0.5632433891296387
2025-01-06 17:07:15,476 - root - INFO - [Train] Epoch 4 [900/35182]: Training loss 0.5456581115722656, current lr 9.798998038714079e-05, contrastive loss 3.3045477867126465, task loss 0.5126126408576965
2025-01-06 17:08:56,147 - root - INFO - [Train] Epoch 4 [1000/35182]: Training loss 0.5774474143981934, current lr 9.798855916545864e-05, contrastive loss 3.3491382598876953, task loss 0.5439560413360596
2025-01-06 17:10:36,835 - root - INFO - [Train] Epoch 4 [1100/35182]: Training loss 0.4708811938762665, current lr 9.798713794377647e-05, contrastive loss 3.185683250427246, task loss 0.4390243589878082
2025-01-06 17:12:17,268 - root - INFO - [Train] Epoch 4 [1200/35182]: Training loss 0.5239360332489014, current lr 9.798571672209432e-05, contrastive loss 2.7962942123413086, task loss 0.4959730803966522
2025-01-06 17:13:57,929 - root - INFO - [Train] Epoch 4 [1300/35182]: Training loss 0.5234854221343994, current lr 9.798429550041216e-05, contrastive loss 3.0361576080322266, task loss 0.4931238293647766
2025-01-06 17:15:38,329 - root - INFO - [Train] Epoch 4 [1400/35182]: Training loss 0.5472649931907654, current lr 9.798287427873e-05, contrastive loss 3.08372163772583, task loss 0.516427755355835
2025-01-06 17:17:19,001 - root - INFO - [Train] Epoch 4 [1500/35182]: Training loss 0.5611516237258911, current lr 9.798145305704785e-05, contrastive loss 3.1614022254943848, task loss 0.5295376181602478
2025-01-06 17:18:59,405 - root - INFO - [Train] Epoch 4 [1600/35182]: Training loss 0.540733277797699, current lr 9.798003183536568e-05, contrastive loss 3.2945728302001953, task loss 0.5077875256538391
2025-01-06 17:20:40,096 - root - INFO - [Train] Epoch 4 [1700/35182]: Training loss 0.46002915501594543, current lr 9.797861061368353e-05, contrastive loss 3.068443775177002, task loss 0.42934471368789673
2025-01-06 17:22:20,500 - root - INFO - [Train] Epoch 4 [1800/35182]: Training loss 0.5332066416740417, current lr 9.797718939200136e-05, contrastive loss 3.152989149093628, task loss 0.5016767382621765
2025-01-06 17:24:01,173 - root - INFO - [Train] Epoch 4 [1900/35182]: Training loss 0.5047898888587952, current lr 9.797576817031922e-05, contrastive loss 3.1774656772613525, task loss 0.4730152487754822
2025-01-06 17:25:41,854 - root - INFO - [Train] Epoch 4 [2000/35182]: Training loss 0.5940805673599243, current lr 9.797434694863705e-05, contrastive loss 3.049915075302124, task loss 0.5635814070701599
2025-01-06 17:27:22,262 - root - INFO - [Train] Epoch 4 [2100/35182]: Training loss 0.4580773711204529, current lr 9.797292572695491e-05, contrastive loss 3.120469570159912, task loss 0.4268726706504822
2025-01-06 17:29:02,945 - root - INFO - [Train] Epoch 4 [2200/35182]: Training loss 0.5438278913497925, current lr 9.797150450527274e-05, contrastive loss 2.978060722351074, task loss 0.5140472650527954
2025-01-06 17:30:43,337 - root - INFO - [Train] Epoch 4 [2300/35182]: Training loss 0.5122646689414978, current lr 9.797008328359058e-05, contrastive loss 3.145562171936035, task loss 0.4808090627193451
2025-01-06 17:32:24,015 - root - INFO - [Train] Epoch 4 [2400/35182]: Training loss 0.45479199290275574, current lr 9.796866206190842e-05, contrastive loss 2.9264116287231445, task loss 0.4255278706550598
2025-01-06 17:34:04,422 - root - INFO - [Train] Epoch 4 [2500/35182]: Training loss 0.6213840842247009, current lr 9.796724084022625e-05, contrastive loss 2.946221351623535, task loss 0.591921865940094
2025-01-06 17:35:45,104 - root - INFO - [Train] Epoch 4 [2600/35182]: Training loss 0.5062283873558044, current lr 9.796581961854411e-05, contrastive loss 3.2138962745666504, task loss 0.47408944368362427
2025-01-06 17:37:25,513 - root - INFO - [Train] Epoch 4 [2700/35182]: Training loss 0.6289653778076172, current lr 9.796439839686194e-05, contrastive loss 3.030893325805664, task loss 0.5986564755439758
2025-01-06 17:39:06,207 - root - INFO - [Train] Epoch 4 [2800/35182]: Training loss 0.4881340265274048, current lr 9.79629771751798e-05, contrastive loss 3.0048818588256836, task loss 0.45808520913124084
2025-01-06 17:40:46,873 - root - INFO - [Train] Epoch 4 [2900/35182]: Training loss 0.5307459831237793, current lr 9.796155595349763e-05, contrastive loss 3.035534381866455, task loss 0.5003906488418579
2025-01-06 17:42:27,272 - root - INFO - [Train] Epoch 4 [3000/35182]: Training loss 0.5678571462631226, current lr 9.796013473181548e-05, contrastive loss 3.184237480163574, task loss 0.5360147953033447
2025-01-06 17:44:07,959 - root - INFO - [Train] Epoch 4 [3100/35182]: Training loss 0.5392670631408691, current lr 9.795871351013331e-05, contrastive loss 3.1412675380706787, task loss 0.5078544020652771
2025-01-06 17:45:48,356 - root - INFO - [Train] Epoch 4 [3200/35182]: Training loss 0.5173990726470947, current lr 9.795729228845116e-05, contrastive loss 2.9076619148254395, task loss 0.4883224368095398
2025-01-06 17:47:29,034 - root - INFO - [Train] Epoch 4 [3300/35182]: Training loss 0.5302810072898865, current lr 9.7955871066769e-05, contrastive loss 2.9432663917541504, task loss 0.5008483529090881
2025-01-06 17:49:09,429 - root - INFO - [Train] Epoch 4 [3400/35182]: Training loss 0.539003312587738, current lr 9.795444984508684e-05, contrastive loss 3.1615357398986816, task loss 0.5073879361152649
2025-01-06 17:50:50,007 - root - INFO - [Train] Epoch 4 [3500/35182]: Training loss 0.48150715231895447, current lr 9.795302862340469e-05, contrastive loss 3.014166831970215, task loss 0.45136547088623047
2025-01-06 17:52:30,400 - root - INFO - [Train] Epoch 4 [3600/35182]: Training loss 0.47848862409591675, current lr 9.795160740172252e-05, contrastive loss 3.2079899311065674, task loss 0.4464087188243866
2025-01-06 17:54:11,071 - root - INFO - [Train] Epoch 4 [3700/35182]: Training loss 0.46516162157058716, current lr 9.795018618004037e-05, contrastive loss 3.353114604949951, task loss 0.4316304922103882
2025-01-06 17:55:51,462 - root - INFO - [Train] Epoch 4 [3800/35182]: Training loss 0.5087172985076904, current lr 9.79487649583582e-05, contrastive loss 2.922234535217285, task loss 0.47949492931365967
2025-01-06 17:57:32,061 - root - INFO - [Train] Epoch 4 [3900/35182]: Training loss 0.5792856812477112, current lr 9.794734373667606e-05, contrastive loss 2.9974160194396973, task loss 0.5493115186691284
2025-01-06 17:59:12,461 - root - INFO - [Train] Epoch 4 [4000/35182]: Training loss 0.6003987193107605, current lr 9.794592251499389e-05, contrastive loss 3.004528045654297, task loss 0.5703534483909607
2025-01-06 18:00:53,159 - root - INFO - [Train] Epoch 4 [4100/35182]: Training loss 0.5752097368240356, current lr 9.794450129331173e-05, contrastive loss 2.9656693935394287, task loss 0.5455530285835266
2025-01-06 18:02:33,823 - root - INFO - [Train] Epoch 4 [4200/35182]: Training loss 0.5934646725654602, current lr 9.794308007162958e-05, contrastive loss 3.085150718688965, task loss 0.5626131892204285
2025-01-06 18:04:14,221 - root - INFO - [Train] Epoch 4 [4300/35182]: Training loss 0.5468851923942566, current lr 9.794165884994742e-05, contrastive loss 3.097459316253662, task loss 0.5159106254577637
2025-01-06 18:05:54,913 - root - INFO - [Train] Epoch 4 [4400/35182]: Training loss 0.5378779768943787, current lr 9.794023762826526e-05, contrastive loss 2.958631992340088, task loss 0.5082916617393494
2025-01-06 18:07:35,325 - root - INFO - [Train] Epoch 4 [4500/35182]: Training loss 0.456351637840271, current lr 9.79388164065831e-05, contrastive loss 3.1796584129333496, task loss 0.42455506324768066
2025-01-06 18:09:15,989 - root - INFO - [Train] Epoch 4 [4600/35182]: Training loss 0.499582439661026, current lr 9.793739518490095e-05, contrastive loss 3.266230344772339, task loss 0.4669201374053955
2025-01-06 18:10:56,390 - root - INFO - [Train] Epoch 4 [4700/35182]: Training loss 0.5678715109825134, current lr 9.793597396321878e-05, contrastive loss 3.2196736335754395, task loss 0.5356747508049011
2025-01-06 18:12:37,080 - root - INFO - [Train] Epoch 4 [4800/35182]: Training loss 0.5324754118919373, current lr 9.793455274153664e-05, contrastive loss 3.1186983585357666, task loss 0.5012884140014648
2025-01-06 18:14:17,490 - root - INFO - [Train] Epoch 4 [4900/35182]: Training loss 0.48891785740852356, current lr 9.793313151985447e-05, contrastive loss 3.159559965133667, task loss 0.45732226967811584
2025-01-06 18:15:58,156 - root - INFO - [Train] Epoch 4 [5000/35182]: Training loss 0.45578816533088684, current lr 9.793171029817232e-05, contrastive loss 3.0283780097961426, task loss 0.4255043864250183
2025-01-06 18:17:38,845 - root - INFO - [Train] Epoch 4 [5100/35182]: Training loss 0.5721747279167175, current lr 9.793028907649015e-05, contrastive loss 3.0693869590759277, task loss 0.5414808392524719
2025-01-06 18:19:19,269 - root - INFO - [Train] Epoch 4 [5200/35182]: Training loss 0.41268390417099, current lr 9.7928867854808e-05, contrastive loss 3.1564197540283203, task loss 0.3811196982860565
2025-01-06 18:20:59,928 - root - INFO - [Train] Epoch 4 [5300/35182]: Training loss 0.5176360607147217, current lr 9.792744663312584e-05, contrastive loss 3.1336417198181152, task loss 0.48629966378211975
2025-01-06 18:22:40,322 - root - INFO - [Train] Epoch 4 [5400/35182]: Training loss 0.512523353099823, current lr 9.792602541144368e-05, contrastive loss 3.1458353996276855, task loss 0.48106497526168823
2025-01-06 18:24:21,014 - root - INFO - [Train] Epoch 4 [5500/35182]: Training loss 0.5410177707672119, current lr 9.792460418976153e-05, contrastive loss 3.222003936767578, task loss 0.5087977051734924
2025-01-06 18:26:01,413 - root - INFO - [Train] Epoch 4 [5600/35182]: Training loss 0.5712228417396545, current lr 9.792318296807936e-05, contrastive loss 3.051358699798584, task loss 0.5407092571258545
2025-01-06 18:27:42,095 - root - INFO - [Train] Epoch 4 [5700/35182]: Training loss 0.5747131705284119, current lr 9.792176174639722e-05, contrastive loss 3.1058945655822754, task loss 0.543654203414917
2025-01-06 18:29:22,486 - root - INFO - [Train] Epoch 4 [5800/35182]: Training loss 0.4760364592075348, current lr 9.792034052471504e-05, contrastive loss 3.059067964553833, task loss 0.4454457759857178
2025-01-06 18:31:03,161 - root - INFO - [Train] Epoch 4 [5900/35182]: Training loss 0.5592598915100098, current lr 9.79189193030329e-05, contrastive loss 3.1394596099853516, task loss 0.5278652906417847
2025-01-06 18:32:43,551 - root - INFO - [Train] Epoch 4 [6000/35182]: Training loss 0.6218433976173401, current lr 9.791749808135073e-05, contrastive loss 3.1402719020843506, task loss 0.5904406905174255
2025-01-06 18:34:24,142 - root - INFO - [Train] Epoch 4 [6100/35182]: Training loss 0.4912532567977905, current lr 9.791607685966858e-05, contrastive loss 3.2148170471191406, task loss 0.45910507440567017
2025-01-06 18:36:04,811 - root - INFO - [Train] Epoch 4 [6200/35182]: Training loss 0.5027713775634766, current lr 9.791465563798642e-05, contrastive loss 3.065189838409424, task loss 0.4721195101737976
2025-01-06 18:37:45,202 - root - INFO - [Train] Epoch 4 [6300/35182]: Training loss 0.5628555417060852, current lr 9.791323441630426e-05, contrastive loss 3.1099846363067627, task loss 0.5317556858062744
2025-01-06 18:39:25,588 - root - INFO - [Train] Epoch 4 [6400/35182]: Training loss 0.5246064066886902, current lr 9.79118131946221e-05, contrastive loss 2.948435068130493, task loss 0.4951220750808716
2025-01-06 18:41:06,198 - root - INFO - [Train] Epoch 4 [6500/35182]: Training loss 0.5013262629508972, current lr 9.791039197293994e-05, contrastive loss 2.984215259552002, task loss 0.47148409485816956
2025-01-06 18:42:46,878 - root - INFO - [Train] Epoch 4 [6600/35182]: Training loss 0.550024151802063, current lr 9.790897075125779e-05, contrastive loss 2.8653342723846436, task loss 0.5213708281517029
2025-01-06 18:44:27,279 - root - INFO - [Train] Epoch 4 [6700/35182]: Training loss 0.5726402997970581, current lr 9.790754952957562e-05, contrastive loss 3.0893006324768066, task loss 0.5417472720146179
2025-01-06 18:46:07,968 - root - INFO - [Train] Epoch 4 [6800/35182]: Training loss 0.5269296169281006, current lr 9.790612830789348e-05, contrastive loss 3.060256004333496, task loss 0.4963270425796509
2025-01-06 18:47:48,375 - root - INFO - [Train] Epoch 4 [6900/35182]: Training loss 0.4986845552921295, current lr 9.790470708621131e-05, contrastive loss 3.1934151649475098, task loss 0.4667504131793976
2025-01-06 18:49:29,044 - root - INFO - [Train] Epoch 4 [7000/35182]: Training loss 0.5473243594169617, current lr 9.790328586452915e-05, contrastive loss 3.012667179107666, task loss 0.5171976685523987
2025-01-06 18:51:09,435 - root - INFO - [Train] Epoch 4 [7100/35182]: Training loss 0.4459768533706665, current lr 9.7901864642847e-05, contrastive loss 2.926999568939209, task loss 0.4167068600654602
2025-01-06 18:52:50,114 - root - INFO - [Train] Epoch 4 [7200/35182]: Training loss 0.5453069806098938, current lr 9.790044342116484e-05, contrastive loss 3.176027774810791, task loss 0.5135467052459717
2025-01-06 18:54:30,499 - root - INFO - [Train] Epoch 4 [7300/35182]: Training loss 0.46512341499328613, current lr 9.789902219948268e-05, contrastive loss 3.305267333984375, task loss 0.4320707321166992
2025-01-06 18:56:11,095 - root - INFO - [Train] Epoch 4 [7400/35182]: Training loss 0.5161052346229553, current lr 9.789760097780051e-05, contrastive loss 3.4384765625, task loss 0.4817204773426056
2025-01-06 18:57:51,481 - root - INFO - [Train] Epoch 4 [7500/35182]: Training loss 0.48514804244041443, current lr 9.789617975611837e-05, contrastive loss 3.2067906856536865, task loss 0.4530801475048065
2025-01-06 18:59:32,170 - root - INFO - [Train] Epoch 4 [7600/35182]: Training loss 0.5694472193717957, current lr 9.78947585344362e-05, contrastive loss 3.1966114044189453, task loss 0.5374811291694641
2025-01-06 19:01:12,573 - root - INFO - [Train] Epoch 4 [7700/35182]: Training loss 0.4975309669971466, current lr 9.789333731275406e-05, contrastive loss 3.443774700164795, task loss 0.46309322118759155
2025-01-06 19:02:53,133 - root - INFO - [Train] Epoch 4 [7800/35182]: Training loss 0.4213991165161133, current lr 9.789191609107189e-05, contrastive loss 3.365041494369507, task loss 0.38774868845939636
2025-01-06 19:04:33,522 - root - INFO - [Train] Epoch 4 [7900/35182]: Training loss 0.467454195022583, current lr 9.789049486938974e-05, contrastive loss 3.4750616550445557, task loss 0.43270358443260193
2025-01-06 19:06:14,095 - root - INFO - [Train] Epoch 4 [8000/35182]: Training loss 0.4628564119338989, current lr 9.788907364770757e-05, contrastive loss 3.521512031555176, task loss 0.4276413023471832
2025-01-06 19:07:54,508 - root - INFO - [Train] Epoch 4 [8100/35182]: Training loss 0.4959402084350586, current lr 9.788765242602542e-05, contrastive loss 3.8793561458587646, task loss 0.45714664459228516
2025-01-06 19:09:35,203 - root - INFO - [Train] Epoch 4 [8200/35182]: Training loss 0.41697192192077637, current lr 9.788623120434326e-05, contrastive loss 3.4717612266540527, task loss 0.38225430250167847
2025-01-06 19:11:15,867 - root - INFO - [Train] Epoch 4 [8300/35182]: Training loss 0.44136935472488403, current lr 9.78848099826611e-05, contrastive loss 3.5857763290405273, task loss 0.4055115878582001
2025-01-06 19:12:56,266 - root - INFO - [Train] Epoch 4 [8400/35182]: Training loss 0.42517441511154175, current lr 9.788338876097895e-05, contrastive loss 3.512664318084717, task loss 0.3900477886199951
2025-01-06 19:14:36,939 - root - INFO - [Train] Epoch 4 [8500/35182]: Training loss 0.42928412556648254, current lr 9.788196753929678e-05, contrastive loss 3.4495365619659424, task loss 0.3947887718677521
2025-01-06 19:16:17,350 - root - INFO - [Train] Epoch 4 [8600/35182]: Training loss 0.46232810616493225, current lr 9.788054631761463e-05, contrastive loss 3.3630332946777344, task loss 0.42869776487350464
2025-01-06 19:17:58,030 - root - INFO - [Train] Epoch 4 [8700/35182]: Training loss 0.4361482858657837, current lr 9.787912509593246e-05, contrastive loss 3.356503963470459, task loss 0.4025832414627075
2025-01-06 19:19:38,426 - root - INFO - [Train] Epoch 4 [8800/35182]: Training loss 0.40275001525878906, current lr 9.787770387425032e-05, contrastive loss 3.5051722526550293, task loss 0.3676982820034027
2025-01-06 19:21:19,006 - root - INFO - [Train] Epoch 4 [8900/35182]: Training loss 0.3843710124492645, current lr 9.787628265256815e-05, contrastive loss 3.3579795360565186, task loss 0.35079121589660645
2025-01-06 19:22:59,398 - root - INFO - [Train] Epoch 4 [9000/35182]: Training loss 0.47269198298454285, current lr 9.787486143088599e-05, contrastive loss 3.4313547611236572, task loss 0.4383784234523773
2025-01-06 19:24:40,085 - root - INFO - [Train] Epoch 4 [9100/35182]: Training loss 0.41781389713287354, current lr 9.787344020920384e-05, contrastive loss 3.3435850143432617, task loss 0.384378045797348
2025-01-06 19:26:20,475 - root - INFO - [Train] Epoch 4 [9200/35182]: Training loss 0.43501144647598267, current lr 9.787201898752168e-05, contrastive loss 3.445075511932373, task loss 0.4005606770515442
2025-01-06 19:28:01,064 - root - INFO - [Train] Epoch 4 [9300/35182]: Training loss 0.46333998441696167, current lr 9.787059776583952e-05, contrastive loss 3.2528958320617676, task loss 0.4308110177516937
2025-01-06 19:29:41,464 - root - INFO - [Train] Epoch 4 [9400/35182]: Training loss 0.3928241431713104, current lr 9.786917654415735e-05, contrastive loss 3.4463729858398438, task loss 0.3583604097366333
2025-01-06 19:31:22,124 - root - INFO - [Train] Epoch 4 [9500/35182]: Training loss 0.3821285367012024, current lr 9.786775532247521e-05, contrastive loss 3.431332588195801, task loss 0.34781521558761597
2025-01-06 19:33:02,810 - root - INFO - [Train] Epoch 4 [9600/35182]: Training loss 0.4389282166957855, current lr 9.786633410079304e-05, contrastive loss 3.2820887565612793, task loss 0.4061073362827301
2025-01-06 19:34:43,209 - root - INFO - [Train] Epoch 4 [9700/35182]: Training loss 0.45496809482574463, current lr 9.78649128791109e-05, contrastive loss 3.505375623703003, task loss 0.4199143350124359
2025-01-06 19:36:23,888 - root - INFO - [Train] Epoch 4 [9800/35182]: Training loss 0.4247583746910095, current lr 9.786349165742873e-05, contrastive loss 3.2989463806152344, task loss 0.3917689025402069
2025-01-06 19:38:04,281 - root - INFO - [Train] Epoch 4 [9900/35182]: Training loss 0.44427618384361267, current lr 9.786207043574658e-05, contrastive loss 3.3247320652008057, task loss 0.4110288619995117
2025-01-06 19:39:44,871 - root - INFO - [Train] Epoch 4 [10000/35182]: Training loss 0.3844605088233948, current lr 9.786064921406441e-05, contrastive loss 3.610168933868408, task loss 0.34835880994796753
2025-01-06 19:41:25,268 - root - INFO - [Train] Epoch 4 [10100/35182]: Training loss 0.3675236403942108, current lr 9.785922799238226e-05, contrastive loss 3.4672584533691406, task loss 0.3328510522842407
2025-01-06 19:43:05,947 - root - INFO - [Train] Epoch 4 [10200/35182]: Training loss 0.4767796993255615, current lr 9.78578067707001e-05, contrastive loss 3.3595638275146484, task loss 0.4431840777397156
2025-01-06 19:44:46,330 - root - INFO - [Train] Epoch 4 [10300/35182]: Training loss 0.40974387526512146, current lr 9.785638554901794e-05, contrastive loss 3.295936346054077, task loss 0.3767845034599304
2025-01-06 19:46:26,920 - root - INFO - [Train] Epoch 4 [10400/35182]: Training loss 0.46706414222717285, current lr 9.785496432733579e-05, contrastive loss 3.4586734771728516, task loss 0.4324774146080017
2025-01-06 19:48:07,303 - root - INFO - [Train] Epoch 4 [10500/35182]: Training loss 0.4503880739212036, current lr 9.785354310565362e-05, contrastive loss 3.5277233123779297, task loss 0.41511085629463196
2025-01-06 19:49:47,878 - root - INFO - [Train] Epoch 4 [10600/35182]: Training loss 0.45046466588974, current lr 9.785212188397147e-05, contrastive loss 3.3503546714782715, task loss 0.416961133480072
2025-01-06 19:51:28,263 - root - INFO - [Train] Epoch 4 [10700/35182]: Training loss 0.4607991576194763, current lr 9.78507006622893e-05, contrastive loss 3.258769989013672, task loss 0.4282114505767822
2025-01-06 19:53:08,847 - root - INFO - [Train] Epoch 4 [10800/35182]: Training loss 0.47656914591789246, current lr 9.784927944060716e-05, contrastive loss 3.411679267883301, task loss 0.4424523413181305
2025-01-06 19:54:49,236 - root - INFO - [Train] Epoch 4 [10900/35182]: Training loss 0.4525114595890045, current lr 9.784785821892499e-05, contrastive loss 3.432967185974121, task loss 0.41818177700042725
2025-01-06 19:56:29,813 - root - INFO - [Train] Epoch 4 [11000/35182]: Training loss 0.43426835536956787, current lr 9.784643699724283e-05, contrastive loss 3.4694268703460693, task loss 0.3995741009712219
2025-01-06 19:58:10,203 - root - INFO - [Train] Epoch 4 [11100/35182]: Training loss 0.4104827642440796, current lr 9.784501577556068e-05, contrastive loss 3.570138931274414, task loss 0.37478137016296387
2025-01-06 19:59:50,886 - root - INFO - [Train] Epoch 4 [11200/35182]: Training loss 0.4320094883441925, current lr 9.784359455387852e-05, contrastive loss 3.4614319801330566, task loss 0.39739516377449036
2025-01-06 20:01:31,291 - root - INFO - [Train] Epoch 4 [11300/35182]: Training loss 0.40886253118515015, current lr 9.784217333219636e-05, contrastive loss 3.5512146949768066, task loss 0.3733503818511963
2025-01-06 20:03:11,974 - root - INFO - [Train] Epoch 4 [11400/35182]: Training loss 0.38445016741752625, current lr 9.78407521105142e-05, contrastive loss 3.195230007171631, task loss 0.3524978756904602
2025-01-06 20:04:52,371 - root - INFO - [Train] Epoch 4 [11500/35182]: Training loss 0.4410286843776703, current lr 9.783933088883205e-05, contrastive loss 3.53035569190979, task loss 0.4057251214981079
2025-01-06 20:06:33,047 - root - INFO - [Train] Epoch 4 [11600/35182]: Training loss 0.4353780448436737, current lr 9.783790966714988e-05, contrastive loss 3.2813830375671387, task loss 0.40256422758102417
2025-01-06 20:08:13,437 - root - INFO - [Train] Epoch 4 [11700/35182]: Training loss 0.41713836789131165, current lr 9.783648844546774e-05, contrastive loss 3.4062414169311523, task loss 0.3830759525299072
2025-01-06 20:09:54,025 - root - INFO - [Train] Epoch 4 [11800/35182]: Training loss 0.3886221647262573, current lr 9.783506722378557e-05, contrastive loss 3.4531490802764893, task loss 0.3540906608104706
2025-01-06 20:11:34,418 - root - INFO - [Train] Epoch 4 [11900/35182]: Training loss 0.3992225229740143, current lr 9.783364600210341e-05, contrastive loss 3.4015657901763916, task loss 0.36520686745643616
2025-01-06 20:13:14,990 - root - INFO - [Train] Epoch 4 [12000/35182]: Training loss 0.45042484998703003, current lr 9.783222478042125e-05, contrastive loss 3.560326099395752, task loss 0.414821594953537
2025-01-06 20:14:55,388 - root - INFO - [Train] Epoch 4 [12100/35182]: Training loss 0.428104966878891, current lr 9.78308035587391e-05, contrastive loss 3.373375177383423, task loss 0.3943712115287781
2025-01-06 20:16:36,080 - root - INFO - [Train] Epoch 4 [12200/35182]: Training loss 0.39011427760124207, current lr 9.782938233705694e-05, contrastive loss 3.433683395385742, task loss 0.35577744245529175
2025-01-06 20:18:16,466 - root - INFO - [Train] Epoch 4 [12300/35182]: Training loss 0.4419131577014923, current lr 9.782796111537478e-05, contrastive loss 3.508467674255371, task loss 0.40682849287986755
2025-01-06 20:19:57,152 - root - INFO - [Train] Epoch 4 [12400/35182]: Training loss 0.4316622316837311, current lr 9.782653989369263e-05, contrastive loss 3.4834306240081787, task loss 0.39682793617248535
2025-01-06 20:21:37,839 - root - INFO - [Train] Epoch 4 [12500/35182]: Training loss 0.411408394575119, current lr 9.782511867201046e-05, contrastive loss 3.1418495178222656, task loss 0.379989892244339
2025-01-06 20:23:18,224 - root - INFO - [Train] Epoch 4 [12600/35182]: Training loss 0.393951952457428, current lr 9.782369745032831e-05, contrastive loss 3.4046971797943115, task loss 0.3599049746990204
2025-01-06 20:24:58,908 - root - INFO - [Train] Epoch 4 [12700/35182]: Training loss 0.415968656539917, current lr 9.782227622864614e-05, contrastive loss 3.275454044342041, task loss 0.3832141160964966
2025-01-06 20:26:39,314 - root - INFO - [Train] Epoch 4 [12800/35182]: Training loss 0.5444862246513367, current lr 9.7820855006964e-05, contrastive loss 3.5476503372192383, task loss 0.5090097188949585
2025-01-06 20:28:19,973 - root - INFO - [Train] Epoch 4 [12900/35182]: Training loss 0.4545435309410095, current lr 9.781943378528183e-05, contrastive loss 3.256211519241333, task loss 0.42198142409324646
2025-01-06 20:30:00,370 - root - INFO - [Train] Epoch 4 [13000/35182]: Training loss 0.4873567223548889, current lr 9.781801256359967e-05, contrastive loss 3.554288387298584, task loss 0.45181384682655334
2025-01-06 20:31:40,968 - root - INFO - [Train] Epoch 4 [13100/35182]: Training loss 0.40817129611968994, current lr 9.781659134191752e-05, contrastive loss 3.600306510925293, task loss 0.37216824293136597
2025-01-06 20:33:21,363 - root - INFO - [Train] Epoch 4 [13200/35182]: Training loss 0.4028143882751465, current lr 9.781517012023536e-05, contrastive loss 3.3325085639953613, task loss 0.36948931217193604
2025-01-06 20:35:02,049 - root - INFO - [Train] Epoch 4 [13300/35182]: Training loss 0.49145716428756714, current lr 9.78137488985532e-05, contrastive loss 3.535101890563965, task loss 0.45610615611076355
2025-01-06 20:36:42,448 - root - INFO - [Train] Epoch 4 [13400/35182]: Training loss 0.4300093650817871, current lr 9.781232767687103e-05, contrastive loss 3.5938212871551514, task loss 0.39407116174697876
2025-01-06 20:38:23,117 - root - INFO - [Train] Epoch 4 [13500/35182]: Training loss 0.40710991621017456, current lr 9.781090645518889e-05, contrastive loss 3.3599987030029297, task loss 0.3735099136829376
2025-01-06 20:40:03,519 - root - INFO - [Train] Epoch 4 [13600/35182]: Training loss 0.450948029756546, current lr 9.780948523350672e-05, contrastive loss 3.441650867462158, task loss 0.4165315330028534
2025-01-06 20:41:44,119 - root - INFO - [Train] Epoch 4 [13700/35182]: Training loss 0.35107341408729553, current lr 9.780806401182458e-05, contrastive loss 3.294548749923706, task loss 0.31812793016433716
2025-01-06 20:43:24,499 - root - INFO - [Train] Epoch 4 [13800/35182]: Training loss 0.41388848423957825, current lr 9.780664279014241e-05, contrastive loss 3.5511136054992676, task loss 0.37837734818458557
2025-01-06 20:45:05,190 - root - INFO - [Train] Epoch 4 [13900/35182]: Training loss 0.41392216086387634, current lr 9.780522156846025e-05, contrastive loss 3.4942984580993652, task loss 0.3789791762828827
2025-01-06 20:46:45,849 - root - INFO - [Train] Epoch 4 [14000/35182]: Training loss 0.462703138589859, current lr 9.78038003467781e-05, contrastive loss 3.6096088886260986, task loss 0.42660704255104065
2025-01-06 20:48:26,242 - root - INFO - [Train] Epoch 4 [14100/35182]: Training loss 0.3968846797943115, current lr 9.780237912509594e-05, contrastive loss 3.579634189605713, task loss 0.3610883355140686
2025-01-06 20:50:06,836 - root - INFO - [Train] Epoch 4 [14200/35182]: Training loss 0.41693058609962463, current lr 9.780095790341378e-05, contrastive loss 3.3645224571228027, task loss 0.38328537344932556
2025-01-06 20:51:47,218 - root - INFO - [Train] Epoch 4 [14300/35182]: Training loss 0.4018764793872833, current lr 9.779953668173162e-05, contrastive loss 3.4991462230682373, task loss 0.3668850064277649
2025-01-06 20:53:27,803 - root - INFO - [Train] Epoch 4 [14400/35182]: Training loss 0.5003268718719482, current lr 9.779811546004947e-05, contrastive loss 3.3989548683166504, task loss 0.46633732318878174
2025-01-06 20:55:08,215 - root - INFO - [Train] Epoch 4 [14500/35182]: Training loss 0.4260878264904022, current lr 9.77966942383673e-05, contrastive loss 3.546985387802124, task loss 0.3906179666519165
2025-01-06 20:56:48,883 - root - INFO - [Train] Epoch 4 [14600/35182]: Training loss 0.3737575113773346, current lr 9.779527301668515e-05, contrastive loss 3.627138614654541, task loss 0.3374861180782318
2025-01-06 20:58:29,291 - root - INFO - [Train] Epoch 4 [14700/35182]: Training loss 0.3753243088722229, current lr 9.779385179500298e-05, contrastive loss 3.5344018936157227, task loss 0.33998027443885803
2025-01-06 21:00:09,963 - root - INFO - [Train] Epoch 4 [14800/35182]: Training loss 0.46752268075942993, current lr 9.779243057332084e-05, contrastive loss 3.533364772796631, task loss 0.432189017534256
2025-01-06 21:01:50,351 - root - INFO - [Train] Epoch 4 [14900/35182]: Training loss 0.4947889447212219, current lr 9.779100935163867e-05, contrastive loss 3.516061782836914, task loss 0.4596283435821533
2025-01-06 21:03:30,934 - root - INFO - [Train] Epoch 4 [15000/35182]: Training loss 0.38309425115585327, current lr 9.778958812995651e-05, contrastive loss 3.6185340881347656, task loss 0.34690892696380615
2025-01-06 21:05:11,335 - root - INFO - [Train] Epoch 4 [15100/35182]: Training loss 0.4172913730144501, current lr 9.778816690827436e-05, contrastive loss 3.54267954826355, task loss 0.3818645775318146
2025-01-06 21:06:52,017 - root - INFO - [Train] Epoch 4 [15200/35182]: Training loss 0.3906118869781494, current lr 9.77867456865922e-05, contrastive loss 3.4155006408691406, task loss 0.3564568758010864
2025-01-06 21:08:32,419 - root - INFO - [Train] Epoch 4 [15300/35182]: Training loss 0.4418976902961731, current lr 9.778532446491004e-05, contrastive loss 3.5138120651245117, task loss 0.4067595601081848
2025-01-06 21:10:13,089 - root - INFO - [Train] Epoch 4 [15400/35182]: Training loss 0.3775811493396759, current lr 9.778390324322787e-05, contrastive loss 3.447021245956421, task loss 0.34311094880104065
2025-01-06 21:11:53,485 - root - INFO - [Train] Epoch 4 [15500/35182]: Training loss 0.38959449529647827, current lr 9.778248202154573e-05, contrastive loss 3.6775307655334473, task loss 0.35281920433044434
2025-01-06 21:13:34,072 - root - INFO - [Train] Epoch 4 [15600/35182]: Training loss 0.36897191405296326, current lr 9.778106079986356e-05, contrastive loss 3.5683979988098145, task loss 0.33328792452812195
2025-01-06 21:15:14,463 - root - INFO - [Train] Epoch 4 [15700/35182]: Training loss 0.43959710001945496, current lr 9.777963957818142e-05, contrastive loss 3.447648525238037, task loss 0.4051206111907959
2025-01-06 21:16:55,135 - root - INFO - [Train] Epoch 4 [15800/35182]: Training loss 0.41777554154396057, current lr 9.777821835649925e-05, contrastive loss 3.449030876159668, task loss 0.3832852244377136
2025-01-06 21:18:35,542 - root - INFO - [Train] Epoch 4 [15900/35182]: Training loss 0.4278714954853058, current lr 9.777679713481709e-05, contrastive loss 3.515890598297119, task loss 0.3927125930786133
2025-01-06 21:20:16,107 - root - INFO - [Train] Epoch 4 [16000/35182]: Training loss 0.3892398178577423, current lr 9.777537591313493e-05, contrastive loss 3.479051113128662, task loss 0.3544493019580841
2025-01-06 21:21:56,502 - root - INFO - [Train] Epoch 4 [16100/35182]: Training loss 0.3824100196361542, current lr 9.777395469145278e-05, contrastive loss 3.385282516479492, task loss 0.3485572040081024
2025-01-06 21:23:37,168 - root - INFO - [Train] Epoch 4 [16200/35182]: Training loss 0.42060020565986633, current lr 9.777253346977062e-05, contrastive loss 3.4760053157806396, task loss 0.38584014773368835
2025-01-06 21:25:17,568 - root - INFO - [Train] Epoch 4 [16300/35182]: Training loss 0.3866693675518036, current lr 9.777111224808847e-05, contrastive loss 3.2980833053588867, task loss 0.3536885380744934
2025-01-06 21:26:58,159 - root - INFO - [Train] Epoch 4 [16400/35182]: Training loss 0.40780237317085266, current lr 9.776969102640631e-05, contrastive loss 3.496218681335449, task loss 0.37284019589424133
2025-01-06 21:28:38,843 - root - INFO - [Train] Epoch 4 [16500/35182]: Training loss 0.3893011212348938, current lr 9.776826980472414e-05, contrastive loss 3.5898056030273438, task loss 0.3534030616283417
2025-01-06 21:30:19,233 - root - INFO - [Train] Epoch 4 [16600/35182]: Training loss 0.35159507393836975, current lr 9.7766848583042e-05, contrastive loss 3.299213409423828, task loss 0.31860294938087463
2025-01-06 21:31:59,812 - root - INFO - [Train] Epoch 4 [16700/35182]: Training loss 0.3995457887649536, current lr 9.776542736135983e-05, contrastive loss 3.2249999046325684, task loss 0.3672958016395569
2025-01-06 21:33:40,206 - root - INFO - [Train] Epoch 4 [16800/35182]: Training loss 0.3774338364601135, current lr 9.776400613967767e-05, contrastive loss 3.4005889892578125, task loss 0.34342795610427856
2025-01-06 21:35:20,888 - root - INFO - [Train] Epoch 4 [16900/35182]: Training loss 0.36718279123306274, current lr 9.776258491799551e-05, contrastive loss 3.332561492919922, task loss 0.3338571786880493
2025-01-06 21:37:01,283 - root - INFO - [Train] Epoch 4 [17000/35182]: Training loss 0.3905545175075531, current lr 9.776116369631336e-05, contrastive loss 3.378861665725708, task loss 0.35676589608192444
2025-01-06 21:38:41,857 - root - INFO - [Train] Epoch 4 [17100/35182]: Training loss 0.45146965980529785, current lr 9.77597424746312e-05, contrastive loss 3.613614797592163, task loss 0.41533350944519043
2025-01-06 21:40:22,280 - root - INFO - [Train] Epoch 4 [17200/35182]: Training loss 0.4555976390838623, current lr 9.775832125294904e-05, contrastive loss 3.6320438385009766, task loss 0.4192771911621094
2025-01-06 21:42:02,942 - root - INFO - [Train] Epoch 4 [17300/35182]: Training loss 0.44284892082214355, current lr 9.775690003126689e-05, contrastive loss 3.498098850250244, task loss 0.4078679382801056
2025-01-06 21:43:43,328 - root - INFO - [Train] Epoch 4 [17400/35182]: Training loss 0.46335649490356445, current lr 9.775547880958472e-05, contrastive loss 3.4206185340881348, task loss 0.4291503131389618
2025-01-06 21:45:23,909 - root - INFO - [Train] Epoch 4 [17500/35182]: Training loss 0.4370090961456299, current lr 9.775405758790257e-05, contrastive loss 3.4842982292175293, task loss 0.40216609835624695
2025-01-07 00:30:59,581 - root - INFO - [Valid] Evaluation Results: HR@5:0.3148,NDCG@5:0.2015,HR@10:0.4772,NDCG@10:0.2539,HR@20:0.6573,NDCG@20:0.2995
2025-01-07 00:31:00,447 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-07 00:31:10,485 - root - INFO - [Train] Epoch 4 [17600/35182]: Training loss 0.3983263671398163, current lr 9.77526363662204e-05, contrastive loss 3.5509724617004395, task loss 0.36281663179397583
2025-01-07 00:32:51,125 - root - INFO - [Train] Epoch 4 [17700/35182]: Training loss 0.45110395550727844, current lr 9.775121514453826e-05, contrastive loss 3.4755191802978516, task loss 0.41634875535964966
2025-01-07 00:34:31,799 - root - INFO - [Train] Epoch 4 [17800/35182]: Training loss 0.45127320289611816, current lr 9.774979392285609e-05, contrastive loss 3.3752169609069824, task loss 0.41752102971076965
2025-01-07 00:36:12,204 - root - INFO - [Train] Epoch 4 [17900/35182]: Training loss 0.3337377905845642, current lr 9.774837270117393e-05, contrastive loss 3.6159543991088867, task loss 0.29757824540138245
2025-01-07 00:37:52,592 - root - INFO - [Train] Epoch 4 [18000/35182]: Training loss 0.4448259174823761, current lr 9.774695147949178e-05, contrastive loss 3.487783670425415, task loss 0.40994808077812195
2025-01-07 00:39:33,190 - root - INFO - [Train] Epoch 4 [18100/35182]: Training loss 0.4516628682613373, current lr 9.774553025780962e-05, contrastive loss 3.412682056427002, task loss 0.41753605008125305
2025-01-07 00:41:13,859 - root - INFO - [Train] Epoch 4 [18200/35182]: Training loss 0.3474558889865875, current lr 9.774410903612746e-05, contrastive loss 3.4090898036956787, task loss 0.31336498260498047
2025-01-07 00:42:54,265 - root - INFO - [Train] Epoch 4 [18300/35182]: Training loss 0.47500428557395935, current lr 9.77426878144453e-05, contrastive loss 3.373948097229004, task loss 0.441264808177948
2025-01-07 00:44:34,943 - root - INFO - [Train] Epoch 4 [18400/35182]: Training loss 0.4064014256000519, current lr 9.774126659276315e-05, contrastive loss 3.446178436279297, task loss 0.37193962931632996
2025-01-07 00:46:15,350 - root - INFO - [Train] Epoch 4 [18500/35182]: Training loss 0.3809661567211151, current lr 9.773984537108098e-05, contrastive loss 3.473996162414551, task loss 0.34622618556022644
2025-01-07 00:47:56,029 - root - INFO - [Train] Epoch 4 [18600/35182]: Training loss 0.38841351866722107, current lr 9.773842414939884e-05, contrastive loss 3.4863319396972656, task loss 0.3535501956939697
2025-01-07 00:49:36,425 - root - INFO - [Train] Epoch 4 [18700/35182]: Training loss 0.49664610624313354, current lr 9.773700292771667e-05, contrastive loss 3.511232614517212, task loss 0.461533784866333
2025-01-07 00:51:17,104 - root - INFO - [Train] Epoch 4 [18800/35182]: Training loss 0.5040072202682495, current lr 9.773558170603451e-05, contrastive loss 3.528269052505493, task loss 0.4687245488166809
2025-01-07 00:52:57,781 - root - INFO - [Train] Epoch 4 [18900/35182]: Training loss 0.433609277009964, current lr 9.773416048435235e-05, contrastive loss 3.494112730026245, task loss 0.3986681401729584
2025-01-07 00:54:38,179 - root - INFO - [Train] Epoch 4 [19000/35182]: Training loss 0.3818824589252472, current lr 9.77327392626702e-05, contrastive loss 3.562347888946533, task loss 0.3462589681148529
2025-01-07 00:56:18,566 - root - INFO - [Train] Epoch 4 [19100/35182]: Training loss 0.45358604192733765, current lr 9.773131804098804e-05, contrastive loss 3.6892271041870117, task loss 0.416693776845932
2025-01-07 00:57:59,162 - root - INFO - [Train] Epoch 4 [19200/35182]: Training loss 0.39240318536758423, current lr 9.772989681930588e-05, contrastive loss 3.340343952178955, task loss 0.35899975895881653
2025-01-07 00:59:39,626 - root - INFO - [Train] Epoch 4 [19300/35182]: Training loss 0.4227698743343353, current lr 9.772847559762373e-05, contrastive loss 3.3277056217193604, task loss 0.38949280977249146
2025-01-07 01:01:20,132 - root - INFO - [Train] Epoch 4 [19400/35182]: Training loss 0.47146955132484436, current lr 9.772705437594156e-05, contrastive loss 3.4517314434051514, task loss 0.43695223331451416
2025-01-07 01:03:00,609 - root - INFO - [Train] Epoch 4 [19500/35182]: Training loss 0.3581768870353699, current lr 9.772563315425941e-05, contrastive loss 3.3013980388641357, task loss 0.3251629173755646
2025-01-07 01:04:41,095 - root - INFO - [Train] Epoch 4 [19600/35182]: Training loss 0.41402262449264526, current lr 9.772421193257724e-05, contrastive loss 3.417677879333496, task loss 0.37984585762023926
2025-01-07 01:06:21,476 - root - INFO - [Train] Epoch 4 [19700/35182]: Training loss 0.3611064851284027, current lr 9.772279071089509e-05, contrastive loss 3.2547731399536133, task loss 0.3285587430000305
2025-01-07 01:08:02,056 - root - INFO - [Train] Epoch 4 [19800/35182]: Training loss 0.3976227641105652, current lr 9.772136948921293e-05, contrastive loss 3.425581932067871, task loss 0.363366961479187
2025-01-07 01:09:42,443 - root - INFO - [Train] Epoch 4 [19900/35182]: Training loss 0.4067554473876953, current lr 9.771994826753077e-05, contrastive loss 3.4341635704040527, task loss 0.3724138140678406
2025-01-07 01:11:23,008 - root - INFO - [Train] Epoch 4 [20000/35182]: Training loss 0.39938461780548096, current lr 9.771852704584862e-05, contrastive loss 3.3771612644195557, task loss 0.36561301350593567
2025-01-07 01:13:03,392 - root - INFO - [Train] Epoch 4 [20100/35182]: Training loss 0.43809324502944946, current lr 9.771710582416646e-05, contrastive loss 3.58042573928833, task loss 0.4022890031337738
2025-01-07 01:14:43,984 - root - INFO - [Train] Epoch 4 [20200/35182]: Training loss 0.4394167363643646, current lr 9.77156846024843e-05, contrastive loss 3.543931007385254, task loss 0.4039774239063263
2025-01-07 01:16:24,381 - root - INFO - [Train] Epoch 4 [20300/35182]: Training loss 0.42361360788345337, current lr 9.771426338080215e-05, contrastive loss 3.279751777648926, task loss 0.3908160924911499
2025-01-07 01:18:05,063 - root - INFO - [Train] Epoch 4 [20400/35182]: Training loss 0.42017436027526855, current lr 9.771284215911999e-05, contrastive loss 3.543682813644409, task loss 0.3847375214099884
2025-01-07 01:19:45,445 - root - INFO - [Train] Epoch 4 [20500/35182]: Training loss 0.3804454803466797, current lr 9.771142093743782e-05, contrastive loss 3.440186023712158, task loss 0.3460436165332794
2025-01-07 01:21:26,040 - root - INFO - [Train] Epoch 4 [20600/35182]: Training loss 0.48345839977264404, current lr 9.770999971575568e-05, contrastive loss 3.4775190353393555, task loss 0.4486832022666931
2025-01-07 01:23:06,430 - root - INFO - [Train] Epoch 4 [20700/35182]: Training loss 0.40909597277641296, current lr 9.77085784940735e-05, contrastive loss 3.5524673461914062, task loss 0.3735713064670563
2025-01-07 01:24:47,103 - root - INFO - [Train] Epoch 4 [20800/35182]: Training loss 0.3740173578262329, current lr 9.770715727239135e-05, contrastive loss 3.4620912075042725, task loss 0.3393964469432831
2025-01-07 01:26:27,495 - root - INFO - [Train] Epoch 4 [20900/35182]: Training loss 0.4562254250049591, current lr 9.770573605070919e-05, contrastive loss 3.621433973312378, task loss 0.42001107335090637
2025-01-07 01:28:08,076 - root - INFO - [Train] Epoch 4 [21000/35182]: Training loss 0.406148225069046, current lr 9.770431482902704e-05, contrastive loss 3.5896384716033936, task loss 0.3702518343925476
2025-01-07 01:29:48,484 - root - INFO - [Train] Epoch 4 [21100/35182]: Training loss 0.433449387550354, current lr 9.770289360734488e-05, contrastive loss 3.3772940635681152, task loss 0.39967644214630127
2025-01-07 01:31:29,162 - root - INFO - [Train] Epoch 4 [21200/35182]: Training loss 0.41326892375946045, current lr 9.770147238566272e-05, contrastive loss 3.4656596183776855, task loss 0.37861233949661255
2025-01-07 01:33:09,540 - root - INFO - [Train] Epoch 4 [21300/35182]: Training loss 0.41306427121162415, current lr 9.770005116398057e-05, contrastive loss 3.5062613487243652, task loss 0.3780016601085663
2025-01-07 01:34:50,114 - root - INFO - [Train] Epoch 4 [21400/35182]: Training loss 0.4168648421764374, current lr 9.76986299422984e-05, contrastive loss 3.7276675701141357, task loss 0.37958815693855286
2025-01-07 01:36:30,494 - root - INFO - [Train] Epoch 4 [21500/35182]: Training loss 0.3963313102722168, current lr 9.769720872061625e-05, contrastive loss 3.5463037490844727, task loss 0.36086827516555786
2025-01-07 01:38:11,084 - root - INFO - [Train] Epoch 4 [21600/35182]: Training loss 0.4096944332122803, current lr 9.769578749893408e-05, contrastive loss 3.5991950035095215, task loss 0.3737024962902069
2025-01-07 01:39:51,463 - root - INFO - [Train] Epoch 4 [21700/35182]: Training loss 0.44292497634887695, current lr 9.769436627725193e-05, contrastive loss 3.486612319946289, task loss 0.40805885195732117
2025-01-07 01:41:32,051 - root - INFO - [Train] Epoch 4 [21800/35182]: Training loss 0.3693382740020752, current lr 9.769294505556977e-05, contrastive loss 3.4604296684265137, task loss 0.3347339630126953
2025-01-07 01:43:12,438 - root - INFO - [Train] Epoch 4 [21900/35182]: Training loss 0.5298147797584534, current lr 9.769152383388761e-05, contrastive loss 3.379715919494629, task loss 0.4960176348686218
2025-01-07 01:44:53,017 - root - INFO - [Train] Epoch 4 [22000/35182]: Training loss 0.405922532081604, current lr 9.769010261220546e-05, contrastive loss 3.4863805770874023, task loss 0.37105873227119446
2025-01-07 01:46:33,404 - root - INFO - [Train] Epoch 4 [22100/35182]: Training loss 0.39687487483024597, current lr 9.76886813905233e-05, contrastive loss 3.3095803260803223, task loss 0.36377906799316406
2025-01-07 01:48:13,977 - root - INFO - [Train] Epoch 4 [22200/35182]: Training loss 0.36363187432289124, current lr 9.768726016884114e-05, contrastive loss 3.7579498291015625, task loss 0.32605236768722534
2025-01-07 01:49:54,382 - root - INFO - [Train] Epoch 4 [22300/35182]: Training loss 0.3781239688396454, current lr 9.768583894715899e-05, contrastive loss 3.4971797466278076, task loss 0.34315216541290283
2025-01-07 01:51:35,053 - root - INFO - [Train] Epoch 4 [22400/35182]: Training loss 0.3718745708465576, current lr 9.768441772547683e-05, contrastive loss 3.5713329315185547, task loss 0.33616122603416443
2025-01-07 01:53:15,430 - root - INFO - [Train] Epoch 4 [22500/35182]: Training loss 0.4394409954547882, current lr 9.768299650379466e-05, contrastive loss 3.5543570518493652, task loss 0.4038974344730377
2025-01-07 01:54:56,023 - root - INFO - [Train] Epoch 4 [22600/35182]: Training loss 0.41231459379196167, current lr 9.768157528211252e-05, contrastive loss 3.6350841522216797, task loss 0.3759637475013733
2025-01-07 01:56:36,420 - root - INFO - [Train] Epoch 4 [22700/35182]: Training loss 0.416452556848526, current lr 9.768015406043035e-05, contrastive loss 3.435271739959717, task loss 0.38209983706474304
2025-01-07 01:58:16,984 - root - INFO - [Train] Epoch 4 [22800/35182]: Training loss 0.437166690826416, current lr 9.767873283874819e-05, contrastive loss 3.471897602081299, task loss 0.40244773030281067
2025-01-07 01:59:57,359 - root - INFO - [Train] Epoch 4 [22900/35182]: Training loss 0.4024929106235504, current lr 9.767731161706603e-05, contrastive loss 3.398188352584839, task loss 0.36851102113723755
2025-01-07 02:01:37,954 - root - INFO - [Train] Epoch 4 [23000/35182]: Training loss 0.3686537742614746, current lr 9.767589039538388e-05, contrastive loss 3.513010025024414, task loss 0.3335236608982086
2025-01-07 02:03:18,344 - root - INFO - [Train] Epoch 4 [23100/35182]: Training loss 0.4404531717300415, current lr 9.767446917370172e-05, contrastive loss 3.475839376449585, task loss 0.40569478273391724
2025-01-07 02:04:58,927 - root - INFO - [Train] Epoch 4 [23200/35182]: Training loss 0.3885200321674347, current lr 9.767304795201956e-05, contrastive loss 3.5568389892578125, task loss 0.35295164585113525
2025-01-07 02:06:39,330 - root - INFO - [Train] Epoch 4 [23300/35182]: Training loss 0.3836541473865509, current lr 9.767162673033741e-05, contrastive loss 3.487460136413574, task loss 0.348779559135437
2025-01-07 02:08:20,002 - root - INFO - [Train] Epoch 4 [23400/35182]: Training loss 0.40224316716194153, current lr 9.767020550865524e-05, contrastive loss 3.411726474761963, task loss 0.36812591552734375
2025-01-07 02:10:00,404 - root - INFO - [Train] Epoch 4 [23500/35182]: Training loss 0.41030487418174744, current lr 9.76687842869731e-05, contrastive loss 3.497556686401367, task loss 0.37532931566238403
2025-01-07 02:11:41,082 - root - INFO - [Train] Epoch 4 [23600/35182]: Training loss 0.40040266513824463, current lr 9.766736306529092e-05, contrastive loss 3.4780941009521484, task loss 0.3656217157840729
2025-01-07 02:13:21,470 - root - INFO - [Train] Epoch 4 [23700/35182]: Training loss 0.41153326630592346, current lr 9.766594184360877e-05, contrastive loss 3.3421006202697754, task loss 0.378112256526947
2025-01-07 02:15:02,056 - root - INFO - [Train] Epoch 4 [23800/35182]: Training loss 0.35709238052368164, current lr 9.766452062192661e-05, contrastive loss 3.326312780380249, task loss 0.3238292634487152
2025-01-07 02:16:42,439 - root - INFO - [Train] Epoch 4 [23900/35182]: Training loss 0.346523255109787, current lr 9.766309940024445e-05, contrastive loss 3.5698366165161133, task loss 0.3108249008655548
2025-01-07 02:18:23,016 - root - INFO - [Train] Epoch 4 [24000/35182]: Training loss 0.3978888988494873, current lr 9.76616781785623e-05, contrastive loss 3.30234694480896, task loss 0.36486542224884033
2025-01-07 02:20:03,405 - root - INFO - [Train] Epoch 4 [24100/35182]: Training loss 0.378867506980896, current lr 9.766025695688014e-05, contrastive loss 3.5216972827911377, task loss 0.34365054965019226
2025-01-07 02:21:43,978 - root - INFO - [Train] Epoch 4 [24200/35182]: Training loss 0.4444434642791748, current lr 9.765883573519798e-05, contrastive loss 3.610055923461914, task loss 0.4083428978919983
2025-01-07 02:23:24,382 - root - INFO - [Train] Epoch 4 [24300/35182]: Training loss 0.4666207432746887, current lr 9.765741451351583e-05, contrastive loss 3.5362467765808105, task loss 0.43125829100608826
2025-01-07 02:25:05,060 - root - INFO - [Train] Epoch 4 [24400/35182]: Training loss 0.39227214455604553, current lr 9.765599329183367e-05, contrastive loss 3.602759838104248, task loss 0.3562445342540741
2025-01-07 02:26:45,451 - root - INFO - [Train] Epoch 4 [24500/35182]: Training loss 0.40657851099967957, current lr 9.76545720701515e-05, contrastive loss 3.359199047088623, task loss 0.3729865252971649
2025-01-07 02:28:26,037 - root - INFO - [Train] Epoch 4 [24600/35182]: Training loss 0.3795279562473297, current lr 9.765315084846934e-05, contrastive loss 3.4075121879577637, task loss 0.34545284509658813
2025-01-07 02:30:06,425 - root - INFO - [Train] Epoch 4 [24700/35182]: Training loss 0.47299474477767944, current lr 9.765172962678719e-05, contrastive loss 3.430253744125366, task loss 0.43869221210479736
2025-01-07 02:31:47,108 - root - INFO - [Train] Epoch 4 [24800/35182]: Training loss 0.42562979459762573, current lr 9.765030840510503e-05, contrastive loss 3.5677437782287598, task loss 0.3899523615837097
2025-01-07 02:33:27,500 - root - INFO - [Train] Epoch 4 [24900/35182]: Training loss 0.38635289669036865, current lr 9.764888718342287e-05, contrastive loss 3.5602238178253174, task loss 0.3507506549358368
2025-01-07 02:35:08,080 - root - INFO - [Train] Epoch 4 [25000/35182]: Training loss 0.4150177240371704, current lr 9.764746596174072e-05, contrastive loss 3.414987802505493, task loss 0.3808678388595581
2025-01-07 02:36:48,480 - root - INFO - [Train] Epoch 4 [25100/35182]: Training loss 0.4129461944103241, current lr 9.764604474005856e-05, contrastive loss 3.4552526473999023, task loss 0.37839367985725403
2025-01-07 02:38:29,160 - root - INFO - [Train] Epoch 4 [25200/35182]: Training loss 0.469582200050354, current lr 9.76446235183764e-05, contrastive loss 3.6385295391082764, task loss 0.43319690227508545
2025-01-07 02:40:09,544 - root - INFO - [Train] Epoch 4 [25300/35182]: Training loss 0.3830083906650543, current lr 9.764320229669425e-05, contrastive loss 3.492384433746338, task loss 0.34808453917503357
2025-01-07 02:41:50,110 - root - INFO - [Train] Epoch 4 [25400/35182]: Training loss 0.33562543988227844, current lr 9.764178107501208e-05, contrastive loss 3.5381126403808594, task loss 0.3002443015575409
2025-01-07 02:43:30,503 - root - INFO - [Train] Epoch 4 [25500/35182]: Training loss 0.4762721359729767, current lr 9.764035985332993e-05, contrastive loss 3.450620651245117, task loss 0.4417659342288971
2025-01-07 02:45:11,082 - root - INFO - [Train] Epoch 4 [25600/35182]: Training loss 0.37853068113327026, current lr 9.763893863164776e-05, contrastive loss 3.403965950012207, task loss 0.34449103474617004
2025-01-07 02:46:51,466 - root - INFO - [Train] Epoch 4 [25700/35182]: Training loss 0.3992825448513031, current lr 9.763751740996561e-05, contrastive loss 3.4515085220336914, task loss 0.364767462015152
2025-01-07 02:48:32,056 - root - INFO - [Train] Epoch 4 [25800/35182]: Training loss 0.36414048075675964, current lr 9.763609618828345e-05, contrastive loss 3.5486459732055664, task loss 0.328654021024704
2025-01-07 02:50:12,452 - root - INFO - [Train] Epoch 4 [25900/35182]: Training loss 0.38329342007637024, current lr 9.76346749666013e-05, contrastive loss 3.597397804260254, task loss 0.34731945395469666
2025-01-07 02:51:53,123 - root - INFO - [Train] Epoch 4 [26000/35182]: Training loss 0.4698953330516815, current lr 9.763325374491914e-05, contrastive loss 3.483206033706665, task loss 0.43506327271461487
2025-01-07 02:53:33,498 - root - INFO - [Train] Epoch 4 [26100/35182]: Training loss 0.3368605971336365, current lr 9.763183252323698e-05, contrastive loss 3.513763666152954, task loss 0.3017229735851288
2025-01-07 02:55:14,099 - root - INFO - [Train] Epoch 4 [26200/35182]: Training loss 0.3198491930961609, current lr 9.763041130155483e-05, contrastive loss 3.3661069869995117, task loss 0.28618812561035156
2025-01-07 02:56:54,493 - root - INFO - [Train] Epoch 4 [26300/35182]: Training loss 0.37982121109962463, current lr 9.762899007987267e-05, contrastive loss 3.5448555946350098, task loss 0.3443726599216461
2025-01-07 02:58:35,166 - root - INFO - [Train] Epoch 4 [26400/35182]: Training loss 0.36919546127319336, current lr 9.762756885819051e-05, contrastive loss 3.4610166549682617, task loss 0.3345853090286255
2025-01-07 03:00:15,549 - root - INFO - [Train] Epoch 4 [26500/35182]: Training loss 0.3842931091785431, current lr 9.762614763650834e-05, contrastive loss 3.50484299659729, task loss 0.3492446839809418
2025-01-07 03:01:56,138 - root - INFO - [Train] Epoch 4 [26600/35182]: Training loss 0.41997411847114563, current lr 9.762472641482619e-05, contrastive loss 3.489450454711914, task loss 0.38507962226867676
2025-01-07 03:03:36,532 - root - INFO - [Train] Epoch 4 [26700/35182]: Training loss 0.367514431476593, current lr 9.762330519314403e-05, contrastive loss 3.705951690673828, task loss 0.33045491576194763
2025-01-07 03:05:17,107 - root - INFO - [Train] Epoch 4 [26800/35182]: Training loss 0.42482301592826843, current lr 9.762188397146187e-05, contrastive loss 3.668586254119873, task loss 0.38813716173171997
2025-01-07 03:06:57,499 - root - INFO - [Train] Epoch 4 [26900/35182]: Training loss 0.43739068508148193, current lr 9.762046274977972e-05, contrastive loss 3.4819724559783936, task loss 0.4025709629058838
2025-01-07 03:08:38,086 - root - INFO - [Train] Epoch 4 [27000/35182]: Training loss 0.3791886568069458, current lr 9.761904152809756e-05, contrastive loss 3.572309732437134, task loss 0.34346556663513184
2025-01-07 03:10:18,499 - root - INFO - [Train] Epoch 4 [27100/35182]: Training loss 0.34724161028862, current lr 9.76176203064154e-05, contrastive loss 3.45989990234375, task loss 0.3126426041126251
2025-01-07 03:11:59,157 - root - INFO - [Train] Epoch 4 [27200/35182]: Training loss 0.45633089542388916, current lr 9.761619908473325e-05, contrastive loss 3.517428398132324, task loss 0.4211566150188446
2025-01-07 03:13:39,539 - root - INFO - [Train] Epoch 4 [27300/35182]: Training loss 0.4050597846508026, current lr 9.761477786305109e-05, contrastive loss 3.6732354164123535, task loss 0.36832743883132935
2025-01-07 03:15:20,124 - root - INFO - [Train] Epoch 4 [27400/35182]: Training loss 0.366095632314682, current lr 9.761335664136892e-05, contrastive loss 3.441628932952881, task loss 0.3316793441772461
2025-01-07 03:17:00,503 - root - INFO - [Train] Epoch 4 [27500/35182]: Training loss 0.3511030375957489, current lr 9.761193541968676e-05, contrastive loss 3.5186924934387207, task loss 0.31591612100601196
2025-01-07 03:18:41,096 - root - INFO - [Train] Epoch 4 [27600/35182]: Training loss 0.4141300916671753, current lr 9.76105141980046e-05, contrastive loss 3.517659902572632, task loss 0.3789534866809845
2025-01-07 03:20:21,470 - root - INFO - [Train] Epoch 4 [27700/35182]: Training loss 0.36824995279312134, current lr 9.760909297632245e-05, contrastive loss 3.509747266769409, task loss 0.3331524729728699
2025-01-07 03:22:02,057 - root - INFO - [Train] Epoch 4 [27800/35182]: Training loss 0.4235387444496155, current lr 9.760767175464029e-05, contrastive loss 3.5499024391174316, task loss 0.3880397081375122
2025-01-07 03:23:42,435 - root - INFO - [Train] Epoch 4 [27900/35182]: Training loss 0.3351578116416931, current lr 9.760625053295814e-05, contrastive loss 3.5109596252441406, task loss 0.30004823207855225
2025-01-07 03:25:23,009 - root - INFO - [Train] Epoch 4 [28000/35182]: Training loss 0.3777385354042053, current lr 9.760482931127598e-05, contrastive loss 3.4396324157714844, task loss 0.34334221482276917
2025-01-07 03:27:03,404 - root - INFO - [Train] Epoch 4 [28100/35182]: Training loss 0.3558255136013031, current lr 9.760340808959382e-05, contrastive loss 3.5290966033935547, task loss 0.3205345571041107
2025-01-07 03:28:43,987 - root - INFO - [Train] Epoch 4 [28200/35182]: Training loss 0.38634395599365234, current lr 9.760198686791167e-05, contrastive loss 3.688387870788574, task loss 0.34946006536483765
2025-01-07 03:30:24,374 - root - INFO - [Train] Epoch 4 [28300/35182]: Training loss 0.3704867362976074, current lr 9.760056564622951e-05, contrastive loss 3.7982091903686523, task loss 0.33250463008880615
2025-01-07 03:32:05,075 - root - INFO - [Train] Epoch 4 [28400/35182]: Training loss 0.3842131793498993, current lr 9.759914442454735e-05, contrastive loss 3.709535598754883, task loss 0.3471178114414215
2025-01-07 03:33:45,465 - root - INFO - [Train] Epoch 4 [28500/35182]: Training loss 0.31977546215057373, current lr 9.759772320286518e-05, contrastive loss 3.745758533477783, task loss 0.2823178768157959
2025-01-07 03:35:26,151 - root - INFO - [Train] Epoch 4 [28600/35182]: Training loss 0.4694831967353821, current lr 9.759630198118303e-05, contrastive loss 3.7677998542785645, task loss 0.43180519342422485
2025-01-07 03:37:06,596 - root - INFO - [Train] Epoch 4 [28700/35182]: Training loss 0.3966781198978424, current lr 9.759488075950087e-05, contrastive loss 3.480651617050171, task loss 0.361871600151062
2025-01-07 03:38:47,117 - root - INFO - [Train] Epoch 4 [28800/35182]: Training loss 0.39437851309776306, current lr 9.759345953781871e-05, contrastive loss 3.683459520339966, task loss 0.3575439155101776
2025-01-07 03:40:27,788 - root - INFO - [Train] Epoch 4 [28900/35182]: Training loss 0.36966726183891296, current lr 9.759203831613656e-05, contrastive loss 3.6066150665283203, task loss 0.33360111713409424
2025-01-07 03:42:08,179 - root - INFO - [Train] Epoch 4 [29000/35182]: Training loss 0.35254064202308655, current lr 9.75906170944544e-05, contrastive loss 3.5005128383636475, task loss 0.31753551959991455
2025-01-07 03:43:48,554 - root - INFO - [Train] Epoch 4 [29100/35182]: Training loss 0.40744495391845703, current lr 9.758919587277224e-05, contrastive loss 3.65122127532959, task loss 0.37093275785446167
2025-01-07 03:45:29,155 - root - INFO - [Train] Epoch 4 [29200/35182]: Training loss 0.3704763352870941, current lr 9.758777465109009e-05, contrastive loss 3.6105027198791504, task loss 0.33437129855155945
2025-01-07 03:47:09,528 - root - INFO - [Train] Epoch 4 [29300/35182]: Training loss 0.3634028434753418, current lr 9.758635342940793e-05, contrastive loss 3.6500158309936523, task loss 0.32690268754959106
2025-01-07 03:48:50,111 - root - INFO - [Train] Epoch 4 [29400/35182]: Training loss 0.3904951214790344, current lr 9.758493220772576e-05, contrastive loss 3.7489004135131836, task loss 0.35300612449645996
2025-01-07 03:50:30,506 - root - INFO - [Train] Epoch 4 [29500/35182]: Training loss 0.3888973891735077, current lr 9.75835109860436e-05, contrastive loss 3.587233781814575, task loss 0.35302504897117615
2025-01-07 03:52:11,087 - root - INFO - [Train] Epoch 4 [29600/35182]: Training loss 0.41510194540023804, current lr 9.758208976436145e-05, contrastive loss 3.502856731414795, task loss 0.3800733685493469
2025-01-07 03:53:51,478 - root - INFO - [Train] Epoch 4 [29700/35182]: Training loss 0.3478093445301056, current lr 9.758066854267929e-05, contrastive loss 3.4337987899780273, task loss 0.31347134709358215
2025-01-07 03:55:32,050 - root - INFO - [Train] Epoch 4 [29800/35182]: Training loss 0.36659541726112366, current lr 9.757924732099713e-05, contrastive loss 3.5758533477783203, task loss 0.3308368921279907
2025-01-07 03:57:12,435 - root - INFO - [Train] Epoch 4 [29900/35182]: Training loss 0.40755826234817505, current lr 9.757782609931498e-05, contrastive loss 3.69486665725708, task loss 0.3706095814704895
2025-01-07 03:58:53,014 - root - INFO - [Train] Epoch 4 [30000/35182]: Training loss 0.3504222631454468, current lr 9.757640487763282e-05, contrastive loss 3.6061301231384277, task loss 0.31436097621917725
2025-01-07 04:00:33,403 - root - INFO - [Train] Epoch 4 [30100/35182]: Training loss 0.362430602312088, current lr 9.757498365595066e-05, contrastive loss 3.67391300201416, task loss 0.32569146156311035
2025-01-07 04:02:13,973 - root - INFO - [Train] Epoch 4 [30200/35182]: Training loss 0.33791735768318176, current lr 9.75735624342685e-05, contrastive loss 3.5765795707702637, task loss 0.30215156078338623
2025-01-07 04:03:54,364 - root - INFO - [Train] Epoch 4 [30300/35182]: Training loss 0.35471898317337036, current lr 9.757214121258635e-05, contrastive loss 3.616922616958618, task loss 0.3185497522354126
2025-01-07 04:05:34,947 - root - INFO - [Train] Epoch 4 [30400/35182]: Training loss 0.41565418243408203, current lr 9.757071999090419e-05, contrastive loss 3.5260977745056152, task loss 0.38039320707321167
2025-01-07 04:07:15,327 - root - INFO - [Train] Epoch 4 [30500/35182]: Training loss 0.37664610147476196, current lr 9.756929876922202e-05, contrastive loss 3.6941590309143066, task loss 0.3397045135498047
2025-01-07 04:08:55,913 - root - INFO - [Train] Epoch 4 [30600/35182]: Training loss 0.32723090052604675, current lr 9.756787754753987e-05, contrastive loss 3.640896797180176, task loss 0.29082193970680237
2025-01-07 04:10:36,302 - root - INFO - [Train] Epoch 4 [30700/35182]: Training loss 0.33272475004196167, current lr 9.756645632585771e-05, contrastive loss 3.414767265319824, task loss 0.29857707023620605
2025-01-07 04:12:16,993 - root - INFO - [Train] Epoch 4 [30800/35182]: Training loss 0.38673943281173706, current lr 9.756503510417555e-05, contrastive loss 3.57309889793396, task loss 0.35100844502449036
2025-01-07 04:13:57,378 - root - INFO - [Train] Epoch 4 [30900/35182]: Training loss 0.32034119963645935, current lr 9.75636138824934e-05, contrastive loss 3.6290225982666016, task loss 0.28405097126960754
2025-01-07 04:15:37,967 - root - INFO - [Train] Epoch 4 [31000/35182]: Training loss 0.37864431738853455, current lr 9.756219266081124e-05, contrastive loss 3.65460467338562, task loss 0.34209826588630676
2025-01-07 04:17:18,356 - root - INFO - [Train] Epoch 4 [31100/35182]: Training loss 0.35036182403564453, current lr 9.756077143912908e-05, contrastive loss 3.5057625770568848, task loss 0.3153041899204254
2025-01-07 04:18:59,028 - root - INFO - [Train] Epoch 4 [31200/35182]: Training loss 0.3532595634460449, current lr 9.755935021744693e-05, contrastive loss 3.529341697692871, task loss 0.31796616315841675
2025-01-07 04:20:39,405 - root - INFO - [Train] Epoch 4 [31300/35182]: Training loss 0.3518233299255371, current lr 9.755792899576477e-05, contrastive loss 3.4410500526428223, task loss 0.3174128234386444
2025-01-07 04:22:19,995 - root - INFO - [Train] Epoch 4 [31400/35182]: Training loss 0.40094852447509766, current lr 9.75565077740826e-05, contrastive loss 3.5879998207092285, task loss 0.3650685250759125
2025-01-07 04:24:00,397 - root - INFO - [Train] Epoch 4 [31500/35182]: Training loss 0.3924567997455597, current lr 9.755508655240044e-05, contrastive loss 3.6812453269958496, task loss 0.3556443452835083
2025-01-07 04:25:41,073 - root - INFO - [Train] Epoch 4 [31600/35182]: Training loss 0.3403409719467163, current lr 9.755366533071829e-05, contrastive loss 3.4529974460601807, task loss 0.30581098794937134
2025-01-07 04:27:21,471 - root - INFO - [Train] Epoch 4 [31700/35182]: Training loss 0.37938395142555237, current lr 9.755224410903613e-05, contrastive loss 3.61556339263916, task loss 0.3432283103466034
2025-01-07 04:29:02,056 - root - INFO - [Train] Epoch 4 [31800/35182]: Training loss 0.3463818430900574, current lr 9.755082288735397e-05, contrastive loss 3.5966708660125732, task loss 0.310415118932724
2025-01-07 04:30:42,449 - root - INFO - [Train] Epoch 4 [31900/35182]: Training loss 0.3769724369049072, current lr 9.754940166567182e-05, contrastive loss 3.44989013671875, task loss 0.3424735367298126
2025-01-07 04:32:23,128 - root - INFO - [Train] Epoch 4 [32000/35182]: Training loss 0.348287969827652, current lr 9.754798044398966e-05, contrastive loss 3.684953212738037, task loss 0.3114384412765503
2025-01-07 04:34:03,512 - root - INFO - [Train] Epoch 4 [32100/35182]: Training loss 0.31987708806991577, current lr 9.75465592223075e-05, contrastive loss 3.6341094970703125, task loss 0.28353598713874817
2025-01-07 04:35:44,088 - root - INFO - [Train] Epoch 4 [32200/35182]: Training loss 0.3694871664047241, current lr 9.754513800062535e-05, contrastive loss 3.565610885620117, task loss 0.3338310718536377
2025-01-07 04:37:24,483 - root - INFO - [Train] Epoch 4 [32300/35182]: Training loss 0.3293715715408325, current lr 9.754371677894319e-05, contrastive loss 3.5096912384033203, task loss 0.2942746579647064
2025-01-07 04:39:05,063 - root - INFO - [Train] Epoch 4 [32400/35182]: Training loss 0.37084415555000305, current lr 9.754229555726102e-05, contrastive loss 3.5691606998443604, task loss 0.3351525366306305
2025-01-07 04:40:45,446 - root - INFO - [Train] Epoch 4 [32500/35182]: Training loss 0.3582795560359955, current lr 9.754087433557886e-05, contrastive loss 3.5578341484069824, task loss 0.32270121574401855
2025-01-07 04:42:26,019 - root - INFO - [Train] Epoch 4 [32600/35182]: Training loss 0.3662700653076172, current lr 9.753945311389671e-05, contrastive loss 3.646439552307129, task loss 0.3298056721687317
2025-01-07 04:44:06,403 - root - INFO - [Train] Epoch 4 [32700/35182]: Training loss 0.3184639513492584, current lr 9.753803189221455e-05, contrastive loss 3.4990410804748535, task loss 0.28347355127334595
2025-01-07 04:45:46,993 - root - INFO - [Train] Epoch 4 [32800/35182]: Training loss 0.38384321331977844, current lr 9.75366106705324e-05, contrastive loss 3.658690929412842, task loss 0.34725630283355713
2025-01-07 04:47:27,391 - root - INFO - [Train] Epoch 4 [32900/35182]: Training loss 0.37184810638427734, current lr 9.753518944885024e-05, contrastive loss 3.5661535263061523, task loss 0.33618655800819397
2025-01-07 04:49:07,968 - root - INFO - [Train] Epoch 4 [33000/35182]: Training loss 0.31566041707992554, current lr 9.753376822716808e-05, contrastive loss 3.6103286743164062, task loss 0.27955713868141174
2025-01-07 04:50:48,362 - root - INFO - [Train] Epoch 4 [33100/35182]: Training loss 0.29995784163475037, current lr 9.753234700548592e-05, contrastive loss 3.6744978427886963, task loss 0.2632128596305847
2025-01-07 04:52:29,042 - root - INFO - [Train] Epoch 4 [33200/35182]: Training loss 0.3074270784854889, current lr 9.753092578380377e-05, contrastive loss 3.39559006690979, task loss 0.2734711766242981
2025-01-07 04:54:09,450 - root - INFO - [Train] Epoch 4 [33300/35182]: Training loss 0.29918503761291504, current lr 9.752950456212161e-05, contrastive loss 3.4687986373901367, task loss 0.2644970417022705
2025-01-07 04:55:50,113 - root - INFO - [Train] Epoch 4 [33400/35182]: Training loss 0.30538713932037354, current lr 9.752808334043944e-05, contrastive loss 3.4970359802246094, task loss 0.270416796207428
2025-01-07 04:57:30,500 - root - INFO - [Train] Epoch 4 [33500/35182]: Training loss 0.36289727687835693, current lr 9.752666211875728e-05, contrastive loss 3.616046905517578, task loss 0.32673680782318115
2025-01-07 04:59:11,092 - root - INFO - [Train] Epoch 4 [33600/35182]: Training loss 0.3757975101470947, current lr 9.752524089707513e-05, contrastive loss 3.5731916427612305, task loss 0.340065598487854
2025-01-07 05:00:51,495 - root - INFO - [Train] Epoch 4 [33700/35182]: Training loss 0.4162679612636566, current lr 9.752381967539297e-05, contrastive loss 3.4560351371765137, task loss 0.3817076086997986
2025-01-07 05:02:32,161 - root - INFO - [Train] Epoch 4 [33800/35182]: Training loss 0.33496496081352234, current lr 9.752239845371081e-05, contrastive loss 3.586635112762451, task loss 0.2990986108779907
2025-01-07 05:04:12,547 - root - INFO - [Train] Epoch 4 [33900/35182]: Training loss 0.3211885690689087, current lr 9.752097723202866e-05, contrastive loss 3.6646738052368164, task loss 0.2845418453216553
2025-01-07 05:05:53,138 - root - INFO - [Train] Epoch 4 [34000/35182]: Training loss 0.3090783357620239, current lr 9.75195560103465e-05, contrastive loss 3.678903579711914, task loss 0.27228930592536926
2025-01-07 05:07:33,523 - root - INFO - [Train] Epoch 4 [34100/35182]: Training loss 0.3014841675758362, current lr 9.751813478866434e-05, contrastive loss 3.6045143604278564, task loss 0.2654390335083008
2025-01-07 05:09:14,098 - root - INFO - [Train] Epoch 4 [34200/35182]: Training loss 0.3364199995994568, current lr 9.751671356698219e-05, contrastive loss 3.6315817832946777, task loss 0.30010417103767395
2025-01-07 05:10:54,487 - root - INFO - [Train] Epoch 4 [34300/35182]: Training loss 0.36821675300598145, current lr 9.751529234530003e-05, contrastive loss 3.6375365257263184, task loss 0.331841379404068
2025-01-07 05:12:35,065 - root - INFO - [Train] Epoch 4 [34400/35182]: Training loss 0.36955928802490234, current lr 9.751387112361786e-05, contrastive loss 3.5167672634124756, task loss 0.33439162373542786
2025-01-07 05:14:15,443 - root - INFO - [Train] Epoch 4 [34500/35182]: Training loss 0.39827704429626465, current lr 9.75124499019357e-05, contrastive loss 3.6095938682556152, task loss 0.3621810972690582
2025-01-07 05:15:56,030 - root - INFO - [Train] Epoch 4 [34600/35182]: Training loss 0.32415738701820374, current lr 9.751102868025355e-05, contrastive loss 3.540452718734741, task loss 0.28875285387039185
2025-01-07 05:17:36,426 - root - INFO - [Train] Epoch 4 [34700/35182]: Training loss 0.3256727159023285, current lr 9.750960745857139e-05, contrastive loss 3.31046986579895, task loss 0.29256802797317505
2025-01-07 05:19:17,098 - root - INFO - [Train] Epoch 4 [34800/35182]: Training loss 0.3477421700954437, current lr 9.750818623688923e-05, contrastive loss 3.5683376789093018, task loss 0.31205880641937256
2025-01-07 05:20:57,486 - root - INFO - [Train] Epoch 4 [34900/35182]: Training loss 0.3601231575012207, current lr 9.750676501520708e-05, contrastive loss 3.4320850372314453, task loss 0.3258022964000702
2025-01-07 05:22:38,074 - root - INFO - [Train] Epoch 4 [35000/35182]: Training loss 0.33908048272132874, current lr 9.750534379352492e-05, contrastive loss 3.4499902725219727, task loss 0.30458056926727295
2025-01-07 05:24:18,457 - root - INFO - [Train] Epoch 4 [35100/35182]: Training loss 0.3600318431854248, current lr 9.750392257184276e-05, contrastive loss 3.582942485809326, task loss 0.32420241832733154
2025-01-07 08:09:42,111 - root - INFO - [Valid] Evaluation Results: HR@5:0.4240,NDCG@5:0.2826,HR@10:0.5983,NDCG@10:0.3389,HR@20:0.7654,NDCG@20:0.3813
2025-01-07 08:09:43,011 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-07 08:09:45,944 - root - INFO - [Train] Epoch 4: Overall Training loss 0.0
2025-01-07 08:09:45,945 - root - INFO - [Time] Epoch 4: Time taken 15.29 hours
2025-01-07 08:09:51,237 - root - INFO - [Train] Epoch 5 [0/35182]: Training loss 0.3403443694114685, current lr 9.750275717006339e-05, contrastive loss 3.795313835144043, task loss 0.3023912310600281
2025-01-07 08:11:31,817 - root - INFO - [Train] Epoch 5 [100/35182]: Training loss 0.35272878408432007, current lr 9.750133594838123e-05, contrastive loss 3.71303129196167, task loss 0.3155984580516815
2025-01-07 08:13:12,218 - root - INFO - [Train] Epoch 5 [200/35182]: Training loss 0.2914418876171112, current lr 9.749991472669907e-05, contrastive loss 3.7599148750305176, task loss 0.2538427412509918
2025-01-07 08:14:52,896 - root - INFO - [Train] Epoch 5 [300/35182]: Training loss 0.35157081484794617, current lr 9.749849350501692e-05, contrastive loss 3.7750043869018555, task loss 0.3138207793235779
2025-01-07 08:16:33,288 - root - INFO - [Train] Epoch 5 [400/35182]: Training loss 0.3180123269557953, current lr 9.749707228333476e-05, contrastive loss 3.5210418701171875, task loss 0.28280189633369446
2025-01-07 08:18:13,971 - root - INFO - [Train] Epoch 5 [500/35182]: Training loss 0.3063637614250183, current lr 9.74956510616526e-05, contrastive loss 3.5539326667785645, task loss 0.2708244323730469
2025-01-07 08:19:54,342 - root - INFO - [Train] Epoch 5 [600/35182]: Training loss 0.3624076843261719, current lr 9.749422983997045e-05, contrastive loss 3.672970771789551, task loss 0.3256779611110687
2025-01-07 08:21:34,936 - root - INFO - [Train] Epoch 5 [700/35182]: Training loss 0.34151777625083923, current lr 9.749280861828829e-05, contrastive loss 3.626185894012451, task loss 0.3052559196949005
2025-01-07 08:23:15,327 - root - INFO - [Train] Epoch 5 [800/35182]: Training loss 0.3436705768108368, current lr 9.749138739660613e-05, contrastive loss 3.5289173126220703, task loss 0.30838140845298767
2025-01-07 08:24:55,902 - root - INFO - [Train] Epoch 5 [900/35182]: Training loss 0.3502956032752991, current lr 9.748996617492398e-05, contrastive loss 3.6925430297851562, task loss 0.3133701682090759
2025-01-07 08:26:36,278 - root - INFO - [Train] Epoch 5 [1000/35182]: Training loss 0.33749210834503174, current lr 9.74885449532418e-05, contrastive loss 3.731106758117676, task loss 0.3001810312271118
2025-01-07 08:28:16,868 - root - INFO - [Train] Epoch 5 [1100/35182]: Training loss 0.3100600242614746, current lr 9.748712373155965e-05, contrastive loss 3.6456236839294434, task loss 0.27360379695892334
2025-01-07 08:29:57,250 - root - INFO - [Train] Epoch 5 [1200/35182]: Training loss 0.33563584089279175, current lr 9.748570250987749e-05, contrastive loss 3.577585220336914, task loss 0.29986000061035156
2025-01-07 08:31:37,837 - root - INFO - [Train] Epoch 5 [1300/35182]: Training loss 0.35870063304901123, current lr 9.748428128819534e-05, contrastive loss 3.7042250633239746, task loss 0.3216583728790283
2025-01-07 08:33:18,223 - root - INFO - [Train] Epoch 5 [1400/35182]: Training loss 0.36768534779548645, current lr 9.748286006651318e-05, contrastive loss 3.6142191886901855, task loss 0.3315431475639343
2025-01-07 08:34:58,804 - root - INFO - [Train] Epoch 5 [1500/35182]: Training loss 0.34584924578666687, current lr 9.748143884483102e-05, contrastive loss 3.465877056121826, task loss 0.31119048595428467
2025-01-07 08:36:39,188 - root - INFO - [Train] Epoch 5 [1600/35182]: Training loss 0.30597952008247375, current lr 9.748001762314887e-05, contrastive loss 3.5797009468078613, task loss 0.2701825201511383
2025-01-07 08:38:19,581 - root - INFO - [Train] Epoch 5 [1700/35182]: Training loss 0.35794076323509216, current lr 9.747859640146671e-05, contrastive loss 3.491006374359131, task loss 0.3230307102203369
2025-01-07 08:40:00,163 - root - INFO - [Train] Epoch 5 [1800/35182]: Training loss 0.36322927474975586, current lr 9.747717517978455e-05, contrastive loss 3.627239227294922, task loss 0.3269568681716919
2025-01-07 08:41:40,538 - root - INFO - [Train] Epoch 5 [1900/35182]: Training loss 0.3310629427433014, current lr 9.747575395810238e-05, contrastive loss 3.6802821159362793, task loss 0.2942601144313812
2025-01-07 08:43:21,119 - root - INFO - [Train] Epoch 5 [2000/35182]: Training loss 0.39051204919815063, current lr 9.747433273642023e-05, contrastive loss 3.4568262100219727, task loss 0.35594379901885986
2025-01-07 08:45:01,500 - root - INFO - [Train] Epoch 5 [2100/35182]: Training loss 0.30747801065444946, current lr 9.747291151473807e-05, contrastive loss 3.6942358016967773, task loss 0.2705356478691101
2025-01-07 08:46:42,075 - root - INFO - [Train] Epoch 5 [2200/35182]: Training loss 0.3840681314468384, current lr 9.747149029305591e-05, contrastive loss 3.4695305824279785, task loss 0.34937283396720886
2025-01-07 08:48:22,471 - root - INFO - [Train] Epoch 5 [2300/35182]: Training loss 0.34328511357307434, current lr 9.747006907137376e-05, contrastive loss 3.6225881576538086, task loss 0.30705922842025757
2025-01-07 08:50:03,047 - root - INFO - [Train] Epoch 5 [2400/35182]: Training loss 0.30092138051986694, current lr 9.74686478496916e-05, contrastive loss 3.5181899070739746, task loss 0.26573947072029114
2025-01-07 08:51:43,425 - root - INFO - [Train] Epoch 5 [2500/35182]: Training loss 0.38035333156585693, current lr 9.746722662800944e-05, contrastive loss 3.602241039276123, task loss 0.34433090686798096
2025-01-07 08:53:24,002 - root - INFO - [Train] Epoch 5 [2600/35182]: Training loss 0.28669750690460205, current lr 9.746580540632729e-05, contrastive loss 3.6130383014678955, task loss 0.25056713819503784
2025-01-07 08:55:04,387 - root - INFO - [Train] Epoch 5 [2700/35182]: Training loss 0.3701947331428528, current lr 9.746438418464513e-05, contrastive loss 3.475029468536377, task loss 0.33544445037841797
2025-01-07 08:56:44,975 - root - INFO - [Train] Epoch 5 [2800/35182]: Training loss 0.36568909883499146, current lr 9.746296296296297e-05, contrastive loss 3.348428249359131, task loss 0.33220481872558594
2025-01-07 08:58:25,354 - root - INFO - [Train] Epoch 5 [2900/35182]: Training loss 0.3348873257637024, current lr 9.746154174128082e-05, contrastive loss 3.5667436122894287, task loss 0.29921990633010864
2025-01-07 09:00:05,938 - root - INFO - [Train] Epoch 5 [3000/35182]: Training loss 0.3697201907634735, current lr 9.746012051959865e-05, contrastive loss 3.6119072437286377, task loss 0.33360111713409424
2025-01-07 09:01:46,317 - root - INFO - [Train] Epoch 5 [3100/35182]: Training loss 0.31950706243515015, current lr 9.745869929791649e-05, contrastive loss 3.6542820930480957, task loss 0.28296422958374023
2025-01-07 09:03:26,900 - root - INFO - [Train] Epoch 5 [3200/35182]: Training loss 0.2972068190574646, current lr 9.745727807623433e-05, contrastive loss 3.421113967895508, task loss 0.2629956901073456
2025-01-07 09:05:07,296 - root - INFO - [Train] Epoch 5 [3300/35182]: Training loss 0.3691113591194153, current lr 9.745585685455218e-05, contrastive loss 3.5236496925354004, task loss 0.3338748514652252
2025-01-07 09:06:47,879 - root - INFO - [Train] Epoch 5 [3400/35182]: Training loss 0.37384817004203796, current lr 9.745443563287002e-05, contrastive loss 3.6869521141052246, task loss 0.33697864413261414
2025-01-07 09:08:28,272 - root - INFO - [Train] Epoch 5 [3500/35182]: Training loss 0.2886846661567688, current lr 9.745301441118786e-05, contrastive loss 3.5608816146850586, task loss 0.25307583808898926
2025-01-07 09:10:08,844 - root - INFO - [Train] Epoch 5 [3600/35182]: Training loss 0.2825457751750946, current lr 9.745159318950571e-05, contrastive loss 3.6732161045074463, task loss 0.24581362307071686
2025-01-07 09:11:49,244 - root - INFO - [Train] Epoch 5 [3700/35182]: Training loss 0.2604784071445465, current lr 9.745017196782355e-05, contrastive loss 3.748203754425049, task loss 0.22299636900424957
2025-01-07 09:13:29,915 - root - INFO - [Train] Epoch 5 [3800/35182]: Training loss 0.3064512014389038, current lr 9.74487507461414e-05, contrastive loss 3.543872594833374, task loss 0.27101248502731323
2025-01-07 09:15:10,288 - root - INFO - [Train] Epoch 5 [3900/35182]: Training loss 0.31405845284461975, current lr 9.744732952445922e-05, contrastive loss 3.682276725769043, task loss 0.2772356867790222
2025-01-07 09:16:50,878 - root - INFO - [Train] Epoch 5 [4000/35182]: Training loss 0.36498209834098816, current lr 9.744590830277707e-05, contrastive loss 3.585094451904297, task loss 0.329131156206131
2025-01-07 09:18:31,265 - root - INFO - [Train] Epoch 5 [4100/35182]: Training loss 0.33058786392211914, current lr 9.744448708109491e-05, contrastive loss 3.5366995334625244, task loss 0.29522088170051575
2025-01-07 09:20:11,853 - root - INFO - [Train] Epoch 5 [4200/35182]: Training loss 0.35637223720550537, current lr 9.744306585941275e-05, contrastive loss 3.641219139099121, task loss 0.3199600577354431
2025-01-07 09:21:52,224 - root - INFO - [Train] Epoch 5 [4300/35182]: Training loss 0.307134747505188, current lr 9.74416446377306e-05, contrastive loss 3.6973304748535156, task loss 0.2701614499092102
2025-01-07 09:23:32,819 - root - INFO - [Train] Epoch 5 [4400/35182]: Training loss 0.3175402581691742, current lr 9.744022341604844e-05, contrastive loss 3.455893039703369, task loss 0.28298133611679077
2025-01-07 09:25:13,204 - root - INFO - [Train] Epoch 5 [4500/35182]: Training loss 0.2885739207267761, current lr 9.743880219436628e-05, contrastive loss 3.594024658203125, task loss 0.252633661031723
2025-01-07 09:26:53,576 - root - INFO - [Train] Epoch 5 [4600/35182]: Training loss 0.28969764709472656, current lr 9.743738097268413e-05, contrastive loss 3.7182765007019043, task loss 0.25251486897468567
2025-01-07 09:28:34,176 - root - INFO - [Train] Epoch 5 [4700/35182]: Training loss 0.3241576552391052, current lr 9.743595975100197e-05, contrastive loss 3.518118143081665, task loss 0.2889764904975891
2025-01-07 09:30:14,565 - root - INFO - [Train] Epoch 5 [4800/35182]: Training loss 0.3185083866119385, current lr 9.743453852931981e-05, contrastive loss 3.6382837295532227, task loss 0.2821255624294281
2025-01-07 09:31:55,153 - root - INFO - [Train] Epoch 5 [4900/35182]: Training loss 0.3056788444519043, current lr 9.743311730763764e-05, contrastive loss 3.6347265243530273, task loss 0.26933157444000244
2025-01-07 09:33:35,829 - root - INFO - [Train] Epoch 5 [5000/35182]: Training loss 0.2656051516532898, current lr 9.743169608595549e-05, contrastive loss 3.543400526046753, task loss 0.23017114400863647
2025-01-07 09:35:16,216 - root - INFO - [Train] Epoch 5 [5100/35182]: Training loss 0.3287789523601532, current lr 9.743027486427333e-05, contrastive loss 3.510561466217041, task loss 0.2936733365058899
2025-01-07 09:36:56,601 - root - INFO - [Train] Epoch 5 [5200/35182]: Training loss 0.23968538641929626, current lr 9.742885364259117e-05, contrastive loss 3.562807559967041, task loss 0.20405730605125427
2025-01-07 09:38:37,170 - root - INFO - [Train] Epoch 5 [5300/35182]: Training loss 0.28537148237228394, current lr 9.742743242090902e-05, contrastive loss 3.5697579383850098, task loss 0.24967391788959503
2025-01-07 09:40:17,545 - root - INFO - [Train] Epoch 5 [5400/35182]: Training loss 0.25459858775138855, current lr 9.742601119922686e-05, contrastive loss 3.749238967895508, task loss 0.21710620820522308
2025-01-07 09:41:58,138 - root - INFO - [Train] Epoch 5 [5500/35182]: Training loss 0.31124719977378845, current lr 9.74245899775447e-05, contrastive loss 3.6112518310546875, task loss 0.2751346826553345
2025-01-07 09:43:38,527 - root - INFO - [Train] Epoch 5 [5600/35182]: Training loss 0.3876197338104248, current lr 9.742316875586255e-05, contrastive loss 3.5828514099121094, task loss 0.35179123282432556
2025-01-07 09:45:19,113 - root - INFO - [Train] Epoch 5 [5700/35182]: Training loss 0.282707154750824, current lr 9.742174753418039e-05, contrastive loss 3.4874353408813477, task loss 0.247832790017128
2025-01-07 09:46:59,491 - root - INFO - [Train] Epoch 5 [5800/35182]: Training loss 0.2996223568916321, current lr 9.742032631249823e-05, contrastive loss 3.5523500442504883, task loss 0.2640988528728485
2025-01-07 09:48:40,070 - root - INFO - [Train] Epoch 5 [5900/35182]: Training loss 0.3168761432170868, current lr 9.741890509081606e-05, contrastive loss 3.6331543922424316, task loss 0.28054460883140564
2025-01-07 09:50:20,447 - root - INFO - [Train] Epoch 5 [6000/35182]: Training loss 0.386116623878479, current lr 9.741748386913391e-05, contrastive loss 3.4975945949554443, task loss 0.35114067792892456
2025-01-07 09:52:01,035 - root - INFO - [Train] Epoch 5 [6100/35182]: Training loss 0.2688048481941223, current lr 9.741606264745175e-05, contrastive loss 3.6332125663757324, task loss 0.2324727177619934
2025-01-07 09:53:41,410 - root - INFO - [Train] Epoch 5 [6200/35182]: Training loss 0.3171292543411255, current lr 9.74146414257696e-05, contrastive loss 3.785604476928711, task loss 0.27927321195602417
2025-01-07 09:55:21,998 - root - INFO - [Train] Epoch 5 [6300/35182]: Training loss 0.38024941086769104, current lr 9.741322020408744e-05, contrastive loss 3.6858296394348145, task loss 0.3433911204338074
2025-01-07 09:57:02,375 - root - INFO - [Train] Epoch 5 [6400/35182]: Training loss 0.3235419690608978, current lr 9.741179898240528e-05, contrastive loss 3.5581579208374023, task loss 0.28796038031578064
2025-01-07 09:58:42,966 - root - INFO - [Train] Epoch 5 [6500/35182]: Training loss 0.29056859016418457, current lr 9.741037776072312e-05, contrastive loss 3.610609531402588, task loss 0.25446248054504395
2025-01-07 10:00:23,338 - root - INFO - [Train] Epoch 5 [6600/35182]: Training loss 0.3226892948150635, current lr 9.740895653904097e-05, contrastive loss 3.392686367034912, task loss 0.2887624204158783
2025-01-07 10:02:03,937 - root - INFO - [Train] Epoch 5 [6700/35182]: Training loss 0.2775438129901886, current lr 9.740753531735881e-05, contrastive loss 3.5667948722839355, task loss 0.24187587201595306
2025-01-07 10:03:44,326 - root - INFO - [Train] Epoch 5 [6800/35182]: Training loss 0.3427365720272064, current lr 9.740611409567665e-05, contrastive loss 3.6391091346740723, task loss 0.30634549260139465
2025-01-07 10:05:24,897 - root - INFO - [Train] Epoch 5 [6900/35182]: Training loss 0.3483380079269409, current lr 9.740469287399448e-05, contrastive loss 3.634977340698242, task loss 0.3119882345199585
2025-01-07 10:07:05,282 - root - INFO - [Train] Epoch 5 [7000/35182]: Training loss 0.3438892364501953, current lr 9.740327165231233e-05, contrastive loss 3.442413330078125, task loss 0.30946511030197144
2025-01-07 10:08:45,869 - root - INFO - [Train] Epoch 5 [7100/35182]: Training loss 0.25558024644851685, current lr 9.740185043063017e-05, contrastive loss 3.5284249782562256, task loss 0.2202959954738617
2025-01-07 10:10:26,253 - root - INFO - [Train] Epoch 5 [7200/35182]: Training loss 0.34843704104423523, current lr 9.740042920894801e-05, contrastive loss 3.5354995727539062, task loss 0.3130820393562317
2025-01-07 10:12:06,834 - root - INFO - [Train] Epoch 5 [7300/35182]: Training loss 0.3353656828403473, current lr 9.739900798726586e-05, contrastive loss 3.351034641265869, task loss 0.30185532569885254
2025-01-07 10:13:47,205 - root - INFO - [Train] Epoch 5 [7400/35182]: Training loss 0.35865089297294617, current lr 9.73975867655837e-05, contrastive loss 3.854426383972168, task loss 0.3201066255569458
2025-01-07 10:15:27,796 - root - INFO - [Train] Epoch 5 [7500/35182]: Training loss 0.3431220054626465, current lr 9.739616554390154e-05, contrastive loss 3.700021266937256, task loss 0.3061217963695526
2025-01-07 10:17:08,179 - root - INFO - [Train] Epoch 5 [7600/35182]: Training loss 0.3050990402698517, current lr 9.739474432221939e-05, contrastive loss 3.477810859680176, task loss 0.27032092213630676
2025-01-07 10:18:48,567 - root - INFO - [Train] Epoch 5 [7700/35182]: Training loss 0.28233057260513306, current lr 9.739332310053723e-05, contrastive loss 3.645134687423706, task loss 0.24587923288345337
2025-01-07 10:20:29,141 - root - INFO - [Train] Epoch 5 [7800/35182]: Training loss 0.2810046672821045, current lr 9.739190187885506e-05, contrastive loss 3.420224189758301, task loss 0.246802419424057
2025-01-07 10:22:09,523 - root - INFO - [Train] Epoch 5 [7900/35182]: Training loss 0.28992196917533875, current lr 9.73904806571729e-05, contrastive loss 3.6766746044158936, task loss 0.2531552314758301
2025-01-07 10:23:50,120 - root - INFO - [Train] Epoch 5 [8000/35182]: Training loss 0.30829885601997375, current lr 9.738905943549075e-05, contrastive loss 3.4647483825683594, task loss 0.2736513614654541
2025-01-07 10:25:30,500 - root - INFO - [Train] Epoch 5 [8100/35182]: Training loss 0.3275291323661804, current lr 9.738763821380859e-05, contrastive loss 3.5750269889831543, task loss 0.2917788624763489
2025-01-07 10:27:11,074 - root - INFO - [Train] Epoch 5 [8200/35182]: Training loss 0.26176217198371887, current lr 9.738621699212643e-05, contrastive loss 3.7509207725524902, task loss 0.22425296902656555
2025-01-07 10:28:51,456 - root - INFO - [Train] Epoch 5 [8300/35182]: Training loss 0.2943692207336426, current lr 9.738479577044428e-05, contrastive loss 3.62814998626709, task loss 0.25808772444725037
2025-01-07 10:30:32,048 - root - INFO - [Train] Epoch 5 [8400/35182]: Training loss 0.2733730375766754, current lr 9.738337454876212e-05, contrastive loss 3.761105537414551, task loss 0.23576197028160095
2025-01-07 10:32:12,425 - root - INFO - [Train] Epoch 5 [8500/35182]: Training loss 0.32411354780197144, current lr 9.738195332707997e-05, contrastive loss 3.6322243213653564, task loss 0.28779131174087524
2025-01-07 10:33:53,005 - root - INFO - [Train] Epoch 5 [8600/35182]: Training loss 0.30292659997940063, current lr 9.738053210539781e-05, contrastive loss 3.642125129699707, task loss 0.2665053606033325
2025-01-07 10:35:33,389 - root - INFO - [Train] Epoch 5 [8700/35182]: Training loss 0.3138200044631958, current lr 9.737911088371565e-05, contrastive loss 3.4510374069213867, task loss 0.27930963039398193
2025-01-07 10:37:13,972 - root - INFO - [Train] Epoch 5 [8800/35182]: Training loss 0.2626342475414276, current lr 9.73776896620335e-05, contrastive loss 3.6672749519348145, task loss 0.22596149146556854
2025-01-07 10:38:54,343 - root - INFO - [Train] Epoch 5 [8900/35182]: Training loss 0.25904715061187744, current lr 9.737626844035133e-05, contrastive loss 3.5931365489959717, task loss 0.22311578691005707
2025-01-07 10:40:34,940 - root - INFO - [Train] Epoch 5 [9000/35182]: Training loss 0.34049513936042786, current lr 9.737484721866917e-05, contrastive loss 3.5834851264953613, task loss 0.30466029047966003
2025-01-07 10:42:15,329 - root - INFO - [Train] Epoch 5 [9100/35182]: Training loss 0.24808037281036377, current lr 9.737342599698701e-05, contrastive loss 3.5133700370788574, task loss 0.21294666826725006
2025-01-07 10:43:55,905 - root - INFO - [Train] Epoch 5 [9200/35182]: Training loss 0.3108765780925751, current lr 9.737200477530486e-05, contrastive loss 3.6287965774536133, task loss 0.27458861470222473
2025-01-07 10:45:36,290 - root - INFO - [Train] Epoch 5 [9300/35182]: Training loss 0.27890723943710327, current lr 9.73705835536227e-05, contrastive loss 3.7043871879577637, task loss 0.24186336994171143
2025-01-07 10:47:16,871 - root - INFO - [Train] Epoch 5 [9400/35182]: Training loss 0.22841432690620422, current lr 9.736916233194054e-05, contrastive loss 3.676953077316284, task loss 0.1916448026895523
2025-01-07 10:48:57,250 - root - INFO - [Train] Epoch 5 [9500/35182]: Training loss 0.2399320751428604, current lr 9.736774111025839e-05, contrastive loss 3.6649162769317627, task loss 0.2032829076051712
2025-01-07 10:50:37,844 - root - INFO - [Train] Epoch 5 [9600/35182]: Training loss 0.26406046748161316, current lr 9.736631988857623e-05, contrastive loss 3.5719282627105713, task loss 0.22834117710590363
2025-01-07 10:52:18,223 - root - INFO - [Train] Epoch 5 [9700/35182]: Training loss 0.26149922609329224, current lr 9.736489866689407e-05, contrastive loss 3.6399269104003906, task loss 0.22509995102882385
2025-01-07 10:53:58,805 - root - INFO - [Train] Epoch 5 [9800/35182]: Training loss 0.30186647176742554, current lr 9.73634774452119e-05, contrastive loss 3.7188053131103516, task loss 0.264678418636322
2025-01-07 10:55:39,197 - root - INFO - [Train] Epoch 5 [9900/35182]: Training loss 0.2891766428947449, current lr 9.736205622352975e-05, contrastive loss 3.474663734436035, task loss 0.25442999601364136
2025-01-07 10:57:19,580 - root - INFO - [Train] Epoch 5 [10000/35182]: Training loss 0.2777169346809387, current lr 9.736063500184759e-05, contrastive loss 3.6893129348754883, task loss 0.24082379043102264
2025-01-07 10:59:00,149 - root - INFO - [Train] Epoch 5 [10100/35182]: Training loss 0.31504955887794495, current lr 9.735921378016543e-05, contrastive loss 3.6363444328308105, task loss 0.2786861062049866
2025-01-07 11:00:40,520 - root - INFO - [Train] Epoch 5 [10200/35182]: Training loss 0.3126924932003021, current lr 9.735779255848328e-05, contrastive loss 3.6721010208129883, task loss 0.2759714722633362
2025-01-07 11:02:21,107 - root - INFO - [Train] Epoch 5 [10300/35182]: Training loss 0.2735789716243744, current lr 9.735637133680112e-05, contrastive loss 3.450666666030884, task loss 0.2390723079442978
2025-01-07 11:04:01,499 - root - INFO - [Train] Epoch 5 [10400/35182]: Training loss 0.3380368649959564, current lr 9.735495011511896e-05, contrastive loss 3.647857666015625, task loss 0.3015582859516144
2025-01-07 11:05:42,086 - root - INFO - [Train] Epoch 5 [10500/35182]: Training loss 0.23363327980041504, current lr 9.73535288934368e-05, contrastive loss 3.7409863471984863, task loss 0.1962234079837799
2025-01-07 11:07:22,462 - root - INFO - [Train] Epoch 5 [10600/35182]: Training loss 0.2896559536457062, current lr 9.735210767175465e-05, contrastive loss 3.6999499797821045, task loss 0.2526564598083496
2025-01-07 11:09:03,046 - root - INFO - [Train] Epoch 5 [10700/35182]: Training loss 0.30967238545417786, current lr 9.735068645007249e-05, contrastive loss 3.5425188541412354, task loss 0.2742471992969513
2025-01-07 11:10:43,423 - root - INFO - [Train] Epoch 5 [10800/35182]: Training loss 0.3191528916358948, current lr 9.734926522839034e-05, contrastive loss 3.563671588897705, task loss 0.28351616859436035
2025-01-07 11:12:24,011 - root - INFO - [Train] Epoch 5 [10900/35182]: Training loss 0.3529678285121918, current lr 9.734784400670817e-05, contrastive loss 3.5770106315612793, task loss 0.31719771027565
2025-01-07 11:14:04,396 - root - INFO - [Train] Epoch 5 [11000/35182]: Training loss 0.3037605881690979, current lr 9.734642278502601e-05, contrastive loss 3.7748920917510986, task loss 0.26601165533065796
2025-01-07 11:15:44,984 - root - INFO - [Train] Epoch 5 [11100/35182]: Training loss 0.28264719247817993, current lr 9.734500156334385e-05, contrastive loss 3.7421116828918457, task loss 0.24522608518600464
2025-01-07 11:17:25,379 - root - INFO - [Train] Epoch 5 [11200/35182]: Training loss 0.26270154118537903, current lr 9.73435803416617e-05, contrastive loss 3.384615182876587, task loss 0.22885540127754211
2025-01-07 11:19:06,073 - root - INFO - [Train] Epoch 5 [11300/35182]: Training loss 0.2518590986728668, current lr 9.734215911997954e-05, contrastive loss 3.7807016372680664, task loss 0.21405208110809326
2025-01-07 11:20:46,479 - root - INFO - [Train] Epoch 5 [11400/35182]: Training loss 0.25191783905029297, current lr 9.734073789829738e-05, contrastive loss 3.5066003799438477, task loss 0.2168518304824829
2025-01-07 11:22:27,152 - root - INFO - [Train] Epoch 5 [11500/35182]: Training loss 0.29210904240608215, current lr 9.733931667661523e-05, contrastive loss 3.8614003658294678, task loss 0.2534950375556946
2025-01-07 11:24:07,541 - root - INFO - [Train] Epoch 5 [11600/35182]: Training loss 0.32495272159576416, current lr 9.733789545493307e-05, contrastive loss 3.7143430709838867, task loss 0.287809282541275
2025-01-07 11:25:48,128 - root - INFO - [Train] Epoch 5 [11700/35182]: Training loss 0.2975963056087494, current lr 9.733647423325091e-05, contrastive loss 3.672271728515625, task loss 0.26087358593940735
2025-01-07 11:27:28,801 - root - INFO - [Train] Epoch 5 [11800/35182]: Training loss 0.2886687219142914, current lr 9.733505301156874e-05, contrastive loss 3.6373138427734375, task loss 0.252295583486557
2025-01-07 11:29:09,192 - root - INFO - [Train] Epoch 5 [11900/35182]: Training loss 0.28925246000289917, current lr 9.733363178988659e-05, contrastive loss 3.7111239433288574, task loss 0.25214120745658875
2025-01-07 11:30:49,868 - root - INFO - [Train] Epoch 5 [12000/35182]: Training loss 0.3003731667995453, current lr 9.733221056820443e-05, contrastive loss 3.646820068359375, task loss 0.26390495896339417
2025-01-07 11:32:30,260 - root - INFO - [Train] Epoch 5 [12100/35182]: Training loss 0.2722240686416626, current lr 9.733078934652227e-05, contrastive loss 3.558239459991455, task loss 0.23664167523384094
2025-01-07 11:34:10,851 - root - INFO - [Train] Epoch 5 [12200/35182]: Training loss 0.2547231912612915, current lr 9.732936812484012e-05, contrastive loss 3.582070827484131, task loss 0.21890246868133545
2025-01-07 11:35:51,256 - root - INFO - [Train] Epoch 5 [12300/35182]: Training loss 0.33462393283843994, current lr 9.732794690315796e-05, contrastive loss 3.55623197555542, task loss 0.2990616261959076
2025-01-07 11:37:31,932 - root - INFO - [Train] Epoch 5 [12400/35182]: Training loss 0.2565273642539978, current lr 9.73265256814758e-05, contrastive loss 3.7689285278320312, task loss 0.21883809566497803
2025-01-07 11:39:12,315 - root - INFO - [Train] Epoch 5 [12500/35182]: Training loss 0.2922435402870178, current lr 9.732510445979365e-05, contrastive loss 3.4621620178222656, task loss 0.2576219141483307
2025-01-07 11:40:52,898 - root - INFO - [Train] Epoch 5 [12600/35182]: Training loss 0.2553348243236542, current lr 9.732368323811149e-05, contrastive loss 3.568380355834961, task loss 0.2196510136127472
2025-01-07 11:42:33,287 - root - INFO - [Train] Epoch 5 [12700/35182]: Training loss 0.25470098853111267, current lr 9.732226201642932e-05, contrastive loss 3.5127806663513184, task loss 0.21957316994667053
2025-01-07 11:44:13,867 - root - INFO - [Train] Epoch 5 [12800/35182]: Training loss 0.38538435101509094, current lr 9.732084079474718e-05, contrastive loss 3.76497220993042, task loss 0.34773463010787964
2025-01-07 11:45:54,261 - root - INFO - [Train] Epoch 5 [12900/35182]: Training loss 0.27274179458618164, current lr 9.7319419573065e-05, contrastive loss 3.4057207107543945, task loss 0.23868459463119507
2025-01-07 11:47:34,828 - root - INFO - [Train] Epoch 5 [13000/35182]: Training loss 0.2836999297142029, current lr 9.731799835138285e-05, contrastive loss 3.6347241401672363, task loss 0.24735267460346222
2025-01-07 11:49:15,218 - root - INFO - [Train] Epoch 5 [13100/35182]: Training loss 0.29954537749290466, current lr 9.73165771297007e-05, contrastive loss 3.5933492183685303, task loss 0.26361188292503357
2025-01-07 11:50:55,595 - root - INFO - [Train] Epoch 5 [13200/35182]: Training loss 0.2891599237918854, current lr 9.731515590801854e-05, contrastive loss 3.5954742431640625, task loss 0.25320518016815186
2025-01-07 11:52:36,169 - root - INFO - [Train] Epoch 5 [13300/35182]: Training loss 0.3144129514694214, current lr 9.731373468633638e-05, contrastive loss 3.6032958030700684, task loss 0.27838000655174255
2025-01-07 11:54:16,548 - root - INFO - [Train] Epoch 5 [13400/35182]: Training loss 0.28270381689071655, current lr 9.731231346465422e-05, contrastive loss 3.7584376335144043, task loss 0.24511943757534027
2025-01-07 11:55:57,142 - root - INFO - [Train] Epoch 5 [13500/35182]: Training loss 0.24361683428287506, current lr 9.731089224297207e-05, contrastive loss 3.5566582679748535, task loss 0.20805025100708008
2025-01-07 11:57:37,522 - root - INFO - [Train] Epoch 5 [13600/35182]: Training loss 0.3096977174282074, current lr 9.730947102128991e-05, contrastive loss 3.669940948486328, task loss 0.27299830317497253
2025-01-07 11:59:18,115 - root - INFO - [Train] Epoch 5 [13700/35182]: Training loss 0.23746247589588165, current lr 9.730804979960775e-05, contrastive loss 3.7815170288085938, task loss 0.19964730739593506
2025-01-07 12:00:58,802 - root - INFO - [Train] Epoch 5 [13800/35182]: Training loss 0.21405749022960663, current lr 9.730662857792558e-05, contrastive loss 3.6496763229370117, task loss 0.1775607317686081
2025-01-07 12:02:39,193 - root - INFO - [Train] Epoch 5 [13900/35182]: Training loss 0.2681862711906433, current lr 9.730520735624343e-05, contrastive loss 3.5415234565734863, task loss 0.23277105391025543
2025-01-07 12:04:19,576 - root - INFO - [Train] Epoch 5 [14000/35182]: Training loss 0.3067888617515564, current lr 9.730378613456127e-05, contrastive loss 3.636676788330078, task loss 0.270422101020813
2025-01-07 12:06:00,151 - root - INFO - [Train] Epoch 5 [14100/35182]: Training loss 0.29931724071502686, current lr 9.730236491287911e-05, contrastive loss 3.674241542816162, task loss 0.26257482171058655
2025-01-07 12:07:40,540 - root - INFO - [Train] Epoch 5 [14200/35182]: Training loss 0.29940861463546753, current lr 9.730094369119696e-05, contrastive loss 3.631828784942627, task loss 0.2630903422832489
2025-01-07 12:09:21,129 - root - INFO - [Train] Epoch 5 [14300/35182]: Training loss 0.2301483303308487, current lr 9.72995224695148e-05, contrastive loss 3.8864479064941406, task loss 0.19128385186195374
2025-01-07 12:11:01,799 - root - INFO - [Train] Epoch 5 [14400/35182]: Training loss 0.3093508780002594, current lr 9.729810124783264e-05, contrastive loss 3.674549102783203, task loss 0.27260538935661316
2025-01-07 12:12:42,185 - root - INFO - [Train] Epoch 5 [14500/35182]: Training loss 0.28002387285232544, current lr 9.729668002615049e-05, contrastive loss 3.853489875793457, task loss 0.241488978266716
2025-01-07 12:14:22,565 - root - INFO - [Train] Epoch 5 [14600/35182]: Training loss 0.2795522212982178, current lr 9.729525880446833e-05, contrastive loss 3.6675496101379395, task loss 0.24287673830986023
2025-01-07 12:16:03,167 - root - INFO - [Train] Epoch 5 [14700/35182]: Training loss 0.20121337473392487, current lr 9.729383758278616e-05, contrastive loss 3.7376492023468018, task loss 0.16383688151836395
2025-01-07 12:17:43,858 - root - INFO - [Train] Epoch 5 [14800/35182]: Training loss 0.25904807448387146, current lr 9.729241636110402e-05, contrastive loss 3.6349453926086426, task loss 0.2226986140012741
2025-01-07 12:19:24,251 - root - INFO - [Train] Epoch 5 [14900/35182]: Training loss 0.31972965598106384, current lr 9.729099513942185e-05, contrastive loss 3.8784141540527344, task loss 0.2809455096721649
2025-01-07 12:21:04,829 - root - INFO - [Train] Epoch 5 [15000/35182]: Training loss 0.24710671603679657, current lr 9.728957391773969e-05, contrastive loss 3.725428581237793, task loss 0.2098524272441864
2025-01-07 12:22:45,211 - root - INFO - [Train] Epoch 5 [15100/35182]: Training loss 0.29550325870513916, current lr 9.728815269605753e-05, contrastive loss 3.7410333156585693, task loss 0.2580929398536682
2025-01-07 12:24:25,594 - root - INFO - [Train] Epoch 5 [15200/35182]: Training loss 0.22619232535362244, current lr 9.728673147437538e-05, contrastive loss 3.5845072269439697, task loss 0.19034725427627563
2025-01-07 12:26:06,186 - root - INFO - [Train] Epoch 5 [15300/35182]: Training loss 0.2849958539009094, current lr 9.728531025269322e-05, contrastive loss 3.5770630836486816, task loss 0.2492252141237259
2025-01-07 12:27:46,574 - root - INFO - [Train] Epoch 5 [15400/35182]: Training loss 0.28340718150138855, current lr 9.728388903101106e-05, contrastive loss 3.6830549240112305, task loss 0.24657663702964783
2025-01-07 12:29:27,170 - root - INFO - [Train] Epoch 5 [15500/35182]: Training loss 0.2544921636581421, current lr 9.728246780932891e-05, contrastive loss 3.828458786010742, task loss 0.2162075638771057
2025-01-07 12:31:07,839 - root - INFO - [Train] Epoch 5 [15600/35182]: Training loss 0.26108595728874207, current lr 9.728104658764674e-05, contrastive loss 3.735361099243164, task loss 0.2237323373556137
2025-01-07 12:32:48,259 - root - INFO - [Train] Epoch 5 [15700/35182]: Training loss 0.2662385106086731, current lr 9.72796253659646e-05, contrastive loss 3.7371296882629395, task loss 0.22886720299720764
2025-01-07 12:34:28,918 - root - INFO - [Train] Epoch 5 [15800/35182]: Training loss 0.25014927983283997, current lr 9.727820414428242e-05, contrastive loss 3.729588508605957, task loss 0.21285338699817657
2025-01-07 12:36:09,301 - root - INFO - [Train] Epoch 5 [15900/35182]: Training loss 0.27406996488571167, current lr 9.727678292260027e-05, contrastive loss 3.7287237644195557, task loss 0.2367827296257019
2025-01-07 12:37:49,882 - root - INFO - [Train] Epoch 5 [16000/35182]: Training loss 0.20719604194164276, current lr 9.727536170091811e-05, contrastive loss 3.7614612579345703, task loss 0.16958142817020416
2025-01-07 12:39:30,277 - root - INFO - [Train] Epoch 5 [16100/35182]: Training loss 0.24854333698749542, current lr 9.727394047923595e-05, contrastive loss 3.609898567199707, task loss 0.21244435012340546
2025-01-07 12:41:10,955 - root - INFO - [Train] Epoch 5 [16200/35182]: Training loss 0.25607365369796753, current lr 9.72725192575538e-05, contrastive loss 3.715196371078491, task loss 0.21892167627811432
2025-01-07 12:42:51,337 - root - INFO - [Train] Epoch 5 [16300/35182]: Training loss 0.2920509874820709, current lr 9.727109803587164e-05, contrastive loss 3.670180320739746, task loss 0.25534918904304504
2025-01-07 12:44:31,939 - root - INFO - [Train] Epoch 5 [16400/35182]: Training loss 0.19074749946594238, current lr 9.726967681418948e-05, contrastive loss 3.785801887512207, task loss 0.15288947522640228
2025-01-07 12:46:12,345 - root - INFO - [Train] Epoch 5 [16500/35182]: Training loss 0.21855245530605316, current lr 9.726825559250733e-05, contrastive loss 3.6837141513824463, task loss 0.18171530961990356
2025-01-07 12:47:53,006 - root - INFO - [Train] Epoch 5 [16600/35182]: Training loss 0.22664222121238708, current lr 9.726683437082517e-05, contrastive loss 3.522854804992676, task loss 0.19141367077827454
2025-01-07 12:49:33,417 - root - INFO - [Train] Epoch 5 [16700/35182]: Training loss 0.2559036612510681, current lr 9.7265413149143e-05, contrastive loss 3.7274465560913086, task loss 0.21862921118736267
2025-01-07 12:51:14,067 - root - INFO - [Train] Epoch 5 [16800/35182]: Training loss 0.22452393174171448, current lr 9.726399192746086e-05, contrastive loss 3.52891206741333, task loss 0.18923480808734894
2025-01-07 12:52:54,459 - root - INFO - [Train] Epoch 5 [16900/35182]: Training loss 0.20463232696056366, current lr 9.726257070577869e-05, contrastive loss 3.5441694259643555, task loss 0.16919063031673431
2025-01-07 12:54:35,069 - root - INFO - [Train] Epoch 5 [17000/35182]: Training loss 0.2151036262512207, current lr 9.726114948409653e-05, contrastive loss 3.4929580688476562, task loss 0.1801740527153015
2025-01-07 12:56:15,459 - root - INFO - [Train] Epoch 5 [17100/35182]: Training loss 0.296039342880249, current lr 9.725972826241437e-05, contrastive loss 3.7052090167999268, task loss 0.2589872479438782
2025-01-07 12:57:56,131 - root - INFO - [Train] Epoch 5 [17200/35182]: Training loss 0.21257179975509644, current lr 9.725830704073222e-05, contrastive loss 3.5775718688964844, task loss 0.1767960786819458
2025-01-07 12:59:36,513 - root - INFO - [Train] Epoch 5 [17300/35182]: Training loss 0.2484336793422699, current lr 9.725688581905006e-05, contrastive loss 3.8218135833740234, task loss 0.21021555364131927
2025-01-07 13:01:17,120 - root - INFO - [Train] Epoch 5 [17400/35182]: Training loss 0.24053741991519928, current lr 9.72554645973679e-05, contrastive loss 3.6677627563476562, task loss 0.20385979115962982
2025-01-07 13:02:57,798 - root - INFO - [Train] Epoch 5 [17500/35182]: Training loss 0.2541200518608093, current lr 9.725404337568575e-05, contrastive loss 3.7659711837768555, task loss 0.21646034717559814
2025-01-07 15:48:23,338 - root - INFO - [Valid] Evaluation Results: HR@5:0.7315,NDCG@5:0.5915,HR@10:0.8281,NDCG@10:0.6229,HR@20:0.9054,NDCG@20:0.6425
2025-01-07 15:48:24,240 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-07 15:48:34,283 - root - INFO - [Train] Epoch 5 [17600/35182]: Training loss 0.21805115044116974, current lr 9.725262215400358e-05, contrastive loss 3.7017195224761963, task loss 0.18103395402431488
2025-01-07 15:50:14,864 - root - INFO - [Train] Epoch 5 [17700/35182]: Training loss 0.2273743897676468, current lr 9.725120093232143e-05, contrastive loss 3.8784494400024414, task loss 0.1885899007320404
2025-01-07 15:51:55,244 - root - INFO - [Train] Epoch 5 [17800/35182]: Training loss 0.22493398189544678, current lr 9.724977971063926e-05, contrastive loss 3.779413938522339, task loss 0.1871398389339447
2025-01-07 15:53:35,829 - root - INFO - [Train] Epoch 5 [17900/35182]: Training loss 0.18289360404014587, current lr 9.724835848895711e-05, contrastive loss 3.6791036128997803, task loss 0.14610256254673004
2025-01-07 15:55:16,216 - root - INFO - [Train] Epoch 5 [18000/35182]: Training loss 0.28369686007499695, current lr 9.724693726727495e-05, contrastive loss 3.6905198097229004, task loss 0.24679166078567505
2025-01-07 15:56:56,595 - root - INFO - [Train] Epoch 5 [18100/35182]: Training loss 0.20712417364120483, current lr 9.72455160455928e-05, contrastive loss 3.688138246536255, task loss 0.1702428013086319
2025-01-07 15:58:37,184 - root - INFO - [Train] Epoch 5 [18200/35182]: Training loss 0.18882232904434204, current lr 9.724409482391064e-05, contrastive loss 3.6876001358032227, task loss 0.1519463211297989
2025-01-07 16:00:17,560 - root - INFO - [Train] Epoch 5 [18300/35182]: Training loss 0.21524953842163086, current lr 9.724267360222848e-05, contrastive loss 3.6910147666931152, task loss 0.1783393919467926
2025-01-07 16:01:58,177 - root - INFO - [Train] Epoch 5 [18400/35182]: Training loss 0.17682063579559326, current lr 9.724125238054633e-05, contrastive loss 3.6776034832000732, task loss 0.14004459977149963
2025-01-07 16:03:38,843 - root - INFO - [Train] Epoch 5 [18500/35182]: Training loss 0.1854030340909958, current lr 9.723983115886417e-05, contrastive loss 3.632866382598877, task loss 0.14907437562942505
2025-01-07 16:05:19,243 - root - INFO - [Train] Epoch 5 [18600/35182]: Training loss 0.21523167192935944, current lr 9.723840993718201e-05, contrastive loss 3.890122890472412, task loss 0.17633044719696045
2025-01-07 16:06:59,918 - root - INFO - [Train] Epoch 5 [18700/35182]: Training loss 0.24537444114685059, current lr 9.723698871549984e-05, contrastive loss 3.741020441055298, task loss 0.2079642415046692
2025-01-07 16:08:40,304 - root - INFO - [Train] Epoch 5 [18800/35182]: Training loss 0.24968567490577698, current lr 9.72355674938177e-05, contrastive loss 3.7042863368988037, task loss 0.21264280378818512
2025-01-07 16:10:20,885 - root - INFO - [Train] Epoch 5 [18900/35182]: Training loss 0.2083018273115158, current lr 9.723414627213553e-05, contrastive loss 3.7679271697998047, task loss 0.1706225574016571
2025-01-07 16:12:01,273 - root - INFO - [Train] Epoch 5 [19000/35182]: Training loss 0.21956345438957214, current lr 9.723272505045337e-05, contrastive loss 3.905879020690918, task loss 0.1805046647787094
2025-01-07 16:13:41,849 - root - INFO - [Train] Epoch 5 [19100/35182]: Training loss 0.19374091923236847, current lr 9.723130382877122e-05, contrastive loss 3.7308311462402344, task loss 0.1564326137304306
2025-01-07 16:15:22,235 - root - INFO - [Train] Epoch 5 [19200/35182]: Training loss 0.19551591575145721, current lr 9.722988260708906e-05, contrastive loss 3.6499595642089844, task loss 0.15901632606983185
2025-01-07 16:17:02,822 - root - INFO - [Train] Epoch 5 [19300/35182]: Training loss 0.22249388694763184, current lr 9.72284613854069e-05, contrastive loss 3.626615524291992, task loss 0.1862277388572693
2025-01-07 16:18:43,226 - root - INFO - [Train] Epoch 5 [19400/35182]: Training loss 0.24568083882331848, current lr 9.722704016372475e-05, contrastive loss 3.7945690155029297, task loss 0.20773515105247498
2025-01-07 16:20:23,900 - root - INFO - [Train] Epoch 5 [19500/35182]: Training loss 0.1717168390750885, current lr 9.722561894204259e-05, contrastive loss 3.5840141773223877, task loss 0.13587670028209686
2025-01-07 16:22:04,310 - root - INFO - [Train] Epoch 5 [19600/35182]: Training loss 0.16040641069412231, current lr 9.722419772036042e-05, contrastive loss 3.7538700103759766, task loss 0.12286771833896637
2025-01-07 16:23:44,993 - root - INFO - [Train] Epoch 5 [19700/35182]: Training loss 0.1950293332338333, current lr 9.722277649867828e-05, contrastive loss 3.72039794921875, task loss 0.15782535076141357
2025-01-07 16:25:25,397 - root - INFO - [Train] Epoch 5 [19800/35182]: Training loss 0.1633668839931488, current lr 9.72213552769961e-05, contrastive loss 3.8306679725646973, task loss 0.12506020069122314
2025-01-07 16:27:06,080 - root - INFO - [Train] Epoch 5 [19900/35182]: Training loss 0.18352490663528442, current lr 9.721993405531395e-05, contrastive loss 3.5889363288879395, task loss 0.1476355493068695
2025-01-07 16:28:46,485 - root - INFO - [Train] Epoch 5 [20000/35182]: Training loss 0.16751845180988312, current lr 9.721851283363179e-05, contrastive loss 3.765353202819824, task loss 0.1298649162054062
2025-01-07 16:30:27,135 - root - INFO - [Train] Epoch 5 [20100/35182]: Training loss 0.23018747568130493, current lr 9.721709161194964e-05, contrastive loss 3.8680200576782227, task loss 0.1915072798728943
2025-01-07 16:32:07,527 - root - INFO - [Train] Epoch 5 [20200/35182]: Training loss 0.2078305184841156, current lr 9.721567039026748e-05, contrastive loss 3.762871742248535, task loss 0.17020179331302643
2025-01-07 16:33:48,109 - root - INFO - [Train] Epoch 5 [20300/35182]: Training loss 0.17794372141361237, current lr 9.721424916858532e-05, contrastive loss 3.671088695526123, task loss 0.14123283326625824
2025-01-07 16:35:28,483 - root - INFO - [Train] Epoch 5 [20400/35182]: Training loss 0.16631004214286804, current lr 9.721282794690317e-05, contrastive loss 3.9731738567352295, task loss 0.12657830119132996
2025-01-07 16:37:09,071 - root - INFO - [Train] Epoch 5 [20500/35182]: Training loss 0.18279922008514404, current lr 9.7211406725221e-05, contrastive loss 3.951469898223877, task loss 0.14328451454639435
2025-01-07 16:38:49,445 - root - INFO - [Train] Epoch 5 [20600/35182]: Training loss 0.2827523648738861, current lr 9.720998550353885e-05, contrastive loss 3.9226574897766113, task loss 0.24352578818798065
2025-01-07 16:40:30,059 - root - INFO - [Train] Epoch 5 [20700/35182]: Training loss 0.184864342212677, current lr 9.720856428185668e-05, contrastive loss 3.903120994567871, task loss 0.14583313465118408
2025-01-07 16:42:10,431 - root - INFO - [Train] Epoch 5 [20800/35182]: Training loss 0.2130124717950821, current lr 9.720714306017454e-05, contrastive loss 3.851273536682129, task loss 0.1744997352361679
2025-01-07 16:43:50,995 - root - INFO - [Train] Epoch 5 [20900/35182]: Training loss 0.21335619688034058, current lr 9.720572183849237e-05, contrastive loss 3.6236143112182617, task loss 0.17712005972862244
2025-01-07 16:45:31,374 - root - INFO - [Train] Epoch 5 [21000/35182]: Training loss 0.22313112020492554, current lr 9.720430061681021e-05, contrastive loss 3.8459291458129883, task loss 0.18467183411121368
2025-01-07 16:47:11,972 - root - INFO - [Train] Epoch 5 [21100/35182]: Training loss 0.18193499743938446, current lr 9.720287939512806e-05, contrastive loss 3.6717190742492676, task loss 0.14521780610084534
2025-01-07 16:48:52,374 - root - INFO - [Train] Epoch 5 [21200/35182]: Training loss 0.18288788199424744, current lr 9.72014581734459e-05, contrastive loss 3.734360694885254, task loss 0.14554427564144135
2025-01-07 16:50:33,045 - root - INFO - [Train] Epoch 5 [21300/35182]: Training loss 0.20346516370773315, current lr 9.720003695176374e-05, contrastive loss 3.7046337127685547, task loss 0.16641883552074432
2025-01-07 16:52:13,421 - root - INFO - [Train] Epoch 5 [21400/35182]: Training loss 0.21830680966377258, current lr 9.719861573008159e-05, contrastive loss 3.6620829105377197, task loss 0.18168598413467407
2025-01-07 16:53:54,012 - root - INFO - [Train] Epoch 5 [21500/35182]: Training loss 0.2018914818763733, current lr 9.719719450839943e-05, contrastive loss 3.6173276901245117, task loss 0.165718212723732
2025-01-07 16:55:34,403 - root - INFO - [Train] Epoch 5 [21600/35182]: Training loss 0.18351128697395325, current lr 9.719577328671726e-05, contrastive loss 3.757160186767578, task loss 0.1459396779537201
2025-01-07 16:57:14,987 - root - INFO - [Train] Epoch 5 [21700/35182]: Training loss 0.21130403876304626, current lr 9.719435206503512e-05, contrastive loss 3.6841697692871094, task loss 0.17446233332157135
2025-01-07 16:58:55,375 - root - INFO - [Train] Epoch 5 [21800/35182]: Training loss 0.14302773773670197, current lr 9.719293084335295e-05, contrastive loss 3.713876247406006, task loss 0.1058889776468277
2025-01-07 17:00:35,953 - root - INFO - [Train] Epoch 5 [21900/35182]: Training loss 0.19223734736442566, current lr 9.719150962167079e-05, contrastive loss 3.588139533996582, task loss 0.15635594725608826
2025-01-07 17:02:16,343 - root - INFO - [Train] Epoch 5 [22000/35182]: Training loss 0.15881527960300446, current lr 9.719008839998863e-05, contrastive loss 3.8765439987182617, task loss 0.1200498417019844
2025-01-07 17:03:56,908 - root - INFO - [Train] Epoch 5 [22100/35182]: Training loss 0.12503604590892792, current lr 9.718866717830648e-05, contrastive loss 3.6790781021118164, task loss 0.08824526518583298
2025-01-07 17:05:37,306 - root - INFO - [Train] Epoch 5 [22200/35182]: Training loss 0.19412559270858765, current lr 9.718724595662432e-05, contrastive loss 3.9232397079467773, task loss 0.1548931896686554
2025-01-07 17:07:17,883 - root - INFO - [Train] Epoch 5 [22300/35182]: Training loss 0.16851480305194855, current lr 9.718582473494216e-05, contrastive loss 3.634978771209717, task loss 0.13216501474380493
2025-01-07 17:08:58,271 - root - INFO - [Train] Epoch 5 [22400/35182]: Training loss 0.14307713508605957, current lr 9.718440351326e-05, contrastive loss 3.747875690460205, task loss 0.10559838265180588
2025-01-07 17:10:38,944 - root - INFO - [Train] Epoch 5 [22500/35182]: Training loss 0.17430055141448975, current lr 9.718298229157784e-05, contrastive loss 3.7055201530456543, task loss 0.13724534213542938
2025-01-07 17:12:19,318 - root - INFO - [Train] Epoch 5 [22600/35182]: Training loss 0.1928863376379013, current lr 9.718156106989569e-05, contrastive loss 3.781024932861328, task loss 0.15507608652114868
2025-01-07 17:13:59,922 - root - INFO - [Train] Epoch 5 [22700/35182]: Training loss 0.17338798940181732, current lr 9.718013984821352e-05, contrastive loss 3.702099323272705, task loss 0.13636699318885803
2025-01-07 17:15:40,297 - root - INFO - [Train] Epoch 5 [22800/35182]: Training loss 0.1726694107055664, current lr 9.717871862653138e-05, contrastive loss 3.6786422729492188, task loss 0.13588298857212067
2025-01-07 17:17:20,887 - root - INFO - [Train] Epoch 5 [22900/35182]: Training loss 0.2492927610874176, current lr 9.717729740484921e-05, contrastive loss 3.7337594032287598, task loss 0.21195515990257263
2025-01-07 17:19:01,299 - root - INFO - [Train] Epoch 5 [23000/35182]: Training loss 0.14428997039794922, current lr 9.717587618316705e-05, contrastive loss 3.769850254058838, task loss 0.10659146308898926
2025-01-07 17:20:41,970 - root - INFO - [Train] Epoch 5 [23100/35182]: Training loss 0.14380255341529846, current lr 9.71744549614849e-05, contrastive loss 3.740133285522461, task loss 0.106401227414608
2025-01-07 17:22:22,366 - root - INFO - [Train] Epoch 5 [23200/35182]: Training loss 0.16145212948322296, current lr 9.717303373980274e-05, contrastive loss 3.843233585357666, task loss 0.12301979213953018
2025-01-07 17:24:03,056 - root - INFO - [Train] Epoch 5 [23300/35182]: Training loss 0.181074857711792, current lr 9.717161251812058e-05, contrastive loss 3.501328468322754, task loss 0.14606156945228577
2025-01-07 17:25:43,450 - root - INFO - [Train] Epoch 5 [23400/35182]: Training loss 0.19742299616336823, current lr 9.717019129643843e-05, contrastive loss 3.719388484954834, task loss 0.16022911667823792
2025-01-07 17:27:24,128 - root - INFO - [Train] Epoch 5 [23500/35182]: Training loss 0.17522765696048737, current lr 9.716877007475627e-05, contrastive loss 3.6603188514709473, task loss 0.13862447440624237
2025-01-07 17:29:04,513 - root - INFO - [Train] Epoch 5 [23600/35182]: Training loss 0.19738349318504333, current lr 9.71673488530741e-05, contrastive loss 3.7359042167663574, task loss 0.16002444922924042
2025-01-07 17:30:45,102 - root - INFO - [Train] Epoch 5 [23700/35182]: Training loss 0.2268887609243393, current lr 9.716592763139196e-05, contrastive loss 3.7450268268585205, task loss 0.18943849205970764
2025-01-07 17:32:25,483 - root - INFO - [Train] Epoch 5 [23800/35182]: Training loss 0.14617303013801575, current lr 9.716450640970979e-05, contrastive loss 3.8612425327301025, task loss 0.10756059736013412
2025-01-07 17:34:06,066 - root - INFO - [Train] Epoch 5 [23900/35182]: Training loss 0.1589076966047287, current lr 9.716308518802763e-05, contrastive loss 3.8018336296081543, task loss 0.12088936567306519
2025-01-07 17:35:46,443 - root - INFO - [Train] Epoch 5 [24000/35182]: Training loss 0.15793858468532562, current lr 9.716166396634547e-05, contrastive loss 3.5406625270843506, task loss 0.1225319653749466
2025-01-07 17:37:27,023 - root - INFO - [Train] Epoch 5 [24100/35182]: Training loss 0.13414420187473297, current lr 9.716024274466332e-05, contrastive loss 3.8837242126464844, task loss 0.09530696272850037
2025-01-07 17:39:07,410 - root - INFO - [Train] Epoch 5 [24200/35182]: Training loss 0.1345365196466446, current lr 9.715882152298116e-05, contrastive loss 3.7869412899017334, task loss 0.09666711091995239
2025-01-07 17:40:47,995 - root - INFO - [Train] Epoch 5 [24300/35182]: Training loss 0.16028839349746704, current lr 9.7157400301299e-05, contrastive loss 3.7094757556915283, task loss 0.1231936439871788
2025-01-07 17:42:28,381 - root - INFO - [Train] Epoch 5 [24400/35182]: Training loss 0.175807386636734, current lr 9.715597907961685e-05, contrastive loss 3.7575759887695312, task loss 0.13823163509368896
2025-01-07 17:44:08,976 - root - INFO - [Train] Epoch 5 [24500/35182]: Training loss 0.19599376618862152, current lr 9.715455785793468e-05, contrastive loss 3.676767349243164, task loss 0.15922608971595764
2025-01-07 17:45:49,356 - root - INFO - [Train] Epoch 5 [24600/35182]: Training loss 0.15783026814460754, current lr 9.715313663625253e-05, contrastive loss 3.772724151611328, task loss 0.12010303139686584
2025-01-07 17:47:29,929 - root - INFO - [Train] Epoch 5 [24700/35182]: Training loss 0.23907434940338135, current lr 9.715171541457036e-05, contrastive loss 3.712045669555664, task loss 0.20195388793945312
2025-01-07 17:49:10,341 - root - INFO - [Train] Epoch 5 [24800/35182]: Training loss 0.17876142263412476, current lr 9.715029419288821e-05, contrastive loss 3.8480417728424072, task loss 0.14028100669384003
2025-01-07 17:50:51,001 - root - INFO - [Train] Epoch 5 [24900/35182]: Training loss 0.21214450895786285, current lr 9.714887297120605e-05, contrastive loss 3.845883846282959, task loss 0.17368566989898682
2025-01-07 17:52:31,408 - root - INFO - [Train] Epoch 5 [25000/35182]: Training loss 0.18468961119651794, current lr 9.71474517495239e-05, contrastive loss 3.7231361865997314, task loss 0.1474582552909851
2025-01-07 17:54:11,983 - root - INFO - [Train] Epoch 5 [25100/35182]: Training loss 0.2552361488342285, current lr 9.714603052784174e-05, contrastive loss 3.6280250549316406, task loss 0.2189559042453766
2025-01-07 17:55:52,364 - root - INFO - [Train] Epoch 5 [25200/35182]: Training loss 0.23015673458576202, current lr 9.714460930615958e-05, contrastive loss 3.8450822830200195, task loss 0.19170591235160828
2025-01-07 17:57:32,939 - root - INFO - [Train] Epoch 5 [25300/35182]: Training loss 0.1333850622177124, current lr 9.714318808447742e-05, contrastive loss 3.8114686012268066, task loss 0.09527037292718887
2025-01-07 17:59:13,314 - root - INFO - [Train] Epoch 5 [25400/35182]: Training loss 0.12953585386276245, current lr 9.714176686279525e-05, contrastive loss 3.7909159660339355, task loss 0.09162670373916626
2025-01-07 18:00:53,894 - root - INFO - [Train] Epoch 5 [25500/35182]: Training loss 0.19523362815380096, current lr 9.714034564111311e-05, contrastive loss 3.6377339363098145, task loss 0.15885628759860992
2025-01-07 18:02:34,290 - root - INFO - [Train] Epoch 5 [25600/35182]: Training loss 0.1944572478532791, current lr 9.713892441943094e-05, contrastive loss 3.776834487915039, task loss 0.1566888988018036
2025-01-07 18:04:14,970 - root - INFO - [Train] Epoch 5 [25700/35182]: Training loss 0.15071837604045868, current lr 9.71375031977488e-05, contrastive loss 3.833470582962036, task loss 0.11238367110490799
2025-01-07 18:05:55,353 - root - INFO - [Train] Epoch 5 [25800/35182]: Training loss 0.18739546835422516, current lr 9.713608197606663e-05, contrastive loss 3.756399154663086, task loss 0.14983147382736206
2025-01-07 18:07:35,955 - root - INFO - [Train] Epoch 5 [25900/35182]: Training loss 0.1911773681640625, current lr 9.713466075438447e-05, contrastive loss 3.767484664916992, task loss 0.15350252389907837
2025-01-07 18:09:16,346 - root - INFO - [Train] Epoch 5 [26000/35182]: Training loss 0.19172093272209167, current lr 9.713323953270231e-05, contrastive loss 3.650178909301758, task loss 0.15521913766860962
2025-01-07 18:10:57,025 - root - INFO - [Train] Epoch 5 [26100/35182]: Training loss 0.12586449086666107, current lr 9.713181831102016e-05, contrastive loss 3.754964828491211, task loss 0.08831483870744705
2025-01-07 18:12:37,409 - root - INFO - [Train] Epoch 5 [26200/35182]: Training loss 0.14133179187774658, current lr 9.7130397089338e-05, contrastive loss 3.6806793212890625, task loss 0.10452499240636826
2025-01-07 18:14:17,998 - root - INFO - [Train] Epoch 5 [26300/35182]: Training loss 0.11495868861675262, current lr 9.712897586765584e-05, contrastive loss 3.779721260070801, task loss 0.07716147601604462
2025-01-07 18:15:58,382 - root - INFO - [Train] Epoch 5 [26400/35182]: Training loss 0.2129889279603958, current lr 9.712755464597369e-05, contrastive loss 3.8659496307373047, task loss 0.17432942986488342
2025-01-07 18:17:38,960 - root - INFO - [Train] Epoch 5 [26500/35182]: Training loss 0.1267615109682083, current lr 9.712613342429152e-05, contrastive loss 3.651796817779541, task loss 0.09024354815483093
2025-01-07 18:19:19,330 - root - INFO - [Train] Epoch 5 [26600/35182]: Training loss 0.13514453172683716, current lr 9.712471220260937e-05, contrastive loss 3.705221176147461, task loss 0.09809232503175735
2025-01-07 18:20:59,926 - root - INFO - [Train] Epoch 5 [26700/35182]: Training loss 0.13960102200508118, current lr 9.71232909809272e-05, contrastive loss 3.6190710067749023, task loss 0.10341031104326248
2025-01-07 18:22:40,305 - root - INFO - [Train] Epoch 5 [26800/35182]: Training loss 0.1399179846048355, current lr 9.712186975924505e-05, contrastive loss 3.666429042816162, task loss 0.10325369238853455
2025-01-07 18:24:20,896 - root - INFO - [Train] Epoch 5 [26900/35182]: Training loss 0.1908554583787918, current lr 9.712044853756289e-05, contrastive loss 3.6923317909240723, task loss 0.1539321392774582
2025-01-07 18:26:01,273 - root - INFO - [Train] Epoch 5 [27000/35182]: Training loss 0.12219147384166718, current lr 9.711902731588073e-05, contrastive loss 3.785365581512451, task loss 0.08433782309293747
2025-01-07 18:27:41,857 - root - INFO - [Train] Epoch 5 [27100/35182]: Training loss 0.1651880294084549, current lr 9.711760609419858e-05, contrastive loss 3.700909376144409, task loss 0.1281789392232895
2025-01-07 18:29:22,233 - root - INFO - [Train] Epoch 5 [27200/35182]: Training loss 0.2252935916185379, current lr 9.711618487251642e-05, contrastive loss 3.693272352218628, task loss 0.18836086988449097
2025-01-07 18:31:02,822 - root - INFO - [Train] Epoch 5 [27300/35182]: Training loss 0.26544737815856934, current lr 9.711476365083426e-05, contrastive loss 3.714682102203369, task loss 0.2283005565404892
2025-01-07 18:32:43,197 - root - INFO - [Train] Epoch 5 [27400/35182]: Training loss 0.20130960643291473, current lr 9.71133424291521e-05, contrastive loss 3.8256702423095703, task loss 0.16305290162563324
2025-01-07 18:34:23,568 - root - INFO - [Train] Epoch 5 [27500/35182]: Training loss 0.1435113549232483, current lr 9.711192120746995e-05, contrastive loss 3.6952738761901855, task loss 0.10655862092971802
2025-01-07 18:36:04,171 - root - INFO - [Train] Epoch 5 [27600/35182]: Training loss 0.14189670979976654, current lr 9.711049998578778e-05, contrastive loss 3.6698641777038574, task loss 0.10519807040691376
2025-01-07 18:37:44,556 - root - INFO - [Train] Epoch 5 [27700/35182]: Training loss 0.12155048549175262, current lr 9.710907876410564e-05, contrastive loss 3.720855474472046, task loss 0.08434192836284637
2025-01-07 18:39:25,133 - root - INFO - [Train] Epoch 5 [27800/35182]: Training loss 0.16864550113677979, current lr 9.710765754242347e-05, contrastive loss 3.6199779510498047, task loss 0.13244572281837463
2025-01-07 18:41:05,520 - root - INFO - [Train] Epoch 5 [27900/35182]: Training loss 0.1788605898618698, current lr 9.710623632074131e-05, contrastive loss 3.6505680084228516, task loss 0.14235490560531616
2025-01-07 18:42:46,116 - root - INFO - [Train] Epoch 5 [28000/35182]: Training loss 0.1437479704618454, current lr 9.710481509905915e-05, contrastive loss 3.667043447494507, task loss 0.10707753151655197
2025-01-07 18:44:26,515 - root - INFO - [Train] Epoch 5 [28100/35182]: Training loss 0.14149242639541626, current lr 9.7103393877377e-05, contrastive loss 3.752546548843384, task loss 0.10396695882081985
2025-01-07 18:46:07,183 - root - INFO - [Train] Epoch 5 [28200/35182]: Training loss 0.11675187945365906, current lr 9.710197265569484e-05, contrastive loss 3.7395663261413574, task loss 0.07935621589422226
2025-01-07 18:47:47,574 - root - INFO - [Train] Epoch 5 [28300/35182]: Training loss 0.15759438276290894, current lr 9.710055143401267e-05, contrastive loss 3.9406204223632812, task loss 0.11818817257881165
2025-01-07 18:49:28,147 - root - INFO - [Train] Epoch 5 [28400/35182]: Training loss 0.14072591066360474, current lr 9.709913021233053e-05, contrastive loss 3.748805046081543, task loss 0.10323786735534668
2025-01-07 18:51:08,536 - root - INFO - [Train] Epoch 5 [28500/35182]: Training loss 0.15651635825634003, current lr 9.709770899064836e-05, contrastive loss 3.8873937129974365, task loss 0.11764242500066757
2025-01-07 18:52:49,115 - root - INFO - [Train] Epoch 5 [28600/35182]: Training loss 0.11900114268064499, current lr 9.709628776896622e-05, contrastive loss 3.7704334259033203, task loss 0.08129680901765823
2025-01-07 18:54:29,508 - root - INFO - [Train] Epoch 5 [28700/35182]: Training loss 0.1905268430709839, current lr 9.709486654728404e-05, contrastive loss 3.579620361328125, task loss 0.1547306478023529
2025-01-07 18:56:10,100 - root - INFO - [Train] Epoch 5 [28800/35182]: Training loss 0.11494213342666626, current lr 9.709344532560189e-05, contrastive loss 3.630228042602539, task loss 0.07863985002040863
2025-01-07 18:57:50,499 - root - INFO - [Train] Epoch 5 [28900/35182]: Training loss 0.1307339370250702, current lr 9.709202410391973e-05, contrastive loss 3.719107151031494, task loss 0.09354285895824432
2025-01-07 18:59:31,174 - root - INFO - [Train] Epoch 5 [29000/35182]: Training loss 0.15295633673667908, current lr 9.709060288223758e-05, contrastive loss 3.7223055362701416, task loss 0.11573328822851181
2025-01-07 19:01:11,854 - root - INFO - [Train] Epoch 5 [29100/35182]: Training loss 0.1407228708267212, current lr 9.708918166055542e-05, contrastive loss 3.819640636444092, task loss 0.10252645611763
2025-01-07 19:02:52,252 - root - INFO - [Train] Epoch 5 [29200/35182]: Training loss 0.17360836267471313, current lr 9.708776043887326e-05, contrastive loss 3.8143150806427, task loss 0.13546520471572876
2025-01-07 19:04:32,946 - root - INFO - [Train] Epoch 5 [29300/35182]: Training loss 0.13892272114753723, current lr 9.70863392171911e-05, contrastive loss 3.706418037414551, task loss 0.10185854882001877
2025-01-07 19:06:13,343 - root - INFO - [Train] Epoch 5 [29400/35182]: Training loss 0.18538370728492737, current lr 9.708491799550894e-05, contrastive loss 3.839827299118042, task loss 0.14698542654514313
2025-01-07 19:07:54,034 - root - INFO - [Train] Epoch 5 [29500/35182]: Training loss 0.183511883020401, current lr 9.708349677382679e-05, contrastive loss 3.7284674644470215, task loss 0.14622721076011658
2025-01-07 19:09:34,445 - root - INFO - [Train] Epoch 5 [29600/35182]: Training loss 0.17832449078559875, current lr 9.708207555214462e-05, contrastive loss 3.6785106658935547, task loss 0.14153937995433807
2025-01-07 19:11:15,102 - root - INFO - [Train] Epoch 5 [29700/35182]: Training loss 0.15957459807395935, current lr 9.708065433046248e-05, contrastive loss 3.5606515407562256, task loss 0.12396808713674545
2025-01-07 19:12:55,486 - root - INFO - [Train] Epoch 5 [29800/35182]: Training loss 0.15426898002624512, current lr 9.707923310878031e-05, contrastive loss 3.7513277530670166, task loss 0.11675569415092468
2025-01-07 19:14:36,080 - root - INFO - [Train] Epoch 5 [29900/35182]: Training loss 0.1809074878692627, current lr 9.707781188709815e-05, contrastive loss 3.7582972049713135, task loss 0.14332452416419983
2025-01-07 19:16:16,469 - root - INFO - [Train] Epoch 5 [30000/35182]: Training loss 0.16173921525478363, current lr 9.7076390665416e-05, contrastive loss 3.6330044269561768, task loss 0.12540917098522186
2025-01-07 19:17:57,138 - root - INFO - [Train] Epoch 5 [30100/35182]: Training loss 0.15799376368522644, current lr 9.707496944373384e-05, contrastive loss 3.7539618015289307, task loss 0.12045414745807648
2025-01-07 19:19:37,523 - root - INFO - [Train] Epoch 5 [30200/35182]: Training loss 0.11167189478874207, current lr 9.707354822205168e-05, contrastive loss 3.7986550331115723, task loss 0.07368534803390503
2025-01-07 19:21:18,112 - root - INFO - [Train] Epoch 5 [30300/35182]: Training loss 0.19616252183914185, current lr 9.707212700036951e-05, contrastive loss 3.7912943363189697, task loss 0.15824957191944122
2025-01-07 19:22:58,491 - root - INFO - [Train] Epoch 5 [30400/35182]: Training loss 0.15458951890468597, current lr 9.707070577868737e-05, contrastive loss 3.699275493621826, task loss 0.11759676039218903
2025-01-07 19:24:39,070 - root - INFO - [Train] Epoch 5 [30500/35182]: Training loss 0.1598546802997589, current lr 9.70692845570052e-05, contrastive loss 3.7118897438049316, task loss 0.12273577600717545
2025-01-07 19:26:19,448 - root - INFO - [Train] Epoch 5 [30600/35182]: Training loss 0.13481658697128296, current lr 9.706786333532306e-05, contrastive loss 3.8921139240264893, task loss 0.0958954393863678
2025-01-07 19:28:00,052 - root - INFO - [Train] Epoch 5 [30700/35182]: Training loss 0.1247701421380043, current lr 9.706644211364089e-05, contrastive loss 3.5178074836730957, task loss 0.08959206938743591
2025-01-07 19:29:40,442 - root - INFO - [Train] Epoch 5 [30800/35182]: Training loss 0.13322167098522186, current lr 9.706502089195873e-05, contrastive loss 3.6667890548706055, task loss 0.09655378013849258
2025-01-07 19:31:21,117 - root - INFO - [Train] Epoch 5 [30900/35182]: Training loss 0.17568323016166687, current lr 9.706359967027657e-05, contrastive loss 3.774463653564453, task loss 0.13793860375881195
2025-01-07 19:33:01,801 - root - INFO - [Train] Epoch 5 [31000/35182]: Training loss 0.16934743523597717, current lr 9.706217844859442e-05, contrastive loss 3.763108730316162, task loss 0.13171634078025818
2025-01-07 19:34:42,198 - root - INFO - [Train] Epoch 5 [31100/35182]: Training loss 0.14764878153800964, current lr 9.706075722691226e-05, contrastive loss 3.6814017295837402, task loss 0.1108347624540329
2025-01-07 19:36:22,615 - root - INFO - [Train] Epoch 5 [31200/35182]: Training loss 0.1265200823545456, current lr 9.70593360052301e-05, contrastive loss 3.786349058151245, task loss 0.08865659683942795
2025-01-07 19:38:03,176 - root - INFO - [Train] Epoch 5 [31300/35182]: Training loss 0.14518526196479797, current lr 9.705791478354795e-05, contrastive loss 3.732464551925659, task loss 0.10786062479019165
2025-01-07 19:39:43,860 - root - INFO - [Train] Epoch 5 [31400/35182]: Training loss 0.1422157734632492, current lr 9.705649356186578e-05, contrastive loss 3.8702564239501953, task loss 0.1035132110118866
2025-01-07 19:41:24,252 - root - INFO - [Train] Epoch 5 [31500/35182]: Training loss 0.14500100910663605, current lr 9.705507234018363e-05, contrastive loss 3.941056966781616, task loss 0.10559044033288956
2025-01-07 19:43:04,832 - root - INFO - [Train] Epoch 5 [31600/35182]: Training loss 0.16844433546066284, current lr 9.705365111850146e-05, contrastive loss 3.7457351684570312, task loss 0.1309869885444641
2025-01-07 19:44:45,246 - root - INFO - [Train] Epoch 5 [31700/35182]: Training loss 0.154598206281662, current lr 9.705222989681932e-05, contrastive loss 3.8485236167907715, task loss 0.11611296981573105
2025-01-07 19:46:25,910 - root - INFO - [Train] Epoch 5 [31800/35182]: Training loss 0.1745549887418747, current lr 9.705080867513715e-05, contrastive loss 3.8129348754882812, task loss 0.13642564415931702
2025-01-07 19:48:06,304 - root - INFO - [Train] Epoch 5 [31900/35182]: Training loss 0.15633946657180786, current lr 9.704938745345499e-05, contrastive loss 3.6886439323425293, task loss 0.11945302784442902
2025-01-07 19:49:46,968 - root - INFO - [Train] Epoch 5 [32000/35182]: Training loss 0.17417675256729126, current lr 9.704796623177284e-05, contrastive loss 3.7567973136901855, task loss 0.1366087794303894
2025-01-07 19:51:27,358 - root - INFO - [Train] Epoch 5 [32100/35182]: Training loss 0.10161848366260529, current lr 9.704654501009068e-05, contrastive loss 3.576272487640381, task loss 0.06585576385259628
2025-01-07 19:53:07,962 - root - INFO - [Train] Epoch 5 [32200/35182]: Training loss 0.22713583707809448, current lr 9.704512378840852e-05, contrastive loss 3.7854785919189453, task loss 0.18928104639053345
2025-01-07 19:54:48,363 - root - INFO - [Train] Epoch 5 [32300/35182]: Training loss 0.19148996472358704, current lr 9.704370256672635e-05, contrastive loss 3.687025547027588, task loss 0.1546197086572647
2025-01-07 19:56:29,039 - root - INFO - [Train] Epoch 5 [32400/35182]: Training loss 0.148204043507576, current lr 9.704228134504421e-05, contrastive loss 3.7318739891052246, task loss 0.11088530719280243
2025-01-07 19:58:09,454 - root - INFO - [Train] Epoch 5 [32500/35182]: Training loss 0.175519198179245, current lr 9.704086012336204e-05, contrastive loss 3.6671805381774902, task loss 0.13884739577770233
2025-01-07 19:59:50,133 - root - INFO - [Train] Epoch 5 [32600/35182]: Training loss 0.1568257361650467, current lr 9.70394389016799e-05, contrastive loss 3.6478915214538574, task loss 0.12034682184457779
2025-01-07 20:01:30,815 - root - INFO - [Train] Epoch 5 [32700/35182]: Training loss 0.10626283288002014, current lr 9.703801767999773e-05, contrastive loss 3.599231719970703, task loss 0.07027051597833633
2025-01-07 20:03:11,209 - root - INFO - [Train] Epoch 5 [32800/35182]: Training loss 0.16427242755889893, current lr 9.703659645831557e-05, contrastive loss 3.8931214809417725, task loss 0.12534122169017792
2025-01-07 20:04:51,873 - root - INFO - [Train] Epoch 5 [32900/35182]: Training loss 0.1154135912656784, current lr 9.703517523663341e-05, contrastive loss 3.643059253692627, task loss 0.07898300141096115
2025-01-07 20:06:32,278 - root - INFO - [Train] Epoch 5 [33000/35182]: Training loss 0.10962271690368652, current lr 9.703375401495126e-05, contrastive loss 3.5898613929748535, task loss 0.07372409850358963
2025-01-07 20:08:12,962 - root - INFO - [Train] Epoch 5 [33100/35182]: Training loss 0.12540224194526672, current lr 9.70323327932691e-05, contrastive loss 3.7410387992858887, task loss 0.0879918560385704
2025-01-07 20:09:53,340 - root - INFO - [Train] Epoch 5 [33200/35182]: Training loss 0.16668684780597687, current lr 9.703091157158693e-05, contrastive loss 3.80112361907959, task loss 0.12867560982704163
2025-01-07 20:11:33,950 - root - INFO - [Train] Epoch 5 [33300/35182]: Training loss 0.12791235744953156, current lr 9.702949034990479e-05, contrastive loss 3.710052728652954, task loss 0.0908118262887001
2025-01-07 20:13:14,350 - root - INFO - [Train] Epoch 5 [33400/35182]: Training loss 0.17815455794334412, current lr 9.702806912822262e-05, contrastive loss 3.823207139968872, task loss 0.13992248475551605
2025-01-07 20:14:55,026 - root - INFO - [Train] Epoch 5 [33500/35182]: Training loss 0.1486068218946457, current lr 9.702664790654047e-05, contrastive loss 3.838958263397217, task loss 0.11021724343299866
2025-01-07 20:16:35,422 - root - INFO - [Train] Epoch 5 [33600/35182]: Training loss 0.13963016867637634, current lr 9.70252266848583e-05, contrastive loss 3.818293571472168, task loss 0.10144723206758499
2025-01-07 20:18:16,100 - root - INFO - [Train] Epoch 5 [33700/35182]: Training loss 0.1871333122253418, current lr 9.702380546317616e-05, contrastive loss 3.691985607147217, task loss 0.15021345019340515
2025-01-07 20:19:56,502 - root - INFO - [Train] Epoch 5 [33800/35182]: Training loss 0.13900402188301086, current lr 9.702238424149399e-05, contrastive loss 3.6918656826019287, task loss 0.10208537429571152
2025-01-07 20:21:37,174 - root - INFO - [Train] Epoch 5 [33900/35182]: Training loss 0.13276633620262146, current lr 9.702096301981183e-05, contrastive loss 3.672623634338379, task loss 0.09604009985923767
2025-01-07 20:23:17,581 - root - INFO - [Train] Epoch 5 [34000/35182]: Training loss 0.14465689659118652, current lr 9.701954179812968e-05, contrastive loss 3.7916741371154785, task loss 0.10674016177654266
2025-01-07 20:24:58,181 - root - INFO - [Train] Epoch 5 [34100/35182]: Training loss 0.10197000950574875, current lr 9.701812057644752e-05, contrastive loss 3.6179254055023193, task loss 0.06579075753688812
2025-01-07 20:26:38,845 - root - INFO - [Train] Epoch 5 [34200/35182]: Training loss 0.17160172760486603, current lr 9.701669935476536e-05, contrastive loss 3.8991904258728027, task loss 0.13260982930660248
2025-01-07 20:28:19,265 - root - INFO - [Train] Epoch 5 [34300/35182]: Training loss 0.20793142914772034, current lr 9.70152781330832e-05, contrastive loss 4.033507347106934, task loss 0.16759635508060455
2025-01-07 20:29:59,928 - root - INFO - [Train] Epoch 5 [34400/35182]: Training loss 0.22135397791862488, current lr 9.701385691140105e-05, contrastive loss 3.722604990005493, task loss 0.18412792682647705
2025-01-07 20:31:40,324 - root - INFO - [Train] Epoch 5 [34500/35182]: Training loss 0.17980243265628815, current lr 9.701243568971888e-05, contrastive loss 3.8143296241760254, task loss 0.14165914058685303
2025-01-07 20:33:20,997 - root - INFO - [Train] Epoch 5 [34600/35182]: Training loss 0.14600428938865662, current lr 9.701101446803674e-05, contrastive loss 3.663264274597168, task loss 0.10937163978815079
2025-01-07 20:35:01,406 - root - INFO - [Train] Epoch 5 [34700/35182]: Training loss 0.14377029240131378, current lr 9.700959324635457e-05, contrastive loss 3.6018238067626953, task loss 0.10775205492973328
2025-01-07 20:36:42,089 - root - INFO - [Train] Epoch 5 [34800/35182]: Training loss 0.1328066885471344, current lr 9.700817202467241e-05, contrastive loss 3.741809368133545, task loss 0.09538859128952026
2025-01-07 20:38:22,489 - root - INFO - [Train] Epoch 5 [34900/35182]: Training loss 0.14999648928642273, current lr 9.700675080299025e-05, contrastive loss 3.69901180267334, task loss 0.11300637573003769
2025-01-07 20:40:03,173 - root - INFO - [Train] Epoch 5 [35000/35182]: Training loss 0.12947063148021698, current lr 9.70053295813081e-05, contrastive loss 3.583397388458252, task loss 0.0936366617679596
2025-01-07 20:41:43,864 - root - INFO - [Train] Epoch 5 [35100/35182]: Training loss 0.17618021368980408, current lr 9.700390835962594e-05, contrastive loss 3.5467350482940674, task loss 0.14071285724639893
2025-01-07 23:27:13,103 - root - INFO - [Valid] Evaluation Results: HR@5:0.7963,NDCG@5:0.7274,HR@10:0.8560,NDCG@10:0.7467,HR@20:0.9141,NDCG@20:0.7615
2025-01-07 23:27:14,015 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-07 23:27:16,929 - root - INFO - [Train] Epoch 5: Overall Training loss 0.0
2025-01-07 23:27:16,929 - root - INFO - [Time] Epoch 5: Time taken 15.29 hours
2025-01-07 23:27:22,239 - root - INFO - [Train] Epoch 6 [0/35182]: Training loss 0.13442149758338928, current lr 9.700274295784658e-05, contrastive loss 3.804558515548706, task loss 0.09637590497732162
2025-01-07 23:29:02,820 - root - INFO - [Train] Epoch 6 [100/35182]: Training loss 0.1158737763762474, current lr 9.70013217361644e-05, contrastive loss 3.8518176078796387, task loss 0.07735560089349747
2025-01-07 23:30:43,210 - root - INFO - [Train] Epoch 6 [200/35182]: Training loss 0.0922473818063736, current lr 9.699990051448226e-05, contrastive loss 3.7488670349121094, task loss 0.0547587126493454
2025-01-07 23:32:23,593 - root - INFO - [Train] Epoch 6 [300/35182]: Training loss 0.12863948941230774, current lr 9.699847929280009e-05, contrastive loss 3.6770198345184326, task loss 0.0918692871928215
2025-01-07 23:34:04,174 - root - INFO - [Train] Epoch 6 [400/35182]: Training loss 0.20695795118808746, current lr 9.699705807111794e-05, contrastive loss 3.697390556335449, task loss 0.16998404264450073
2025-01-07 23:35:44,559 - root - INFO - [Train] Epoch 6 [500/35182]: Training loss 0.10599493980407715, current lr 9.699563684943578e-05, contrastive loss 3.630784034729004, task loss 0.06968710571527481
2025-01-07 23:37:25,138 - root - INFO - [Train] Epoch 6 [600/35182]: Training loss 0.2087036669254303, current lr 9.699421562775362e-05, contrastive loss 3.726857900619507, task loss 0.17143508791923523
2025-01-07 23:39:05,526 - root - INFO - [Train] Epoch 6 [700/35182]: Training loss 0.16348715126514435, current lr 9.699279440607147e-05, contrastive loss 3.777310609817505, task loss 0.12571404874324799
2025-01-07 23:40:46,096 - root - INFO - [Train] Epoch 6 [800/35182]: Training loss 0.11972659826278687, current lr 9.69913731843893e-05, contrastive loss 3.6216254234313965, task loss 0.08351033926010132
2025-01-07 23:42:26,493 - root - INFO - [Train] Epoch 6 [900/35182]: Training loss 0.1695261001586914, current lr 9.698995196270715e-05, contrastive loss 3.859370470046997, task loss 0.13093239068984985
2025-01-07 23:44:07,067 - root - INFO - [Train] Epoch 6 [1000/35182]: Training loss 0.19470833241939545, current lr 9.698853074102498e-05, contrastive loss 3.887967586517334, task loss 0.15582865476608276
2025-01-07 23:45:47,461 - root - INFO - [Train] Epoch 6 [1100/35182]: Training loss 0.12744252383708954, current lr 9.698710951934284e-05, contrastive loss 3.6976523399353027, task loss 0.09046600013971329
2025-01-07 23:47:28,030 - root - INFO - [Train] Epoch 6 [1200/35182]: Training loss 0.1550033837556839, current lr 9.698568829766067e-05, contrastive loss 3.78493332862854, task loss 0.11715404689311981
2025-01-07 23:49:08,414 - root - INFO - [Train] Epoch 6 [1300/35182]: Training loss 0.1177852675318718, current lr 9.698426707597853e-05, contrastive loss 3.7938342094421387, task loss 0.07984692603349686
2025-01-07 23:50:49,013 - root - INFO - [Train] Epoch 6 [1400/35182]: Training loss 0.11156152933835983, current lr 9.698284585429636e-05, contrastive loss 3.6663899421691895, task loss 0.0748976320028305
2025-01-07 23:52:29,422 - root - INFO - [Train] Epoch 6 [1500/35182]: Training loss 0.11618617177009583, current lr 9.69814246326142e-05, contrastive loss 3.68148136138916, task loss 0.07937135547399521
2025-01-07 23:54:10,087 - root - INFO - [Train] Epoch 6 [1600/35182]: Training loss 0.13959228992462158, current lr 9.698000341093204e-05, contrastive loss 3.9420275688171387, task loss 0.1001720130443573
2025-01-07 23:55:50,476 - root - INFO - [Train] Epoch 6 [1700/35182]: Training loss 0.16449123620986938, current lr 9.697858218924989e-05, contrastive loss 3.677964448928833, task loss 0.12771159410476685
2025-01-07 23:57:31,135 - root - INFO - [Train] Epoch 6 [1800/35182]: Training loss 0.15961933135986328, current lr 9.697716096756773e-05, contrastive loss 3.8033461570739746, task loss 0.12158586829900742
2025-01-07 23:59:11,510 - root - INFO - [Train] Epoch 6 [1900/35182]: Training loss 0.11187650263309479, current lr 9.697573974588556e-05, contrastive loss 3.8242297172546387, task loss 0.07363420724868774
2025-01-08 00:00:52,115 - root - INFO - [Train] Epoch 6 [2000/35182]: Training loss 0.15679852664470673, current lr 9.697431852420342e-05, contrastive loss 3.5195155143737793, task loss 0.12160337716341019
2025-01-08 00:02:32,500 - root - INFO - [Train] Epoch 6 [2100/35182]: Training loss 0.13950082659721375, current lr 9.697289730252125e-05, contrastive loss 3.7805566787719727, task loss 0.10169526189565659
2025-01-08 00:04:13,090 - root - INFO - [Train] Epoch 6 [2200/35182]: Training loss 0.1282816380262375, current lr 9.69714760808391e-05, contrastive loss 3.673135995864868, task loss 0.09155027568340302
2025-01-08 00:05:53,466 - root - INFO - [Train] Epoch 6 [2300/35182]: Training loss 0.12019366025924683, current lr 9.697005485915693e-05, contrastive loss 3.9192581176757812, task loss 0.08100108057260513
2025-01-08 00:07:34,048 - root - INFO - [Train] Epoch 6 [2400/35182]: Training loss 0.1503516137599945, current lr 9.696863363747478e-05, contrastive loss 3.7511000633239746, task loss 0.11284061521291733
2025-01-08 00:09:14,444 - root - INFO - [Train] Epoch 6 [2500/35182]: Training loss 0.14996472001075745, current lr 9.696721241579262e-05, contrastive loss 3.7804813385009766, task loss 0.11215990036725998
2025-01-08 00:10:55,013 - root - INFO - [Train] Epoch 6 [2600/35182]: Training loss 0.17019835114479065, current lr 9.696579119411046e-05, contrastive loss 3.9431381225585938, task loss 0.13076697289943695
2025-01-08 00:12:35,398 - root - INFO - [Train] Epoch 6 [2700/35182]: Training loss 0.1437796652317047, current lr 9.69643699724283e-05, contrastive loss 3.7230470180511475, task loss 0.10654918849468231
2025-01-08 00:14:15,977 - root - INFO - [Train] Epoch 6 [2800/35182]: Training loss 0.14489534497261047, current lr 9.696294875074614e-05, contrastive loss 3.691126823425293, task loss 0.10798407346010208
2025-01-08 00:15:56,372 - root - INFO - [Train] Epoch 6 [2900/35182]: Training loss 0.14901837706565857, current lr 9.696152752906399e-05, contrastive loss 3.8079328536987305, task loss 0.11093905568122864
2025-01-08 00:17:36,952 - root - INFO - [Train] Epoch 6 [3000/35182]: Training loss 0.11195046454668045, current lr 9.696010630738182e-05, contrastive loss 3.7433571815490723, task loss 0.0745168924331665
2025-01-08 00:19:17,350 - root - INFO - [Train] Epoch 6 [3100/35182]: Training loss 0.16430161893367767, current lr 9.695868508569968e-05, contrastive loss 3.6866848468780518, task loss 0.12743477523326874
2025-01-08 00:20:58,029 - root - INFO - [Train] Epoch 6 [3200/35182]: Training loss 0.11487707495689392, current lr 9.695726386401751e-05, contrastive loss 3.864011764526367, task loss 0.07623695582151413
2025-01-08 00:22:38,427 - root - INFO - [Train] Epoch 6 [3300/35182]: Training loss 0.15301525592803955, current lr 9.695584264233535e-05, contrastive loss 3.685206890106201, task loss 0.11616318672895432
2025-01-08 00:24:18,990 - root - INFO - [Train] Epoch 6 [3400/35182]: Training loss 0.1779514104127884, current lr 9.69544214206532e-05, contrastive loss 3.886157512664795, task loss 0.13908983767032623
2025-01-08 00:25:59,373 - root - INFO - [Train] Epoch 6 [3500/35182]: Training loss 0.12152982503175735, current lr 9.695300019897104e-05, contrastive loss 3.758321762084961, task loss 0.0839466080069542
2025-01-08 00:27:39,963 - root - INFO - [Train] Epoch 6 [3600/35182]: Training loss 0.09823788702487946, current lr 9.695157897728888e-05, contrastive loss 3.6648666858673096, task loss 0.06158922240138054
2025-01-08 00:29:20,341 - root - INFO - [Train] Epoch 6 [3700/35182]: Training loss 0.1675201803445816, current lr 9.695015775560671e-05, contrastive loss 3.882981538772583, task loss 0.12869036197662354
2025-01-08 00:31:00,926 - root - INFO - [Train] Epoch 6 [3800/35182]: Training loss 0.13324406743049622, current lr 9.694873653392457e-05, contrastive loss 3.7681007385253906, task loss 0.09556306898593903
2025-01-08 00:32:41,324 - root - INFO - [Train] Epoch 6 [3900/35182]: Training loss 0.12687954306602478, current lr 9.69473153122424e-05, contrastive loss 3.779038429260254, task loss 0.08908916264772415
2025-01-08 00:34:21,890 - root - INFO - [Train] Epoch 6 [4000/35182]: Training loss 0.15653547644615173, current lr 9.694589409056026e-05, contrastive loss 3.7182328701019287, task loss 0.11935315281152725
2025-01-08 00:36:02,278 - root - INFO - [Train] Epoch 6 [4100/35182]: Training loss 0.1360345482826233, current lr 9.694447286887809e-05, contrastive loss 3.6290130615234375, task loss 0.09974440932273865
2025-01-08 00:37:42,856 - root - INFO - [Train] Epoch 6 [4200/35182]: Training loss 0.1330331563949585, current lr 9.694305164719594e-05, contrastive loss 3.634141206741333, task loss 0.09669174253940582
2025-01-08 00:39:23,234 - root - INFO - [Train] Epoch 6 [4300/35182]: Training loss 0.12393339723348618, current lr 9.694163042551377e-05, contrastive loss 3.692847728729248, task loss 0.08700492233037949
2025-01-08 00:41:03,820 - root - INFO - [Train] Epoch 6 [4400/35182]: Training loss 0.11696437746286392, current lr 9.694020920383162e-05, contrastive loss 3.666980028152466, task loss 0.08029457926750183
2025-01-08 00:42:44,207 - root - INFO - [Train] Epoch 6 [4500/35182]: Training loss 0.12822017073631287, current lr 9.693878798214946e-05, contrastive loss 3.60341215133667, task loss 0.09218604117631912
2025-01-08 00:44:24,595 - root - INFO - [Train] Epoch 6 [4600/35182]: Training loss 0.14605367183685303, current lr 9.69373667604673e-05, contrastive loss 3.930077075958252, task loss 0.1067528948187828
2025-01-08 00:46:05,182 - root - INFO - [Train] Epoch 6 [4700/35182]: Training loss 0.10159879922866821, current lr 9.693594553878515e-05, contrastive loss 3.7213821411132812, task loss 0.06438497453927994
2025-01-08 00:47:45,860 - root - INFO - [Train] Epoch 6 [4800/35182]: Training loss 0.12197545915842056, current lr 9.693452431710298e-05, contrastive loss 3.7968533039093018, task loss 0.08400692790746689
2025-01-08 00:49:26,249 - root - INFO - [Train] Epoch 6 [4900/35182]: Training loss 0.08706396818161011, current lr 9.693310309542083e-05, contrastive loss 3.8714499473571777, task loss 0.048349469900131226
2025-01-08 00:51:06,834 - root - INFO - [Train] Epoch 6 [5000/35182]: Training loss 0.16483378410339355, current lr 9.693168187373866e-05, contrastive loss 3.6488070487976074, task loss 0.12834571301937103
2025-01-08 00:52:47,225 - root - INFO - [Train] Epoch 6 [5100/35182]: Training loss 0.15176193416118622, current lr 9.693026065205652e-05, contrastive loss 3.7218284606933594, task loss 0.11454364657402039
2025-01-08 00:54:27,853 - root - INFO - [Train] Epoch 6 [5200/35182]: Training loss 0.09798826277256012, current lr 9.692883943037435e-05, contrastive loss 3.662003993988037, task loss 0.06136822700500488
2025-01-08 00:56:08,239 - root - INFO - [Train] Epoch 6 [5300/35182]: Training loss 0.1465388834476471, current lr 9.69274182086922e-05, contrastive loss 3.6644363403320312, task loss 0.1098945215344429
2025-01-08 00:57:48,873 - root - INFO - [Train] Epoch 6 [5400/35182]: Training loss 0.099602609872818, current lr 9.692599698701004e-05, contrastive loss 3.792422294616699, task loss 0.061678387224674225
2025-01-08 00:59:29,258 - root - INFO - [Train] Epoch 6 [5500/35182]: Training loss 0.15839101374149323, current lr 9.692457576532788e-05, contrastive loss 3.8410685062408447, task loss 0.1199803277850151
2025-01-08 01:01:09,850 - root - INFO - [Train] Epoch 6 [5600/35182]: Training loss 0.18294841051101685, current lr 9.692315454364572e-05, contrastive loss 3.7058300971984863, task loss 0.14589011669158936
2025-01-08 01:02:50,255 - root - INFO - [Train] Epoch 6 [5700/35182]: Training loss 0.15759611129760742, current lr 9.692173332196355e-05, contrastive loss 3.8145222663879395, task loss 0.1194508820772171
2025-01-08 01:04:30,931 - root - INFO - [Train] Epoch 6 [5800/35182]: Training loss 0.15656313300132751, current lr 9.692031210028141e-05, contrastive loss 3.846052408218384, task loss 0.11810261011123657
2025-01-08 01:06:11,326 - root - INFO - [Train] Epoch 6 [5900/35182]: Training loss 0.13339661061763763, current lr 9.691889087859924e-05, contrastive loss 3.8278279304504395, task loss 0.09511832892894745
2025-01-08 01:07:51,896 - root - INFO - [Train] Epoch 6 [6000/35182]: Training loss 0.21921515464782715, current lr 9.69174696569171e-05, contrastive loss 3.743791103363037, task loss 0.1817772537469864
2025-01-08 01:09:32,289 - root - INFO - [Train] Epoch 6 [6100/35182]: Training loss 0.12938973307609558, current lr 9.691604843523493e-05, contrastive loss 3.6807730197906494, task loss 0.09258200973272324
2025-01-08 01:11:12,953 - root - INFO - [Train] Epoch 6 [6200/35182]: Training loss 0.13071954250335693, current lr 9.691462721355278e-05, contrastive loss 3.7447280883789062, task loss 0.09327226877212524
2025-01-08 01:12:53,333 - root - INFO - [Train] Epoch 6 [6300/35182]: Training loss 0.15903861820697784, current lr 9.691320599187061e-05, contrastive loss 3.781648635864258, task loss 0.1212221309542656
2025-01-08 01:14:33,941 - root - INFO - [Train] Epoch 6 [6400/35182]: Training loss 0.1743358075618744, current lr 9.691178477018846e-05, contrastive loss 3.7043771743774414, task loss 0.1372920274734497
2025-01-08 01:16:14,321 - root - INFO - [Train] Epoch 6 [6500/35182]: Training loss 0.11541369557380676, current lr 9.69103635485063e-05, contrastive loss 3.8302204608917236, task loss 0.07711149007081985
2025-01-08 01:17:54,898 - root - INFO - [Train] Epoch 6 [6600/35182]: Training loss 0.1507464498281479, current lr 9.690894232682414e-05, contrastive loss 3.624258279800415, task loss 0.11450386792421341
2025-01-08 01:19:35,280 - root - INFO - [Train] Epoch 6 [6700/35182]: Training loss 0.1133088767528534, current lr 9.690752110514199e-05, contrastive loss 3.745903730392456, task loss 0.07584983855485916
2025-01-08 01:21:15,865 - root - INFO - [Train] Epoch 6 [6800/35182]: Training loss 0.13040560483932495, current lr 9.690609988345982e-05, contrastive loss 3.639617919921875, task loss 0.09400942176580429
2025-01-08 01:22:56,249 - root - INFO - [Train] Epoch 6 [6900/35182]: Training loss 0.13051392138004303, current lr 9.690467866177767e-05, contrastive loss 3.792889356613159, task loss 0.09258502721786499
2025-01-08 01:24:36,836 - root - INFO - [Train] Epoch 6 [7000/35182]: Training loss 0.12281350791454315, current lr 9.69032574400955e-05, contrastive loss 3.63271427154541, task loss 0.08648636192083359
2025-01-08 01:26:17,244 - root - INFO - [Train] Epoch 6 [7100/35182]: Training loss 0.1027892529964447, current lr 9.690183621841336e-05, contrastive loss 3.7057180404663086, task loss 0.06573207676410675
2025-01-08 01:27:57,913 - root - INFO - [Train] Epoch 6 [7200/35182]: Training loss 0.1683943122625351, current lr 9.690041499673119e-05, contrastive loss 3.597714424133301, task loss 0.13241717219352722
2025-01-08 01:29:38,310 - root - INFO - [Train] Epoch 6 [7300/35182]: Training loss 0.14120233058929443, current lr 9.689899377504903e-05, contrastive loss 3.6091225147247314, task loss 0.10511109977960587
2025-01-08 01:31:18,979 - root - INFO - [Train] Epoch 6 [7400/35182]: Training loss 0.14903518557548523, current lr 9.689757255336688e-05, contrastive loss 3.926637649536133, task loss 0.109768807888031
2025-01-08 01:32:59,375 - root - INFO - [Train] Epoch 6 [7500/35182]: Training loss 0.13927286863327026, current lr 9.689615133168472e-05, contrastive loss 3.7430500984191895, task loss 0.10184236615896225
2025-01-08 01:34:39,964 - root - INFO - [Train] Epoch 6 [7600/35182]: Training loss 0.10475212335586548, current lr 9.689473011000256e-05, contrastive loss 3.6548781394958496, task loss 0.06820333749055862
2025-01-08 01:36:20,374 - root - INFO - [Train] Epoch 6 [7700/35182]: Training loss 0.11521480977535248, current lr 9.68933088883204e-05, contrastive loss 3.741809368133545, task loss 0.07779671996831894
2025-01-08 01:38:01,052 - root - INFO - [Train] Epoch 6 [7800/35182]: Training loss 0.1362365335226059, current lr 9.689188766663825e-05, contrastive loss 3.692986488342285, task loss 0.09930667281150818
2025-01-08 01:39:41,452 - root - INFO - [Train] Epoch 6 [7900/35182]: Training loss 0.16334328055381775, current lr 9.689046644495608e-05, contrastive loss 3.7058796882629395, task loss 0.12628448009490967
2025-01-08 01:41:22,127 - root - INFO - [Train] Epoch 6 [8000/35182]: Training loss 0.10248032212257385, current lr 9.688904522327394e-05, contrastive loss 3.740093469619751, task loss 0.06507938355207443
2025-01-08 01:43:02,817 - root - INFO - [Train] Epoch 6 [8100/35182]: Training loss 0.13758797943592072, current lr 9.688762400159177e-05, contrastive loss 3.7026896476745605, task loss 0.10056108236312866
2025-01-08 01:44:43,209 - root - INFO - [Train] Epoch 6 [8200/35182]: Training loss 0.12850812077522278, current lr 9.688620277990962e-05, contrastive loss 3.743716239929199, task loss 0.09107095748186111
2025-01-08 01:46:23,598 - root - INFO - [Train] Epoch 6 [8300/35182]: Training loss 0.17004182934761047, current lr 9.688478155822745e-05, contrastive loss 3.8126060962677, task loss 0.13191576302051544
2025-01-08 01:48:04,186 - root - INFO - [Train] Epoch 6 [8400/35182]: Training loss 0.12962636351585388, current lr 9.68833603365453e-05, contrastive loss 3.921809673309326, task loss 0.09040826559066772
2025-01-08 01:49:44,857 - root - INFO - [Train] Epoch 6 [8500/35182]: Training loss 0.13215608894824982, current lr 9.688193911486314e-05, contrastive loss 3.6578643321990967, task loss 0.09557744860649109
2025-01-08 01:51:25,245 - root - INFO - [Train] Epoch 6 [8600/35182]: Training loss 0.1235591322183609, current lr 9.688051789318097e-05, contrastive loss 3.6825759410858154, task loss 0.086733378469944
2025-01-08 01:53:05,832 - root - INFO - [Train] Epoch 6 [8700/35182]: Training loss 0.1287107765674591, current lr 9.687909667149883e-05, contrastive loss 3.520346164703369, task loss 0.0935073122382164
2025-01-08 01:54:46,212 - root - INFO - [Train] Epoch 6 [8800/35182]: Training loss 0.0959634855389595, current lr 9.687767544981666e-05, contrastive loss 3.732320785522461, task loss 0.05864027887582779
2025-01-08 01:56:26,596 - root - INFO - [Train] Epoch 6 [8900/35182]: Training loss 0.12686532735824585, current lr 9.687625422813451e-05, contrastive loss 3.6486356258392334, task loss 0.09037896990776062
2025-01-08 01:58:07,182 - root - INFO - [Train] Epoch 6 [9000/35182]: Training loss 0.13889437913894653, current lr 9.687483300645234e-05, contrastive loss 3.53464412689209, task loss 0.10354794561862946
2025-01-08 01:59:47,567 - root - INFO - [Train] Epoch 6 [9100/35182]: Training loss 0.10314770042896271, current lr 9.68734117847702e-05, contrastive loss 3.531090497970581, task loss 0.06783679127693176
2025-01-08 02:01:28,151 - root - INFO - [Train] Epoch 6 [9200/35182]: Training loss 0.14196127653121948, current lr 9.687199056308803e-05, contrastive loss 3.73659086227417, task loss 0.1045953780412674
2025-01-08 02:03:08,539 - root - INFO - [Train] Epoch 6 [9300/35182]: Training loss 0.15903840959072113, current lr 9.687056934140587e-05, contrastive loss 3.5681068897247314, task loss 0.12335734069347382
2025-01-08 02:04:49,112 - root - INFO - [Train] Epoch 6 [9400/35182]: Training loss 0.1502450853586197, current lr 9.686914811972372e-05, contrastive loss 3.5498383045196533, task loss 0.11474670469760895
2025-01-08 02:06:29,501 - root - INFO - [Train] Epoch 6 [9500/35182]: Training loss 0.11178532242774963, current lr 9.686772689804156e-05, contrastive loss 3.6703920364379883, task loss 0.07508140802383423
2025-01-08 02:08:10,100 - root - INFO - [Train] Epoch 6 [9600/35182]: Training loss 0.12394826114177704, current lr 9.68663056763594e-05, contrastive loss 3.4532647132873535, task loss 0.08941560983657837
2025-01-08 02:09:50,486 - root - INFO - [Train] Epoch 6 [9700/35182]: Training loss 0.1311289668083191, current lr 9.686488445467723e-05, contrastive loss 3.6178488731384277, task loss 0.09495048224925995
2025-01-08 02:11:31,150 - root - INFO - [Train] Epoch 6 [9800/35182]: Training loss 0.10951056331396103, current lr 9.686346323299509e-05, contrastive loss 3.6154181957244873, task loss 0.07335638254880905
2025-01-08 02:13:11,550 - root - INFO - [Train] Epoch 6 [9900/35182]: Training loss 0.17473551630973816, current lr 9.686204201131292e-05, contrastive loss 3.682899236679077, task loss 0.1379065215587616
2025-01-08 02:14:52,140 - root - INFO - [Train] Epoch 6 [10000/35182]: Training loss 0.15112687647342682, current lr 9.686062078963078e-05, contrastive loss 3.897921323776245, task loss 0.11214766651391983
2025-01-08 02:16:32,822 - root - INFO - [Train] Epoch 6 [10100/35182]: Training loss 0.1442740261554718, current lr 9.685919956794861e-05, contrastive loss 3.595198631286621, task loss 0.10832204669713974
2025-01-08 02:18:13,205 - root - INFO - [Train] Epoch 6 [10200/35182]: Training loss 0.11176171898841858, current lr 9.685777834626647e-05, contrastive loss 3.763265609741211, task loss 0.07412905991077423
2025-01-08 02:19:53,604 - root - INFO - [Train] Epoch 6 [10300/35182]: Training loss 0.08557163178920746, current lr 9.68563571245843e-05, contrastive loss 3.6471877098083496, task loss 0.04909975826740265
2025-01-08 02:21:34,174 - root - INFO - [Train] Epoch 6 [10400/35182]: Training loss 0.18556895852088928, current lr 9.685493590290214e-05, contrastive loss 3.8237838745117188, task loss 0.1473311185836792
2025-01-08 02:23:14,865 - root - INFO - [Train] Epoch 6 [10500/35182]: Training loss 0.11990679800510406, current lr 9.685351468121998e-05, contrastive loss 3.715179443359375, task loss 0.08275499939918518
2025-01-08 02:24:55,255 - root - INFO - [Train] Epoch 6 [10600/35182]: Training loss 0.09887738525867462, current lr 9.685209345953781e-05, contrastive loss 3.6465582847595215, task loss 0.06241180747747421
2025-01-08 02:26:35,835 - root - INFO - [Train] Epoch 6 [10700/35182]: Training loss 0.13784120976924896, current lr 9.685067223785567e-05, contrastive loss 3.586745023727417, task loss 0.1019737645983696
2025-01-08 02:28:16,219 - root - INFO - [Train] Epoch 6 [10800/35182]: Training loss 0.10701145231723785, current lr 9.68492510161735e-05, contrastive loss 3.585395574569702, task loss 0.07115750014781952
2025-01-08 02:29:56,594 - root - INFO - [Train] Epoch 6 [10900/35182]: Training loss 0.15919573605060577, current lr 9.684782979449136e-05, contrastive loss 3.5874862670898438, task loss 0.12332087755203247
2025-01-08 02:31:37,186 - root - INFO - [Train] Epoch 6 [11000/35182]: Training loss 0.12401735782623291, current lr 9.684640857280919e-05, contrastive loss 3.6028480529785156, task loss 0.08798888325691223
2025-01-08 02:33:17,569 - root - INFO - [Train] Epoch 6 [11100/35182]: Training loss 0.13681739568710327, current lr 9.684498735112704e-05, contrastive loss 3.7455501556396484, task loss 0.09936188906431198
2025-01-08 02:34:58,149 - root - INFO - [Train] Epoch 6 [11200/35182]: Training loss 0.11164712905883789, current lr 9.684356612944487e-05, contrastive loss 3.6794729232788086, task loss 0.07485239952802658
2025-01-08 02:36:38,533 - root - INFO - [Train] Epoch 6 [11300/35182]: Training loss 0.1545630246400833, current lr 9.684214490776272e-05, contrastive loss 3.712289333343506, task loss 0.1174401342868805
2025-01-08 02:38:19,115 - root - INFO - [Train] Epoch 6 [11400/35182]: Training loss 0.11491187661886215, current lr 9.684072368608056e-05, contrastive loss 3.724679470062256, task loss 0.07766508311033249
2025-01-08 02:39:59,499 - root - INFO - [Train] Epoch 6 [11500/35182]: Training loss 0.15451902151107788, current lr 9.68393024643984e-05, contrastive loss 3.770637035369873, task loss 0.11681264638900757
2025-01-08 02:41:40,081 - root - INFO - [Train] Epoch 6 [11600/35182]: Training loss 0.12764210999011993, current lr 9.683788124271625e-05, contrastive loss 3.811396598815918, task loss 0.0895281434059143
2025-01-08 02:43:20,471 - root - INFO - [Train] Epoch 6 [11700/35182]: Training loss 0.1358337700366974, current lr 9.683646002103408e-05, contrastive loss 3.645101308822632, task loss 0.09938275068998337
2025-01-08 02:45:01,057 - root - INFO - [Train] Epoch 6 [11800/35182]: Training loss 0.09749932587146759, current lr 9.683503879935193e-05, contrastive loss 3.596236228942871, task loss 0.061536964029073715
2025-01-08 02:46:41,458 - root - INFO - [Train] Epoch 6 [11900/35182]: Training loss 0.19571280479431152, current lr 9.683361757766976e-05, contrastive loss 3.666848659515381, task loss 0.1590443253517151
2025-01-08 02:48:22,119 - root - INFO - [Train] Epoch 6 [12000/35182]: Training loss 0.14818809926509857, current lr 9.683219635598762e-05, contrastive loss 3.746572494506836, task loss 0.1107223704457283
2025-01-08 02:50:02,522 - root - INFO - [Train] Epoch 6 [12100/35182]: Training loss 0.13348889350891113, current lr 9.683077513430545e-05, contrastive loss 3.7307167053222656, task loss 0.096181720495224
2025-01-08 02:51:43,090 - root - INFO - [Train] Epoch 6 [12200/35182]: Training loss 0.09594178199768066, current lr 9.68293539126233e-05, contrastive loss 3.7596945762634277, task loss 0.05834484100341797
2025-01-08 02:53:23,483 - root - INFO - [Train] Epoch 6 [12300/35182]: Training loss 0.1785307079553604, current lr 9.682793269094114e-05, contrastive loss 3.5829997062683105, task loss 0.14270071685314178
2025-01-08 02:55:04,071 - root - INFO - [Train] Epoch 6 [12400/35182]: Training loss 0.13369250297546387, current lr 9.682651146925898e-05, contrastive loss 3.7763659954071045, task loss 0.09592884033918381
2025-01-08 02:56:44,446 - root - INFO - [Train] Epoch 6 [12500/35182]: Training loss 0.1805904507637024, current lr 9.682509024757682e-05, contrastive loss 3.349930763244629, task loss 0.14709113538265228
2025-01-08 02:58:25,021 - root - INFO - [Train] Epoch 6 [12600/35182]: Training loss 0.09893818199634552, current lr 9.682366902589465e-05, contrastive loss 3.562793493270874, task loss 0.06331024318933487
2025-01-08 03:00:05,427 - root - INFO - [Train] Epoch 6 [12700/35182]: Training loss 0.12911781668663025, current lr 9.682224780421251e-05, contrastive loss 3.421257257461548, task loss 0.09490524977445602
2025-01-08 03:01:46,107 - root - INFO - [Train] Epoch 6 [12800/35182]: Training loss 0.1416301131248474, current lr 9.682082658253034e-05, contrastive loss 3.8080480098724365, task loss 0.10354962944984436
2025-01-08 03:03:26,484 - root - INFO - [Train] Epoch 6 [12900/35182]: Training loss 0.10610581934452057, current lr 9.68194053608482e-05, contrastive loss 3.733543872833252, task loss 0.0687703862786293
2025-01-08 03:05:07,076 - root - INFO - [Train] Epoch 6 [13000/35182]: Training loss 0.12826037406921387, current lr 9.681798413916603e-05, contrastive loss 3.618870258331299, task loss 0.09207167476415634
2025-01-08 03:06:47,478 - root - INFO - [Train] Epoch 6 [13100/35182]: Training loss 0.14486190676689148, current lr 9.681656291748388e-05, contrastive loss 3.655460834503174, task loss 0.10830730199813843
2025-01-08 03:08:28,150 - root - INFO - [Train] Epoch 6 [13200/35182]: Training loss 0.18498271703720093, current lr 9.681514169580171e-05, contrastive loss 3.4953250885009766, task loss 0.15002946555614471
2025-01-08 03:10:08,839 - root - INFO - [Train] Epoch 6 [13300/35182]: Training loss 0.11572262644767761, current lr 9.681372047411956e-05, contrastive loss 3.7271907329559326, task loss 0.07845072448253632
2025-01-08 03:11:49,228 - root - INFO - [Train] Epoch 6 [13400/35182]: Training loss 0.10393728315830231, current lr 9.68122992524374e-05, contrastive loss 3.7376279830932617, task loss 0.0665610060095787
2025-01-08 03:13:29,811 - root - INFO - [Train] Epoch 6 [13500/35182]: Training loss 0.16656288504600525, current lr 9.681087803075524e-05, contrastive loss 3.567816972732544, task loss 0.13088470697402954
2025-01-08 03:15:10,207 - root - INFO - [Train] Epoch 6 [13600/35182]: Training loss 0.16325652599334717, current lr 9.680945680907309e-05, contrastive loss 3.7818498611450195, task loss 0.12543803453445435
2025-01-08 03:16:50,885 - root - INFO - [Train] Epoch 6 [13700/35182]: Training loss 0.1005660891532898, current lr 9.680803558739092e-05, contrastive loss 3.834885358810425, task loss 0.06221723183989525
2025-01-08 03:18:31,289 - root - INFO - [Train] Epoch 6 [13800/35182]: Training loss 0.16641977429389954, current lr 9.680661436570877e-05, contrastive loss 3.769695281982422, task loss 0.12872281670570374
2025-01-08 03:20:11,857 - root - INFO - [Train] Epoch 6 [13900/35182]: Training loss 0.18001732230186462, current lr 9.68051931440266e-05, contrastive loss 3.6100192070007324, task loss 0.14391712844371796
2025-01-08 03:21:52,243 - root - INFO - [Train] Epoch 6 [14000/35182]: Training loss 0.15850478410720825, current lr 9.680377192234446e-05, contrastive loss 3.717055320739746, task loss 0.12133423984050751
2025-01-08 03:23:32,939 - root - INFO - [Train] Epoch 6 [14100/35182]: Training loss 0.1550172120332718, current lr 9.680235070066229e-05, contrastive loss 3.6211910247802734, task loss 0.11880530416965485
2025-01-08 03:25:13,354 - root - INFO - [Train] Epoch 6 [14200/35182]: Training loss 0.13575208187103271, current lr 9.680092947898015e-05, contrastive loss 3.675020217895508, task loss 0.09900188446044922
2025-01-08 03:26:54,018 - root - INFO - [Train] Epoch 6 [14300/35182]: Training loss 0.13222455978393555, current lr 9.679950825729798e-05, contrastive loss 3.7186803817749023, task loss 0.09503776580095291
2025-01-08 03:28:34,415 - root - INFO - [Train] Epoch 6 [14400/35182]: Training loss 0.14804036915302277, current lr 9.679808703561582e-05, contrastive loss 3.5630745887756348, task loss 0.11240962892770767
2025-01-08 03:30:14,988 - root - INFO - [Train] Epoch 6 [14500/35182]: Training loss 0.16499146819114685, current lr 9.679666581393366e-05, contrastive loss 3.8483448028564453, task loss 0.12650801241397858
2025-01-08 03:31:55,374 - root - INFO - [Train] Epoch 6 [14600/35182]: Training loss 0.17369885742664337, current lr 9.679524459225149e-05, contrastive loss 3.8826098442077637, task loss 0.13487276434898376
2025-01-08 03:33:36,045 - root - INFO - [Train] Epoch 6 [14700/35182]: Training loss 0.10671618580818176, current lr 9.679382337056935e-05, contrastive loss 3.6113505363464355, task loss 0.07060267776250839
2025-01-08 03:35:16,442 - root - INFO - [Train] Epoch 6 [14800/35182]: Training loss 0.10556894540786743, current lr 9.679240214888718e-05, contrastive loss 3.6975910663604736, task loss 0.06859303265810013
2025-01-08 03:36:57,031 - root - INFO - [Train] Epoch 6 [14900/35182]: Training loss 0.09203562140464783, current lr 9.679098092720504e-05, contrastive loss 3.876981735229492, task loss 0.05326581001281738
2025-01-08 03:38:37,435 - root - INFO - [Train] Epoch 6 [15000/35182]: Training loss 0.09124839305877686, current lr 9.678955970552287e-05, contrastive loss 3.7013254165649414, task loss 0.05423513427376747
2025-01-08 03:40:18,118 - root - INFO - [Train] Epoch 6 [15100/35182]: Training loss 0.15060651302337646, current lr 9.678813848384072e-05, contrastive loss 3.6875534057617188, task loss 0.11373098194599152
2025-01-08 03:41:58,802 - root - INFO - [Train] Epoch 6 [15200/35182]: Training loss 0.1282193809747696, current lr 9.678671726215855e-05, contrastive loss 3.742457628250122, task loss 0.09079480171203613
2025-01-08 03:43:39,211 - root - INFO - [Train] Epoch 6 [15300/35182]: Training loss 0.14300289750099182, current lr 9.67852960404764e-05, contrastive loss 3.5729501247406006, task loss 0.1072733998298645
2025-01-08 03:45:19,891 - root - INFO - [Train] Epoch 6 [15400/35182]: Training loss 0.14745822548866272, current lr 9.678387481879424e-05, contrastive loss 3.6089749336242676, task loss 0.11136848479509354
2025-01-08 03:47:00,293 - root - INFO - [Train] Epoch 6 [15500/35182]: Training loss 0.15574726462364197, current lr 9.678245359711208e-05, contrastive loss 3.7884764671325684, task loss 0.11786250025033951
2025-01-08 03:48:40,976 - root - INFO - [Train] Epoch 6 [15600/35182]: Training loss 0.10597851872444153, current lr 9.678103237542993e-05, contrastive loss 3.6759159564971924, task loss 0.06921935826539993
2025-01-08 03:50:21,368 - root - INFO - [Train] Epoch 6 [15700/35182]: Training loss 0.1349775493144989, current lr 9.677961115374776e-05, contrastive loss 3.7680397033691406, task loss 0.09729714691638947
2025-01-08 03:52:02,032 - root - INFO - [Train] Epoch 6 [15800/35182]: Training loss 0.1326025277376175, current lr 9.677818993206561e-05, contrastive loss 3.605404853820801, task loss 0.09654848277568817
2025-01-08 03:53:42,440 - root - INFO - [Train] Epoch 6 [15900/35182]: Training loss 0.17512859404087067, current lr 9.677676871038344e-05, contrastive loss 3.7251245975494385, task loss 0.13787734508514404
2025-01-08 03:55:23,019 - root - INFO - [Train] Epoch 6 [16000/35182]: Training loss 0.12215597927570343, current lr 9.67753474887013e-05, contrastive loss 3.740600109100342, task loss 0.0847499743103981
2025-01-08 03:57:03,436 - root - INFO - [Train] Epoch 6 [16100/35182]: Training loss 0.17363476753234863, current lr 9.677392626701913e-05, contrastive loss 3.6803252696990967, task loss 0.13683150708675385
2025-01-08 03:58:44,111 - root - INFO - [Train] Epoch 6 [16200/35182]: Training loss 0.10961734503507614, current lr 9.677250504533699e-05, contrastive loss 3.577042579650879, task loss 0.07384692132472992
2025-01-08 04:00:24,507 - root - INFO - [Train] Epoch 6 [16300/35182]: Training loss 0.12191864848136902, current lr 9.677108382365482e-05, contrastive loss 3.694946765899658, task loss 0.0849691852927208
2025-01-08 04:02:05,180 - root - INFO - [Train] Epoch 6 [16400/35182]: Training loss 0.10071368515491486, current lr 9.676966260197266e-05, contrastive loss 3.6542625427246094, task loss 0.06417106091976166
2025-01-08 04:03:45,748 - root - INFO - [Train] Epoch 6 [16500/35182]: Training loss 0.10062269866466522, current lr 9.67682413802905e-05, contrastive loss 3.6768107414245605, task loss 0.06385459005832672
2025-01-08 04:05:26,149 - root - INFO - [Train] Epoch 6 [16600/35182]: Training loss 0.1024022251367569, current lr 9.676682015860833e-05, contrastive loss 3.4998254776000977, task loss 0.06740397214889526
2025-01-08 04:07:06,532 - root - INFO - [Train] Epoch 6 [16700/35182]: Training loss 0.1713123470544815, current lr 9.676539893692619e-05, contrastive loss 3.7072958946228027, task loss 0.13423939049243927
2025-01-08 04:08:47,115 - root - INFO - [Train] Epoch 6 [16800/35182]: Training loss 0.11665178835391998, current lr 9.676397771524402e-05, contrastive loss 3.6410439014434814, task loss 0.08024135231971741
2025-01-08 04:10:27,797 - root - INFO - [Train] Epoch 6 [16900/35182]: Training loss 0.11918352544307709, current lr 9.676255649356188e-05, contrastive loss 3.488661050796509, task loss 0.08429691940546036
2025-01-08 04:12:08,204 - root - INFO - [Train] Epoch 6 [17000/35182]: Training loss 0.11227428913116455, current lr 9.676113527187971e-05, contrastive loss 3.5131783485412598, task loss 0.07714250683784485
2025-01-08 04:13:48,876 - root - INFO - [Train] Epoch 6 [17100/35182]: Training loss 0.207962766289711, current lr 9.675971405019756e-05, contrastive loss 3.5934205055236816, task loss 0.1720285564661026
2025-01-08 04:15:29,254 - root - INFO - [Train] Epoch 6 [17200/35182]: Training loss 0.10511649399995804, current lr 9.67582928285154e-05, contrastive loss 3.673063278198242, task loss 0.06838586181402206
2025-01-08 04:17:09,846 - root - INFO - [Train] Epoch 6 [17300/35182]: Training loss 0.14989475905895233, current lr 9.675687160683324e-05, contrastive loss 3.660871982574463, task loss 0.11328604072332382
2025-01-08 04:18:50,230 - root - INFO - [Train] Epoch 6 [17400/35182]: Training loss 0.09850867092609406, current lr 9.675545038515108e-05, contrastive loss 3.690514087677002, task loss 0.06160352751612663
2025-01-08 04:20:30,815 - root - INFO - [Train] Epoch 6 [17500/35182]: Training loss 0.1309383064508438, current lr 9.675402916346892e-05, contrastive loss 3.809282064437866, task loss 0.09284548461437225
2025-01-08 07:06:34,710 - root - INFO - [Valid] Evaluation Results: HR@5:0.8037,NDCG@5:0.7395,HR@10:0.8616,NDCG@10:0.7583,HR@20:0.9165,NDCG@20:0.7722
2025-01-08 07:06:35,563 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-08 07:06:45,602 - root - INFO - [Train] Epoch 6 [17600/35182]: Training loss 0.10226082801818848, current lr 9.675260794178677e-05, contrastive loss 3.551128387451172, task loss 0.06674954295158386
2025-01-08 07:08:26,138 - root - INFO - [Train] Epoch 6 [17700/35182]: Training loss 0.11297859251499176, current lr 9.67511867201046e-05, contrastive loss 3.6912312507629395, task loss 0.0760662779211998
2025-01-08 07:10:06,533 - root - INFO - [Train] Epoch 6 [17800/35182]: Training loss 0.14728817343711853, current lr 9.674976549842245e-05, contrastive loss 3.593863010406494, task loss 0.11134955286979675
2025-01-08 07:11:47,126 - root - INFO - [Train] Epoch 6 [17900/35182]: Training loss 0.11077693104743958, current lr 9.674834427674028e-05, contrastive loss 3.606505870819092, task loss 0.074711874127388
2025-01-08 07:13:27,797 - root - INFO - [Train] Epoch 6 [18000/35182]: Training loss 0.17980647087097168, current lr 9.674692305505814e-05, contrastive loss 3.7497401237487793, task loss 0.1423090696334839
2025-01-08 07:15:08,196 - root - INFO - [Train] Epoch 6 [18100/35182]: Training loss 0.13277548551559448, current lr 9.674550183337597e-05, contrastive loss 3.6059722900390625, task loss 0.0967157632112503
2025-01-08 07:16:48,878 - root - INFO - [Train] Epoch 6 [18200/35182]: Training loss 0.15513098239898682, current lr 9.674408061169383e-05, contrastive loss 3.687680244445801, task loss 0.11825418472290039
2025-01-08 07:18:29,276 - root - INFO - [Train] Epoch 6 [18300/35182]: Training loss 0.13965275883674622, current lr 9.674265939001166e-05, contrastive loss 3.732473373413086, task loss 0.10232803225517273
2025-01-08 07:20:09,966 - root - INFO - [Train] Epoch 6 [18400/35182]: Training loss 0.1714964359998703, current lr 9.67412381683295e-05, contrastive loss 3.782172918319702, task loss 0.1336747109889984
2025-01-08 07:21:50,352 - root - INFO - [Train] Epoch 6 [18500/35182]: Training loss 0.16629137098789215, current lr 9.673981694664734e-05, contrastive loss 3.6501898765563965, task loss 0.12978947162628174
2025-01-08 07:23:30,928 - root - INFO - [Train] Epoch 6 [18600/35182]: Training loss 0.11985316872596741, current lr 9.673839572496517e-05, contrastive loss 3.6945977210998535, task loss 0.08290719240903854
2025-01-08 07:25:11,306 - root - INFO - [Train] Epoch 6 [18700/35182]: Training loss 0.19478148221969604, current lr 9.673697450328303e-05, contrastive loss 3.7322635650634766, task loss 0.1574588418006897
2025-01-08 07:26:51,985 - root - INFO - [Train] Epoch 6 [18800/35182]: Training loss 0.191189706325531, current lr 9.673555328160086e-05, contrastive loss 3.610091209411621, task loss 0.15508879721164703
2025-01-08 07:28:32,383 - root - INFO - [Train] Epoch 6 [18900/35182]: Training loss 0.15124425292015076, current lr 9.673413205991872e-05, contrastive loss 3.7065000534057617, task loss 0.11417924612760544
2025-01-08 07:30:12,977 - root - INFO - [Train] Epoch 6 [19000/35182]: Training loss 0.08528824150562286, current lr 9.673271083823655e-05, contrastive loss 3.8207130432128906, task loss 0.0470811165869236
2025-01-08 07:31:53,373 - root - INFO - [Train] Epoch 6 [19100/35182]: Training loss 0.15142947435379028, current lr 9.67312896165544e-05, contrastive loss 3.956319570541382, task loss 0.11186627298593521
2025-01-08 07:33:33,944 - root - INFO - [Train] Epoch 6 [19200/35182]: Training loss 0.0827094316482544, current lr 9.672986839487223e-05, contrastive loss 3.7528276443481445, task loss 0.04518115893006325
2025-01-08 07:35:14,334 - root - INFO - [Train] Epoch 6 [19300/35182]: Training loss 0.1339912861585617, current lr 9.672844717319008e-05, contrastive loss 3.700155258178711, task loss 0.09698973596096039
2025-01-08 07:36:54,910 - root - INFO - [Train] Epoch 6 [19400/35182]: Training loss 0.12873761355876923, current lr 9.672702595150792e-05, contrastive loss 3.8118767738342285, task loss 0.09061884880065918
2025-01-08 07:38:35,299 - root - INFO - [Train] Epoch 6 [19500/35182]: Training loss 0.1290319263935089, current lr 9.672560472982576e-05, contrastive loss 3.5885744094848633, task loss 0.0931461900472641
2025-01-08 07:40:15,872 - root - INFO - [Train] Epoch 6 [19600/35182]: Training loss 0.08125057071447372, current lr 9.672418350814361e-05, contrastive loss 3.7718749046325684, task loss 0.04353182390332222
2025-01-08 07:41:56,263 - root - INFO - [Train] Epoch 6 [19700/35182]: Training loss 0.14186832308769226, current lr 9.672276228646144e-05, contrastive loss 3.547032356262207, task loss 0.10639800876379013
2025-01-08 07:43:36,839 - root - INFO - [Train] Epoch 6 [19800/35182]: Training loss 0.12051402032375336, current lr 9.67213410647793e-05, contrastive loss 3.733222484588623, task loss 0.08318179100751877
2025-01-08 07:45:17,232 - root - INFO - [Train] Epoch 6 [19900/35182]: Training loss 0.14414191246032715, current lr 9.671991984309712e-05, contrastive loss 3.5912554264068604, task loss 0.10822935402393341
2025-01-08 07:46:57,918 - root - INFO - [Train] Epoch 6 [20000/35182]: Training loss 0.08184588700532913, current lr 9.671849862141498e-05, contrastive loss 3.6483211517333984, task loss 0.0453626774251461
2025-01-08 07:48:38,300 - root - INFO - [Train] Epoch 6 [20100/35182]: Training loss 0.15079613029956818, current lr 9.671707739973281e-05, contrastive loss 3.8483729362487793, task loss 0.11231240630149841
2025-01-08 07:50:18,882 - root - INFO - [Train] Epoch 6 [20200/35182]: Training loss 0.13831649720668793, current lr 9.671565617805067e-05, contrastive loss 3.660219669342041, task loss 0.10171430557966232
2025-01-08 07:51:59,279 - root - INFO - [Train] Epoch 6 [20300/35182]: Training loss 0.10242370516061783, current lr 9.67142349563685e-05, contrastive loss 3.7399120330810547, task loss 0.06502458453178406
2025-01-08 07:53:39,970 - root - INFO - [Train] Epoch 6 [20400/35182]: Training loss 0.11501666903495789, current lr 9.671281373468634e-05, contrastive loss 3.8357601165771484, task loss 0.07665906846523285
2025-01-08 07:55:20,382 - root - INFO - [Train] Epoch 6 [20500/35182]: Training loss 0.11244994401931763, current lr 9.671139251300419e-05, contrastive loss 3.7688968181610107, task loss 0.07476098090410233
2025-01-08 07:57:01,039 - root - INFO - [Train] Epoch 6 [20600/35182]: Training loss 0.147836834192276, current lr 9.670997129132201e-05, contrastive loss 3.725205898284912, task loss 0.11058478057384491
2025-01-08 07:58:41,420 - root - INFO - [Train] Epoch 6 [20700/35182]: Training loss 0.15943677723407745, current lr 9.670855006963987e-05, contrastive loss 3.6994454860687256, task loss 0.1224423199892044
2025-01-08 08:00:22,013 - root - INFO - [Train] Epoch 6 [20800/35182]: Training loss 0.1303013563156128, current lr 9.67071288479577e-05, contrastive loss 3.622777223587036, task loss 0.0940735936164856
2025-01-08 08:02:02,409 - root - INFO - [Train] Epoch 6 [20900/35182]: Training loss 0.14021670818328857, current lr 9.670570762627556e-05, contrastive loss 3.7628495693206787, task loss 0.1025882214307785
2025-01-08 08:03:42,985 - root - INFO - [Train] Epoch 6 [21000/35182]: Training loss 0.13491421937942505, current lr 9.670428640459339e-05, contrastive loss 3.789600372314453, task loss 0.09701821953058243
2025-01-08 08:05:23,392 - root - INFO - [Train] Epoch 6 [21100/35182]: Training loss 0.15657863020896912, current lr 9.670286518291125e-05, contrastive loss 3.6388044357299805, task loss 0.1201905906200409
2025-01-08 08:07:04,061 - root - INFO - [Train] Epoch 6 [21200/35182]: Training loss 0.1417691707611084, current lr 9.670144396122908e-05, contrastive loss 3.7450549602508545, task loss 0.10431861877441406
2025-01-08 08:08:44,452 - root - INFO - [Train] Epoch 6 [21300/35182]: Training loss 0.09927146136760712, current lr 9.670002273954692e-05, contrastive loss 3.5815248489379883, task loss 0.0634562149643898
2025-01-08 08:10:25,020 - root - INFO - [Train] Epoch 6 [21400/35182]: Training loss 0.14351946115493774, current lr 9.669860151786476e-05, contrastive loss 3.640960216522217, task loss 0.10710985213518143
2025-01-08 08:12:05,401 - root - INFO - [Train] Epoch 6 [21500/35182]: Training loss 0.11302636563777924, current lr 9.66971802961826e-05, contrastive loss 3.6565890312194824, task loss 0.07646047323942184
2025-01-08 08:13:45,986 - root - INFO - [Train] Epoch 6 [21600/35182]: Training loss 0.1674528419971466, current lr 9.669575907450045e-05, contrastive loss 3.824108600616455, task loss 0.12921175360679626
2025-01-08 08:15:26,373 - root - INFO - [Train] Epoch 6 [21700/35182]: Training loss 0.14602060616016388, current lr 9.669433785281828e-05, contrastive loss 3.648545742034912, task loss 0.10953515022993088
2025-01-08 08:17:06,951 - root - INFO - [Train] Epoch 6 [21800/35182]: Training loss 0.10172019898891449, current lr 9.669291663113614e-05, contrastive loss 3.773923397064209, task loss 0.06398096680641174
2025-01-08 08:18:47,332 - root - INFO - [Train] Epoch 6 [21900/35182]: Training loss 0.17158010601997375, current lr 9.669149540945397e-05, contrastive loss 3.6467607021331787, task loss 0.13511249423027039
2025-01-08 08:20:27,931 - root - INFO - [Train] Epoch 6 [22000/35182]: Training loss 0.10619279742240906, current lr 9.669007418777182e-05, contrastive loss 3.706120491027832, task loss 0.06913159787654877
2025-01-08 08:22:08,320 - root - INFO - [Train] Epoch 6 [22100/35182]: Training loss 0.07421603798866272, current lr 9.668865296608965e-05, contrastive loss 3.7070231437683105, task loss 0.03714580461382866
2025-01-08 08:23:48,998 - root - INFO - [Train] Epoch 6 [22200/35182]: Training loss 0.12537942826747894, current lr 9.668723174440751e-05, contrastive loss 3.847975969314575, task loss 0.08689966797828674
2025-01-08 08:25:29,378 - root - INFO - [Train] Epoch 6 [22300/35182]: Training loss 0.12145163118839264, current lr 9.668581052272534e-05, contrastive loss 3.631845712661743, task loss 0.08513317257165909
2025-01-08 08:27:09,959 - root - INFO - [Train] Epoch 6 [22400/35182]: Training loss 0.14298206567764282, current lr 9.668438930104318e-05, contrastive loss 3.732239246368408, task loss 0.10565967857837677
2025-01-08 08:28:50,351 - root - INFO - [Train] Epoch 6 [22500/35182]: Training loss 0.15105298161506653, current lr 9.668296807936103e-05, contrastive loss 3.7294068336486816, task loss 0.11375891417264938
2025-01-08 08:30:30,928 - root - INFO - [Train] Epoch 6 [22600/35182]: Training loss 0.14368994534015656, current lr 9.668154685767886e-05, contrastive loss 3.5970895290374756, task loss 0.1077190488576889
2025-01-08 08:32:11,316 - root - INFO - [Train] Epoch 6 [22700/35182]: Training loss 0.13766080141067505, current lr 9.668012563599671e-05, contrastive loss 3.780641555786133, task loss 0.09985439479351044
2025-01-08 08:33:51,894 - root - INFO - [Train] Epoch 6 [22800/35182]: Training loss 0.12121263891458511, current lr 9.667870441431454e-05, contrastive loss 3.713744878768921, task loss 0.0840751901268959
2025-01-08 08:35:32,272 - root - INFO - [Train] Epoch 6 [22900/35182]: Training loss 0.16037751734256744, current lr 9.66772831926324e-05, contrastive loss 3.601978302001953, task loss 0.12435773760080338
2025-01-08 08:37:12,972 - root - INFO - [Train] Epoch 6 [23000/35182]: Training loss 0.151018425822258, current lr 9.667586197095023e-05, contrastive loss 3.690077781677246, task loss 0.11411764472723007
2025-01-08 08:38:53,364 - root - INFO - [Train] Epoch 6 [23100/35182]: Training loss 0.10021716356277466, current lr 9.667444074926809e-05, contrastive loss 3.7234134674072266, task loss 0.06298302859067917
2025-01-08 08:40:33,945 - root - INFO - [Train] Epoch 6 [23200/35182]: Training loss 0.12865780293941498, current lr 9.667301952758592e-05, contrastive loss 3.827547788619995, task loss 0.09038233011960983
2025-01-08 08:42:14,331 - root - INFO - [Train] Epoch 6 [23300/35182]: Training loss 0.11850008368492126, current lr 9.667159830590376e-05, contrastive loss 3.5529708862304688, task loss 0.08297038078308105
2025-01-08 08:43:54,903 - root - INFO - [Train] Epoch 6 [23400/35182]: Training loss 0.1457388997077942, current lr 9.66701770842216e-05, contrastive loss 3.674389600753784, task loss 0.1089949980378151
2025-01-08 08:45:35,302 - root - INFO - [Train] Epoch 6 [23500/35182]: Training loss 0.1587221771478653, current lr 9.666875586253945e-05, contrastive loss 3.592193603515625, task loss 0.12280023843050003
2025-01-08 08:47:15,989 - root - INFO - [Train] Epoch 6 [23600/35182]: Training loss 0.13871552050113678, current lr 9.666733464085729e-05, contrastive loss 3.6190319061279297, task loss 0.10252520442008972
2025-01-08 08:48:56,386 - root - INFO - [Train] Epoch 6 [23700/35182]: Training loss 0.15682873129844666, current lr 9.666591341917512e-05, contrastive loss 3.619025707244873, task loss 0.12063848227262497
2025-01-08 08:50:37,067 - root - INFO - [Train] Epoch 6 [23800/35182]: Training loss 0.13651853799819946, current lr 9.666449219749298e-05, contrastive loss 3.6057896614074707, task loss 0.10046064108610153
2025-01-08 08:52:17,462 - root - INFO - [Train] Epoch 6 [23900/35182]: Training loss 0.10264986753463745, current lr 9.66630709758108e-05, contrastive loss 3.7403159141540527, task loss 0.06524670869112015
2025-01-08 08:53:58,053 - root - INFO - [Train] Epoch 6 [24000/35182]: Training loss 0.09712086617946625, current lr 9.666164975412866e-05, contrastive loss 3.47050142288208, task loss 0.06241585686802864
2025-01-08 08:55:38,446 - root - INFO - [Train] Epoch 6 [24100/35182]: Training loss 0.11430491507053375, current lr 9.666022853244649e-05, contrastive loss 3.8215079307556152, task loss 0.07608983665704727
2025-01-08 08:57:19,122 - root - INFO - [Train] Epoch 6 [24200/35182]: Training loss 0.10875584185123444, current lr 9.665880731076434e-05, contrastive loss 3.749180316925049, task loss 0.07126404345035553
2025-01-08 08:58:59,509 - root - INFO - [Train] Epoch 6 [24300/35182]: Training loss 0.13565793633460999, current lr 9.665738608908218e-05, contrastive loss 3.6131458282470703, task loss 0.09952647238969803
2025-01-08 09:00:40,083 - root - INFO - [Train] Epoch 6 [24400/35182]: Training loss 0.14632469415664673, current lr 9.665596486740002e-05, contrastive loss 3.8496897220611572, task loss 0.1078277975320816
2025-01-08 09:02:20,482 - root - INFO - [Train] Epoch 6 [24500/35182]: Training loss 0.10524106025695801, current lr 9.665454364571787e-05, contrastive loss 3.60331130027771, task loss 0.06920795142650604
2025-01-08 09:04:01,162 - root - INFO - [Train] Epoch 6 [24600/35182]: Training loss 0.1294526308774948, current lr 9.66531224240357e-05, contrastive loss 3.6421422958374023, task loss 0.09303121268749237
2025-01-08 09:05:41,551 - root - INFO - [Train] Epoch 6 [24700/35182]: Training loss 0.21615688502788544, current lr 9.665170120235355e-05, contrastive loss 3.7490787506103516, task loss 0.1786660999059677
2025-01-08 09:07:22,132 - root - INFO - [Train] Epoch 6 [24800/35182]: Training loss 0.14767077565193176, current lr 9.665027998067138e-05, contrastive loss 3.6752798557281494, task loss 0.11091797053813934
2025-01-08 09:09:02,823 - root - INFO - [Train] Epoch 6 [24900/35182]: Training loss 0.1806592047214508, current lr 9.664885875898924e-05, contrastive loss 3.6463756561279297, task loss 0.14419545233249664
2025-01-08 09:10:43,215 - root - INFO - [Train] Epoch 6 [25000/35182]: Training loss 0.1704787015914917, current lr 9.664743753730707e-05, contrastive loss 3.561952590942383, task loss 0.13485917448997498
2025-01-08 09:12:23,883 - root - INFO - [Train] Epoch 6 [25100/35182]: Training loss 0.15453973412513733, current lr 9.664601631562493e-05, contrastive loss 3.65899920463562, task loss 0.11794974654912949
2025-01-08 09:14:04,268 - root - INFO - [Train] Epoch 6 [25200/35182]: Training loss 0.1561727374792099, current lr 9.664459509394276e-05, contrastive loss 3.648545742034912, task loss 0.1196872815489769
2025-01-08 09:15:44,873 - root - INFO - [Train] Epoch 6 [25300/35182]: Training loss 0.12546971440315247, current lr 9.66431738722606e-05, contrastive loss 3.6018800735473633, task loss 0.0894509106874466
2025-01-08 09:17:25,256 - root - INFO - [Train] Epoch 6 [25400/35182]: Training loss 0.0813630223274231, current lr 9.664175265057844e-05, contrastive loss 3.8179821968078613, task loss 0.04318320378661156
2025-01-08 09:19:05,843 - root - INFO - [Train] Epoch 6 [25500/35182]: Training loss 0.15029959380626678, current lr 9.664033142889629e-05, contrastive loss 3.695568084716797, task loss 0.11334390938282013
2025-01-08 09:20:46,240 - root - INFO - [Train] Epoch 6 [25600/35182]: Training loss 0.1459503024816513, current lr 9.663891020721413e-05, contrastive loss 3.7292842864990234, task loss 0.10865745693445206
2025-01-08 09:22:26,904 - root - INFO - [Train] Epoch 6 [25700/35182]: Training loss 0.14840759336948395, current lr 9.663748898553196e-05, contrastive loss 3.6160173416137695, task loss 0.11224742233753204
2025-01-08 09:24:07,315 - root - INFO - [Train] Epoch 6 [25800/35182]: Training loss 0.10625246167182922, current lr 9.663606776384982e-05, contrastive loss 3.5775856971740723, task loss 0.07047660648822784
2025-01-08 09:25:47,990 - root - INFO - [Train] Epoch 6 [25900/35182]: Training loss 0.14093324542045593, current lr 9.663464654216765e-05, contrastive loss 3.760913848876953, task loss 0.10332411527633667
2025-01-08 09:27:28,391 - root - INFO - [Train] Epoch 6 [26000/35182]: Training loss 0.20017290115356445, current lr 9.66332253204855e-05, contrastive loss 3.538548231124878, task loss 0.1647874265909195
2025-01-08 09:29:08,973 - root - INFO - [Train] Epoch 6 [26100/35182]: Training loss 0.0939026027917862, current lr 9.663180409880333e-05, contrastive loss 3.634600877761841, task loss 0.057556599378585815
2025-01-08 09:30:49,355 - root - INFO - [Train] Epoch 6 [26200/35182]: Training loss 0.09843616187572479, current lr 9.663038287712118e-05, contrastive loss 3.592160701751709, task loss 0.06251455098390579
2025-01-08 09:32:29,925 - root - INFO - [Train] Epoch 6 [26300/35182]: Training loss 0.09236431121826172, current lr 9.662896165543902e-05, contrastive loss 3.776020050048828, task loss 0.05460411682724953
2025-01-08 09:34:10,332 - root - INFO - [Train] Epoch 6 [26400/35182]: Training loss 0.1588394045829773, current lr 9.662754043375686e-05, contrastive loss 3.533374547958374, task loss 0.12350566685199738
2025-01-08 09:35:50,989 - root - INFO - [Train] Epoch 6 [26500/35182]: Training loss 0.09113994240760803, current lr 9.662611921207471e-05, contrastive loss 3.6364564895629883, task loss 0.05477537959814072
2025-01-08 09:37:31,394 - root - INFO - [Train] Epoch 6 [26600/35182]: Training loss 0.10795725882053375, current lr 9.662469799039254e-05, contrastive loss 3.643254518508911, task loss 0.0715247094631195
2025-01-08 09:39:11,975 - root - INFO - [Train] Epoch 6 [26700/35182]: Training loss 0.11876457184553146, current lr 9.66232767687104e-05, contrastive loss 3.589548349380493, task loss 0.08286909013986588
2025-01-08 09:40:52,372 - root - INFO - [Train] Epoch 6 [26800/35182]: Training loss 0.1142098605632782, current lr 9.662185554702822e-05, contrastive loss 3.627156972885132, task loss 0.07793829590082169
2025-01-08 09:42:33,047 - root - INFO - [Train] Epoch 6 [26900/35182]: Training loss 0.198153555393219, current lr 9.662043432534608e-05, contrastive loss 3.5998306274414062, task loss 0.16215524077415466
2025-01-08 09:44:13,437 - root - INFO - [Train] Epoch 6 [27000/35182]: Training loss 0.08624676614999771, current lr 9.661901310366391e-05, contrastive loss 3.6607041358947754, task loss 0.04963972419500351
2025-01-08 09:45:54,015 - root - INFO - [Train] Epoch 6 [27100/35182]: Training loss 0.16043886542320251, current lr 9.661759188198177e-05, contrastive loss 3.6953954696655273, task loss 0.12348490953445435
2025-01-08 09:47:34,409 - root - INFO - [Train] Epoch 6 [27200/35182]: Training loss 0.1675100326538086, current lr 9.66161706602996e-05, contrastive loss 3.5904364585876465, task loss 0.13160566985607147
2025-01-08 09:49:14,986 - root - INFO - [Train] Epoch 6 [27300/35182]: Training loss 0.20189231634140015, current lr 9.661474943861744e-05, contrastive loss 3.785468578338623, task loss 0.16403762996196747
2025-01-08 09:50:55,383 - root - INFO - [Train] Epoch 6 [27400/35182]: Training loss 0.1571826934814453, current lr 9.661332821693528e-05, contrastive loss 3.6853551864624023, task loss 0.12032914906740189
2025-01-08 09:52:36,049 - root - INFO - [Train] Epoch 6 [27500/35182]: Training loss 0.11062557995319366, current lr 9.661190699525313e-05, contrastive loss 3.644282817840576, task loss 0.07418275624513626
2025-01-08 09:54:16,441 - root - INFO - [Train] Epoch 6 [27600/35182]: Training loss 0.1160595491528511, current lr 9.661048577357097e-05, contrastive loss 3.640768051147461, task loss 0.07965186983346939
2025-01-08 09:55:57,033 - root - INFO - [Train] Epoch 6 [27700/35182]: Training loss 0.10738129913806915, current lr 9.66090645518888e-05, contrastive loss 3.7302498817443848, task loss 0.07007879763841629
2025-01-08 09:57:37,411 - root - INFO - [Train] Epoch 6 [27800/35182]: Training loss 0.1458229124546051, current lr 9.660764333020666e-05, contrastive loss 3.5585684776306152, task loss 0.1102372258901596
2025-01-08 09:59:17,994 - root - INFO - [Train] Epoch 6 [27900/35182]: Training loss 0.11706428229808807, current lr 9.660622210852449e-05, contrastive loss 3.66420316696167, task loss 0.08042225241661072
2025-01-08 10:00:58,376 - root - INFO - [Train] Epoch 6 [28000/35182]: Training loss 0.12750782072544098, current lr 9.660480088684234e-05, contrastive loss 3.442265510559082, task loss 0.0930851623415947
2025-01-08 10:02:38,958 - root - INFO - [Train] Epoch 6 [28100/35182]: Training loss 0.10737928003072739, current lr 9.660337966516017e-05, contrastive loss 3.754793167114258, task loss 0.06983134895563126
2025-01-08 10:04:19,349 - root - INFO - [Train] Epoch 6 [28200/35182]: Training loss 0.1086956337094307, current lr 9.660195844347802e-05, contrastive loss 3.6668429374694824, task loss 0.07202720642089844
2025-01-08 10:05:59,934 - root - INFO - [Train] Epoch 6 [28300/35182]: Training loss 0.13450877368450165, current lr 9.660053722179586e-05, contrastive loss 3.743781089782715, task loss 0.09707096219062805
2025-01-08 10:07:40,318 - root - INFO - [Train] Epoch 6 [28400/35182]: Training loss 0.11107970774173737, current lr 9.65991160001137e-05, contrastive loss 3.6762428283691406, task loss 0.07431728392839432
2025-01-08 10:09:20,897 - root - INFO - [Train] Epoch 6 [28500/35182]: Training loss 0.09717772901058197, current lr 9.659769477843155e-05, contrastive loss 3.7715415954589844, task loss 0.05946230888366699
2025-01-08 10:11:01,303 - root - INFO - [Train] Epoch 6 [28600/35182]: Training loss 0.09076986461877823, current lr 9.659627355674938e-05, contrastive loss 3.703312873840332, task loss 0.05373673513531685
2025-01-08 10:12:41,972 - root - INFO - [Train] Epoch 6 [28700/35182]: Training loss 0.16224989295005798, current lr 9.659485233506723e-05, contrastive loss 3.582491874694824, task loss 0.12642498314380646
2025-01-08 10:14:22,370 - root - INFO - [Train] Epoch 6 [28800/35182]: Training loss 0.09424704313278198, current lr 9.659343111338506e-05, contrastive loss 3.575420618057251, task loss 0.058492835611104965
2025-01-08 10:16:03,060 - root - INFO - [Train] Epoch 6 [28900/35182]: Training loss 0.11040590703487396, current lr 9.659200989170292e-05, contrastive loss 3.74184513092041, task loss 0.07298745959997177
2025-01-08 10:17:43,461 - root - INFO - [Train] Epoch 6 [29000/35182]: Training loss 0.10146169364452362, current lr 9.659058867002075e-05, contrastive loss 3.707244396209717, task loss 0.06438925117254257
2025-01-08 10:19:24,154 - root - INFO - [Train] Epoch 6 [29100/35182]: Training loss 0.12228751182556152, current lr 9.65891674483386e-05, contrastive loss 3.79060959815979, task loss 0.08438141644001007
2025-01-08 10:21:04,831 - root - INFO - [Train] Epoch 6 [29200/35182]: Training loss 0.13071292638778687, current lr 9.658774622665644e-05, contrastive loss 3.7096409797668457, task loss 0.09361652284860611
2025-01-08 10:22:45,229 - root - INFO - [Train] Epoch 6 [29300/35182]: Training loss 0.10774301737546921, current lr 9.658632500497428e-05, contrastive loss 3.6246376037597656, task loss 0.0714966431260109
2025-01-08 10:24:25,913 - root - INFO - [Train] Epoch 6 [29400/35182]: Training loss 0.13549722731113434, current lr 9.658490378329212e-05, contrastive loss 3.743042469024658, task loss 0.09806680679321289
2025-01-08 10:26:06,334 - root - INFO - [Train] Epoch 6 [29500/35182]: Training loss 0.14083679020404816, current lr 9.658348256160997e-05, contrastive loss 3.6479716300964355, task loss 0.10435707122087479
2025-01-08 10:27:47,004 - root - INFO - [Train] Epoch 6 [29600/35182]: Training loss 0.12438741326332092, current lr 9.658206133992781e-05, contrastive loss 3.7560014724731445, task loss 0.08682740479707718
2025-01-08 10:29:27,404 - root - INFO - [Train] Epoch 6 [29700/35182]: Training loss 0.1425570845603943, current lr 9.658064011824564e-05, contrastive loss 3.560887575149536, task loss 0.10694821178913116
2025-01-08 10:31:08,096 - root - INFO - [Train] Epoch 6 [29800/35182]: Training loss 0.12631608545780182, current lr 9.65792188965635e-05, contrastive loss 3.748507022857666, task loss 0.08883101493120193
2025-01-08 10:32:48,508 - root - INFO - [Train] Epoch 6 [29900/35182]: Training loss 0.1282714605331421, current lr 9.657779767488133e-05, contrastive loss 3.7701871395111084, task loss 0.09056958556175232
2025-01-08 10:34:29,159 - root - INFO - [Train] Epoch 6 [30000/35182]: Training loss 0.15118002891540527, current lr 9.657637645319918e-05, contrastive loss 3.667919874191284, task loss 0.11450083553791046
2025-01-08 10:36:09,849 - root - INFO - [Train] Epoch 6 [30100/35182]: Training loss 0.10967522859573364, current lr 9.657495523151701e-05, contrastive loss 3.680245876312256, task loss 0.07287277281284332
2025-01-08 10:37:50,247 - root - INFO - [Train] Epoch 6 [30200/35182]: Training loss 0.12338325381278992, current lr 9.657353400983486e-05, contrastive loss 3.685102939605713, task loss 0.08653222024440765
2025-01-08 10:39:30,933 - root - INFO - [Train] Epoch 6 [30300/35182]: Training loss 0.17641133069992065, current lr 9.65721127881527e-05, contrastive loss 3.678380012512207, task loss 0.13962753117084503
2025-01-08 10:41:11,326 - root - INFO - [Train] Epoch 6 [30400/35182]: Training loss 0.1202271431684494, current lr 9.657069156647054e-05, contrastive loss 3.558344841003418, task loss 0.08464369922876358
2025-01-08 10:42:52,008 - root - INFO - [Train] Epoch 6 [30500/35182]: Training loss 0.14160536229610443, current lr 9.656927034478839e-05, contrastive loss 3.5628504753112793, task loss 0.10597685724496841
2025-01-08 10:44:32,423 - root - INFO - [Train] Epoch 6 [30600/35182]: Training loss 0.1290723979473114, current lr 9.656784912310622e-05, contrastive loss 3.715503215789795, task loss 0.09191736578941345
2025-01-08 10:46:13,105 - root - INFO - [Train] Epoch 6 [30700/35182]: Training loss 0.09309814870357513, current lr 9.656642790142408e-05, contrastive loss 3.526439666748047, task loss 0.057833753526210785
2025-01-08 10:47:53,508 - root - INFO - [Train] Epoch 6 [30800/35182]: Training loss 0.11125028133392334, current lr 9.65650066797419e-05, contrastive loss 3.528343915939331, task loss 0.07596684247255325
2025-01-08 10:49:34,187 - root - INFO - [Train] Epoch 6 [30900/35182]: Training loss 0.16953586041927338, current lr 9.656358545805976e-05, contrastive loss 3.7110342979431152, task loss 0.13242551684379578
2025-01-08 10:51:14,875 - root - INFO - [Train] Epoch 6 [31000/35182]: Training loss 0.1303500384092331, current lr 9.656216423637759e-05, contrastive loss 3.681750774383545, task loss 0.09353253245353699
2025-01-08 10:52:55,279 - root - INFO - [Train] Epoch 6 [31100/35182]: Training loss 0.1048947349190712, current lr 9.656074301469544e-05, contrastive loss 3.5659708976745605, task loss 0.06923502683639526
2025-01-08 10:54:35,951 - root - INFO - [Train] Epoch 6 [31200/35182]: Training loss 0.12704741954803467, current lr 9.655932179301328e-05, contrastive loss 3.6237406730651855, task loss 0.09081001579761505
2025-01-08 10:56:16,365 - root - INFO - [Train] Epoch 6 [31300/35182]: Training loss 0.1391170769929886, current lr 9.655790057133112e-05, contrastive loss 3.555305004119873, task loss 0.10356403142213821
2025-01-08 10:57:57,055 - root - INFO - [Train] Epoch 6 [31400/35182]: Training loss 0.08889831602573395, current lr 9.655647934964897e-05, contrastive loss 3.684983730316162, task loss 0.05204848200082779
2025-01-08 10:59:37,458 - root - INFO - [Train] Epoch 6 [31500/35182]: Training loss 0.11748816072940826, current lr 9.655505812796681e-05, contrastive loss 3.6484737396240234, task loss 0.08100342005491257
2025-01-08 11:01:18,122 - root - INFO - [Train] Epoch 6 [31600/35182]: Training loss 0.12303048372268677, current lr 9.655363690628465e-05, contrastive loss 3.604579210281372, task loss 0.08698469400405884
2025-01-08 11:02:58,805 - root - INFO - [Train] Epoch 6 [31700/35182]: Training loss 0.1448485553264618, current lr 9.655221568460248e-05, contrastive loss 3.618077278137207, task loss 0.1086677759885788
2025-01-08 11:04:39,199 - root - INFO - [Train] Epoch 6 [31800/35182]: Training loss 0.14765188097953796, current lr 9.655079446292034e-05, contrastive loss 3.5448107719421387, task loss 0.11220376938581467
2025-01-08 11:06:19,883 - root - INFO - [Train] Epoch 6 [31900/35182]: Training loss 0.11569243669509888, current lr 9.654937324123817e-05, contrastive loss 3.539271593093872, task loss 0.08029972016811371
2025-01-08 11:08:00,281 - root - INFO - [Train] Epoch 6 [32000/35182]: Training loss 0.12462082505226135, current lr 9.654795201955601e-05, contrastive loss 3.610274314880371, task loss 0.08851808309555054
2025-01-08 11:09:40,974 - root - INFO - [Train] Epoch 6 [32100/35182]: Training loss 0.08213149011135101, current lr 9.654653079787386e-05, contrastive loss 3.5498132705688477, task loss 0.04663335904479027
2025-01-08 11:11:21,377 - root - INFO - [Train] Epoch 6 [32200/35182]: Training loss 0.17728833854198456, current lr 9.65451095761917e-05, contrastive loss 3.5956246852874756, task loss 0.14133208990097046
2025-01-08 11:13:02,082 - root - INFO - [Train] Epoch 6 [32300/35182]: Training loss 0.17626717686653137, current lr 9.654368835450954e-05, contrastive loss 3.4614219665527344, task loss 0.14165295660495758
2025-01-08 11:14:42,496 - root - INFO - [Train] Epoch 6 [32400/35182]: Training loss 0.13464020192623138, current lr 9.654226713282739e-05, contrastive loss 3.6528005599975586, task loss 0.09811219573020935
2025-01-08 11:16:23,147 - root - INFO - [Train] Epoch 6 [32500/35182]: Training loss 0.16749104857444763, current lr 9.654084591114523e-05, contrastive loss 3.664611339569092, task loss 0.13084493577480316
2025-01-08 11:18:03,826 - root - INFO - [Train] Epoch 6 [32600/35182]: Training loss 0.13152307271957397, current lr 9.653942468946306e-05, contrastive loss 3.642106056213379, task loss 0.09510200470685959
2025-01-08 11:19:44,246 - root - INFO - [Train] Epoch 6 [32700/35182]: Training loss 0.0976761057972908, current lr 9.653800346778092e-05, contrastive loss 3.5454578399658203, task loss 0.062221527099609375
2025-01-08 11:21:24,910 - root - INFO - [Train] Epoch 6 [32800/35182]: Training loss 0.1303606480360031, current lr 9.653658224609875e-05, contrastive loss 3.6389737129211426, task loss 0.09397090971469879
2025-01-08 11:23:05,337 - root - INFO - [Train] Epoch 6 [32900/35182]: Training loss 0.12058133631944656, current lr 9.65351610244166e-05, contrastive loss 3.658616542816162, task loss 0.08399517089128494
2025-01-08 11:24:45,999 - root - INFO - [Train] Epoch 6 [33000/35182]: Training loss 0.10459244251251221, current lr 9.653373980273443e-05, contrastive loss 3.512770652770996, task loss 0.06946473568677902
2025-01-08 11:26:26,407 - root - INFO - [Train] Epoch 6 [33100/35182]: Training loss 0.09263001382350922, current lr 9.653231858105228e-05, contrastive loss 3.5292606353759766, task loss 0.057337407022714615
2025-01-08 11:28:07,090 - root - INFO - [Train] Epoch 6 [33200/35182]: Training loss 0.11643308401107788, current lr 9.653089735937012e-05, contrastive loss 3.643311023712158, task loss 0.07999997586011887
2025-01-08 11:29:47,494 - root - INFO - [Train] Epoch 6 [33300/35182]: Training loss 0.11428052186965942, current lr 9.652947613768796e-05, contrastive loss 3.7172553539276123, task loss 0.07710796594619751
2025-01-08 11:31:28,158 - root - INFO - [Train] Epoch 6 [33400/35182]: Training loss 0.1895504891872406, current lr 9.65280549160058e-05, contrastive loss 3.631153106689453, task loss 0.1532389521598816
2025-01-08 11:33:08,843 - root - INFO - [Train] Epoch 6 [33500/35182]: Training loss 0.13581301271915436, current lr 9.652663369432365e-05, contrastive loss 3.647801637649536, task loss 0.09933499246835709
2025-01-08 11:34:49,235 - root - INFO - [Train] Epoch 6 [33600/35182]: Training loss 0.13774679601192474, current lr 9.652521247264149e-05, contrastive loss 3.6168200969696045, task loss 0.10157860070466995
2025-01-08 11:36:29,928 - root - INFO - [Train] Epoch 6 [33700/35182]: Training loss 0.15920008718967438, current lr 9.652379125095932e-05, contrastive loss 3.412349224090576, task loss 0.12507659196853638
2025-01-08 11:38:10,323 - root - INFO - [Train] Epoch 6 [33800/35182]: Training loss 0.11164765059947968, current lr 9.652237002927718e-05, contrastive loss 3.6164348125457764, task loss 0.0754832997918129
2025-01-08 11:39:51,006 - root - INFO - [Train] Epoch 6 [33900/35182]: Training loss 0.15517383813858032, current lr 9.652094880759501e-05, contrastive loss 3.5440406799316406, task loss 0.11973343789577484
2025-01-08 11:41:31,409 - root - INFO - [Train] Epoch 6 [34000/35182]: Training loss 0.14465869963169098, current lr 9.651952758591285e-05, contrastive loss 3.625277042388916, task loss 0.10840592533349991
2025-01-08 11:43:12,113 - root - INFO - [Train] Epoch 6 [34100/35182]: Training loss 0.0861804336309433, current lr 9.65181063642307e-05, contrastive loss 3.5887603759765625, task loss 0.05029283091425896
2025-01-08 11:44:52,527 - root - INFO - [Train] Epoch 6 [34200/35182]: Training loss 0.13305485248565674, current lr 9.651668514254854e-05, contrastive loss 3.6703572273254395, task loss 0.09635128825902939
2025-01-08 11:46:33,209 - root - INFO - [Train] Epoch 6 [34300/35182]: Training loss 0.14515869319438934, current lr 9.651526392086638e-05, contrastive loss 3.7731354236602783, task loss 0.10742733627557755
2025-01-08 11:48:13,868 - root - INFO - [Train] Epoch 6 [34400/35182]: Training loss 0.1481553167104721, current lr 9.651384269918423e-05, contrastive loss 3.7004234790802, task loss 0.1111510843038559
2025-01-08 11:49:54,279 - root - INFO - [Train] Epoch 6 [34500/35182]: Training loss 0.1557433009147644, current lr 9.651242147750207e-05, contrastive loss 3.6915535926818848, task loss 0.11882776021957397
2025-01-08 11:51:34,954 - root - INFO - [Train] Epoch 6 [34600/35182]: Training loss 0.09738266468048096, current lr 9.65110002558199e-05, contrastive loss 3.5671708583831787, task loss 0.06171095371246338
2025-01-08 11:53:15,372 - root - INFO - [Train] Epoch 6 [34700/35182]: Training loss 0.14044605195522308, current lr 9.650957903413776e-05, contrastive loss 3.5059595108032227, task loss 0.10538645833730698
2025-01-08 11:54:56,026 - root - INFO - [Train] Epoch 6 [34800/35182]: Training loss 0.10862486064434052, current lr 9.650815781245559e-05, contrastive loss 3.671405792236328, task loss 0.0719108060002327
2025-01-08 11:56:36,438 - root - INFO - [Train] Epoch 6 [34900/35182]: Training loss 0.14092934131622314, current lr 9.650673659077344e-05, contrastive loss 3.6250405311584473, task loss 0.10467894375324249
2025-01-08 11:58:17,107 - root - INFO - [Train] Epoch 6 [35000/35182]: Training loss 0.11827625334262848, current lr 9.650531536909127e-05, contrastive loss 3.453054428100586, task loss 0.08374571055173874
2025-01-08 11:59:57,799 - root - INFO - [Train] Epoch 6 [35100/35182]: Training loss 0.16891004145145416, current lr 9.650389414740912e-05, contrastive loss 3.908299446105957, task loss 0.12982705235481262
2025-01-08 14:44:21,292 - root - INFO - [Valid] Evaluation Results: HR@5:0.8038,NDCG@5:0.7409,HR@10:0.8611,NDCG@10:0.7594,HR@20:0.9152,NDCG@20:0.7731
2025-01-08 14:44:22,206 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-08 14:44:25,258 - root - INFO - [Train] Epoch 6: Overall Training loss 0.0
2025-01-08 14:44:25,258 - root - INFO - [Time] Epoch 6: Time taken 15.29 hours
2025-01-08 14:44:30,759 - root - INFO - [Train] Epoch 7 [0/35182]: Training loss 0.12164950370788574, current lr 9.650272874562975e-05, contrastive loss 3.9586777687072754, task loss 0.082062728703022
2025-01-08 14:46:11,186 - root - INFO - [Train] Epoch 7 [100/35182]: Training loss 0.12206485122442245, current lr 9.65013075239476e-05, contrastive loss 3.878248453140259, task loss 0.08328236639499664
2025-01-08 14:47:51,900 - root - INFO - [Train] Epoch 7 [200/35182]: Training loss 0.07592277228832245, current lr 9.649988630226542e-05, contrastive loss 3.5589311122894287, task loss 0.04033345729112625
2025-01-08 14:49:32,337 - root - INFO - [Train] Epoch 7 [300/35182]: Training loss 0.10424233973026276, current lr 9.649846508058328e-05, contrastive loss 3.6724157333374023, task loss 0.06751818209886551
2025-01-08 14:51:12,986 - root - INFO - [Train] Epoch 7 [400/35182]: Training loss 0.17680838704109192, current lr 9.649704385890111e-05, contrastive loss 3.6702487468719482, task loss 0.14010590314865112
2025-01-08 14:52:53,401 - root - INFO - [Train] Epoch 7 [500/35182]: Training loss 0.0849602222442627, current lr 9.649562263721897e-05, contrastive loss 3.520263910293579, task loss 0.04975758492946625
2025-01-08 14:54:34,078 - root - INFO - [Train] Epoch 7 [600/35182]: Training loss 0.17638841271400452, current lr 9.64942014155368e-05, contrastive loss 3.5389790534973145, task loss 0.14099861681461334
2025-01-08 14:56:14,494 - root - INFO - [Train] Epoch 7 [700/35182]: Training loss 0.12345291674137115, current lr 9.649278019385464e-05, contrastive loss 3.635524272918701, task loss 0.08709767460823059
2025-01-08 14:57:55,161 - root - INFO - [Train] Epoch 7 [800/35182]: Training loss 0.12394773215055466, current lr 9.649135897217248e-05, contrastive loss 3.6128110885620117, task loss 0.08781962096691132
2025-01-08 14:59:35,831 - root - INFO - [Train] Epoch 7 [900/35182]: Training loss 0.14386676251888275, current lr 9.648993775049033e-05, contrastive loss 3.7574000358581543, task loss 0.10629276186227798
2025-01-08 15:01:16,232 - root - INFO - [Train] Epoch 7 [1000/35182]: Training loss 0.18312641978263855, current lr 9.648851652880817e-05, contrastive loss 3.7398641109466553, task loss 0.14572778344154358
2025-01-08 15:02:56,913 - root - INFO - [Train] Epoch 7 [1100/35182]: Training loss 0.13670670986175537, current lr 9.6487095307126e-05, contrastive loss 3.6518757343292236, task loss 0.10018795728683472
2025-01-08 15:04:37,340 - root - INFO - [Train] Epoch 7 [1200/35182]: Training loss 0.14899422228336334, current lr 9.648567408544386e-05, contrastive loss 3.4483718872070312, task loss 0.11451050639152527
2025-01-08 15:06:17,999 - root - INFO - [Train] Epoch 7 [1300/35182]: Training loss 0.10114143788814545, current lr 9.648425286376169e-05, contrastive loss 3.5850720405578613, task loss 0.06529071927070618
2025-01-08 15:07:58,427 - root - INFO - [Train] Epoch 7 [1400/35182]: Training loss 0.11365795135498047, current lr 9.648283164207954e-05, contrastive loss 3.667710542678833, task loss 0.07698085159063339
2025-01-08 15:09:39,092 - root - INFO - [Train] Epoch 7 [1500/35182]: Training loss 0.10246991366147995, current lr 9.648141042039737e-05, contrastive loss 3.57790207862854, task loss 0.06669089198112488
2025-01-08 15:11:19,510 - root - INFO - [Train] Epoch 7 [1600/35182]: Training loss 0.12700189650058746, current lr 9.647998919871522e-05, contrastive loss 3.6845898628234863, task loss 0.09015600383281708
2025-01-08 15:13:00,165 - root - INFO - [Train] Epoch 7 [1700/35182]: Training loss 0.13899630308151245, current lr 9.647856797703306e-05, contrastive loss 3.609600067138672, task loss 0.10290029644966125
2025-01-08 15:14:40,851 - root - INFO - [Train] Epoch 7 [1800/35182]: Training loss 0.15506409108638763, current lr 9.64771467553509e-05, contrastive loss 3.614809036254883, task loss 0.11891599744558334
2025-01-08 15:16:21,253 - root - INFO - [Train] Epoch 7 [1900/35182]: Training loss 0.12489478290081024, current lr 9.647572553366875e-05, contrastive loss 3.651313304901123, task loss 0.08838165551424026
2025-01-08 15:18:01,943 - root - INFO - [Train] Epoch 7 [2000/35182]: Training loss 0.12590010464191437, current lr 9.647430431198659e-05, contrastive loss 3.4513492584228516, task loss 0.09138661623001099
2025-01-08 15:19:42,350 - root - INFO - [Train] Epoch 7 [2100/35182]: Training loss 0.12017880380153656, current lr 9.647288309030443e-05, contrastive loss 3.4615426063537598, task loss 0.08556338399648666
2025-01-08 15:21:23,005 - root - INFO - [Train] Epoch 7 [2200/35182]: Training loss 0.10305821895599365, current lr 9.647146186862226e-05, contrastive loss 3.5083632469177246, task loss 0.06797458231449127
2025-01-08 15:23:03,397 - root - INFO - [Train] Epoch 7 [2300/35182]: Training loss 0.0900675356388092, current lr 9.647004064694012e-05, contrastive loss 3.714812755584717, task loss 0.0529194101691246
2025-01-08 15:24:43,985 - root - INFO - [Train] Epoch 7 [2400/35182]: Training loss 0.11219284683465958, current lr 9.646861942525795e-05, contrastive loss 3.5794856548309326, task loss 0.07639799267053604
2025-01-08 15:26:24,386 - root - INFO - [Train] Epoch 7 [2500/35182]: Training loss 0.13754673302173615, current lr 9.646719820357581e-05, contrastive loss 3.6653025150299072, task loss 0.10089371353387833
2025-01-08 15:28:05,061 - root - INFO - [Train] Epoch 7 [2600/35182]: Training loss 0.17689281702041626, current lr 9.646577698189364e-05, contrastive loss 3.757202386856079, task loss 0.13932079076766968
2025-01-08 15:29:45,449 - root - INFO - [Train] Epoch 7 [2700/35182]: Training loss 0.1093011200428009, current lr 9.646435576021148e-05, contrastive loss 3.454479932785034, task loss 0.07475632429122925
2025-01-08 15:31:26,032 - root - INFO - [Train] Epoch 7 [2800/35182]: Training loss 0.14730215072631836, current lr 9.646293453852933e-05, contrastive loss 3.534209728240967, task loss 0.11196006089448929
2025-01-08 15:33:06,441 - root - INFO - [Train] Epoch 7 [2900/35182]: Training loss 0.11596023291349411, current lr 9.646151331684717e-05, contrastive loss 3.407423973083496, task loss 0.08188599348068237
2025-01-08 15:34:47,112 - root - INFO - [Train] Epoch 7 [3000/35182]: Training loss 0.10416645556688309, current lr 9.646009209516501e-05, contrastive loss 3.5219669342041016, task loss 0.06894678622484207
2025-01-08 15:36:27,798 - root - INFO - [Train] Epoch 7 [3100/35182]: Training loss 0.1475653350353241, current lr 9.645867087348284e-05, contrastive loss 3.56669282913208, task loss 0.11189841479063034
2025-01-08 15:38:08,200 - root - INFO - [Train] Epoch 7 [3200/35182]: Training loss 0.07439328730106354, current lr 9.64572496518007e-05, contrastive loss 3.5563697814941406, task loss 0.03882959485054016
2025-01-08 15:39:48,881 - root - INFO - [Train] Epoch 7 [3300/35182]: Training loss 0.12874871492385864, current lr 9.645582843011853e-05, contrastive loss 3.5951571464538574, task loss 0.09279713779687881
2025-01-08 15:41:29,287 - root - INFO - [Train] Epoch 7 [3400/35182]: Training loss 0.21289849281311035, current lr 9.645440720843639e-05, contrastive loss 3.702012062072754, task loss 0.1758783757686615
2025-01-08 15:43:09,968 - root - INFO - [Train] Epoch 7 [3500/35182]: Training loss 0.1173115149140358, current lr 9.645298598675422e-05, contrastive loss 3.3796377182006836, task loss 0.08351513743400574
2025-01-08 15:44:50,380 - root - INFO - [Train] Epoch 7 [3600/35182]: Training loss 0.09484148025512695, current lr 9.645156476507206e-05, contrastive loss 3.6517465114593506, task loss 0.05832401290535927
2025-01-08 15:46:31,054 - root - INFO - [Train] Epoch 7 [3700/35182]: Training loss 0.13236631453037262, current lr 9.64501435433899e-05, contrastive loss 3.7088732719421387, task loss 0.0952775850892067
2025-01-08 15:48:11,460 - root - INFO - [Train] Epoch 7 [3800/35182]: Training loss 0.09318069368600845, current lr 9.644872232170775e-05, contrastive loss 3.5891342163085938, task loss 0.057289350777864456
2025-01-08 15:49:52,136 - root - INFO - [Train] Epoch 7 [3900/35182]: Training loss 0.10248447954654694, current lr 9.644730110002559e-05, contrastive loss 3.5434412956237793, task loss 0.06705006211996078
2025-01-08 15:51:32,820 - root - INFO - [Train] Epoch 7 [4000/35182]: Training loss 0.15801362693309784, current lr 9.644587987834343e-05, contrastive loss 3.642270088195801, task loss 0.12159092724323273
2025-01-08 15:53:13,225 - root - INFO - [Train] Epoch 7 [4100/35182]: Training loss 0.11562947928905487, current lr 9.644445865666128e-05, contrastive loss 3.5459001064300537, task loss 0.08017048239707947
2025-01-08 15:54:53,911 - root - INFO - [Train] Epoch 7 [4200/35182]: Training loss 0.10997287184000015, current lr 9.64430374349791e-05, contrastive loss 3.551985263824463, task loss 0.07445301860570908
2025-01-08 15:56:34,323 - root - INFO - [Train] Epoch 7 [4300/35182]: Training loss 0.07880640774965286, current lr 9.644161621329696e-05, contrastive loss 3.552051067352295, task loss 0.04328589886426926
2025-01-08 15:58:14,991 - root - INFO - [Train] Epoch 7 [4400/35182]: Training loss 0.09688591957092285, current lr 9.644019499161479e-05, contrastive loss 3.6443538665771484, task loss 0.060442376881837845
2025-01-08 15:59:55,398 - root - INFO - [Train] Epoch 7 [4500/35182]: Training loss 0.1139393001794815, current lr 9.643877376993264e-05, contrastive loss 3.4700403213500977, task loss 0.0792388990521431
2025-01-08 16:01:36,073 - root - INFO - [Train] Epoch 7 [4600/35182]: Training loss 0.14395463466644287, current lr 9.643735254825048e-05, contrastive loss 3.7610387802124023, task loss 0.10634425282478333
2025-01-08 16:03:16,479 - root - INFO - [Train] Epoch 7 [4700/35182]: Training loss 0.1267215609550476, current lr 9.643593132656832e-05, contrastive loss 3.6010327339172363, task loss 0.09071124345064163
2025-01-08 16:04:57,167 - root - INFO - [Train] Epoch 7 [4800/35182]: Training loss 0.10770832002162933, current lr 9.643451010488617e-05, contrastive loss 3.6274733543395996, task loss 0.07143358886241913
2025-01-08 16:06:37,839 - root - INFO - [Train] Epoch 7 [4900/35182]: Training loss 0.07655192911624908, current lr 9.643308888320401e-05, contrastive loss 3.7136518955230713, task loss 0.03941541165113449
2025-01-08 16:08:18,242 - root - INFO - [Train] Epoch 7 [5000/35182]: Training loss 0.14095334708690643, current lr 9.643166766152185e-05, contrastive loss 3.5665283203125, task loss 0.105288065969944
2025-01-08 16:09:58,937 - root - INFO - [Train] Epoch 7 [5100/35182]: Training loss 0.11923432350158691, current lr 9.643024643983968e-05, contrastive loss 3.5664587020874023, task loss 0.08356973528862
2025-01-08 16:11:39,358 - root - INFO - [Train] Epoch 7 [5200/35182]: Training loss 0.08347074687480927, current lr 9.642882521815754e-05, contrastive loss 3.489229440689087, task loss 0.048578448593616486
2025-01-08 16:13:20,013 - root - INFO - [Train] Epoch 7 [5300/35182]: Training loss 0.15494710206985474, current lr 9.642740399647537e-05, contrastive loss 3.6034982204437256, task loss 0.11891211569309235
2025-01-08 16:15:00,419 - root - INFO - [Train] Epoch 7 [5400/35182]: Training loss 0.08532906323671341, current lr 9.642598277479323e-05, contrastive loss 3.7420880794525146, task loss 0.04790818318724632
2025-01-08 16:16:41,106 - root - INFO - [Train] Epoch 7 [5500/35182]: Training loss 0.11735809594392776, current lr 9.642456155311106e-05, contrastive loss 3.6343538761138916, task loss 0.08101455867290497
2025-01-08 16:18:21,517 - root - INFO - [Train] Epoch 7 [5600/35182]: Training loss 0.11859364807605743, current lr 9.64231403314289e-05, contrastive loss 3.578610897064209, task loss 0.08280754089355469
2025-01-08 16:20:02,188 - root - INFO - [Train] Epoch 7 [5700/35182]: Training loss 0.15382178127765656, current lr 9.642171910974674e-05, contrastive loss 3.475897789001465, task loss 0.11906280368566513
2025-01-08 16:21:42,864 - root - INFO - [Train] Epoch 7 [5800/35182]: Training loss 0.15378402173519135, current lr 9.642029788806459e-05, contrastive loss 3.7268078327178955, task loss 0.11651594191789627
2025-01-08 16:23:23,281 - root - INFO - [Train] Epoch 7 [5900/35182]: Training loss 0.13079628348350525, current lr 9.641887666638243e-05, contrastive loss 3.7521893978118896, task loss 0.09327439963817596
2025-01-08 16:25:03,950 - root - INFO - [Train] Epoch 7 [6000/35182]: Training loss 0.19094902276992798, current lr 9.641745544470027e-05, contrastive loss 3.6468005180358887, task loss 0.15448102355003357
2025-01-08 16:26:44,357 - root - INFO - [Train] Epoch 7 [6100/35182]: Training loss 0.11397434771060944, current lr 9.641603422301812e-05, contrastive loss 3.5251195430755615, task loss 0.07872315496206284
2025-01-08 16:28:25,027 - root - INFO - [Train] Epoch 7 [6200/35182]: Training loss 0.10873360931873322, current lr 9.641461300133595e-05, contrastive loss 3.7029173374176025, task loss 0.0717044323682785
2025-01-08 16:30:05,458 - root - INFO - [Train] Epoch 7 [6300/35182]: Training loss 0.15962344408035278, current lr 9.64131917796538e-05, contrastive loss 3.6027700901031494, task loss 0.12359573692083359
2025-01-08 16:31:46,129 - root - INFO - [Train] Epoch 7 [6400/35182]: Training loss 0.1473185122013092, current lr 9.641177055797163e-05, contrastive loss 3.5886149406433105, task loss 0.11143235862255096
2025-01-08 16:33:26,540 - root - INFO - [Train] Epoch 7 [6500/35182]: Training loss 0.09241129457950592, current lr 9.641034933628948e-05, contrastive loss 3.6286978721618652, task loss 0.05612431466579437
2025-01-08 16:35:07,239 - root - INFO - [Train] Epoch 7 [6600/35182]: Training loss 0.12245810776948929, current lr 9.640892811460732e-05, contrastive loss 3.4259214401245117, task loss 0.08819889277219772
2025-01-08 16:36:47,879 - root - INFO - [Train] Epoch 7 [6700/35182]: Training loss 0.06702170521020889, current lr 9.640750689292516e-05, contrastive loss 3.669562816619873, task loss 0.030326077714562416
2025-01-08 16:38:28,310 - root - INFO - [Train] Epoch 7 [6800/35182]: Training loss 0.10498917102813721, current lr 9.6406085671243e-05, contrastive loss 3.5726218223571777, task loss 0.06926295906305313
2025-01-08 16:40:08,966 - root - INFO - [Train] Epoch 7 [6900/35182]: Training loss 0.12290816009044647, current lr 9.640466444956085e-05, contrastive loss 3.697803020477295, task loss 0.08593013137578964
2025-01-08 16:41:49,372 - root - INFO - [Train] Epoch 7 [7000/35182]: Training loss 0.12344059348106384, current lr 9.64032432278787e-05, contrastive loss 3.391747236251831, task loss 0.08952312171459198
2025-01-08 16:43:30,059 - root - INFO - [Train] Epoch 7 [7100/35182]: Training loss 0.10820850729942322, current lr 9.640182200619652e-05, contrastive loss 3.50483775138855, task loss 0.07316012680530548
2025-01-08 16:45:10,465 - root - INFO - [Train] Epoch 7 [7200/35182]: Training loss 0.14803141355514526, current lr 9.640040078451438e-05, contrastive loss 3.5407581329345703, task loss 0.11262383311986923
2025-01-08 16:46:51,138 - root - INFO - [Train] Epoch 7 [7300/35182]: Training loss 0.12848427891731262, current lr 9.639897956283221e-05, contrastive loss 3.4725732803344727, task loss 0.09375853836536407
2025-01-08 16:48:31,815 - root - INFO - [Train] Epoch 7 [7400/35182]: Training loss 0.12323959171772003, current lr 9.639755834115007e-05, contrastive loss 3.8557729721069336, task loss 0.08468186110258102
2025-01-08 16:50:12,225 - root - INFO - [Train] Epoch 7 [7500/35182]: Training loss 0.1033436506986618, current lr 9.63961371194679e-05, contrastive loss 3.623913049697876, task loss 0.0671045258641243
2025-01-08 16:51:52,895 - root - INFO - [Train] Epoch 7 [7600/35182]: Training loss 0.0906606912612915, current lr 9.639471589778574e-05, contrastive loss 3.528541088104248, task loss 0.05537528172135353
2025-01-08 16:53:33,318 - root - INFO - [Train] Epoch 7 [7700/35182]: Training loss 0.09618788957595825, current lr 9.639329467610358e-05, contrastive loss 3.609893560409546, task loss 0.060088954865932465
2025-01-08 16:55:13,993 - root - INFO - [Train] Epoch 7 [7800/35182]: Training loss 0.11331140995025635, current lr 9.639187345442143e-05, contrastive loss 3.4760026931762695, task loss 0.07855138927698135
2025-01-08 16:56:54,411 - root - INFO - [Train] Epoch 7 [7900/35182]: Training loss 0.13876518607139587, current lr 9.639045223273927e-05, contrastive loss 3.5576534271240234, task loss 0.1031886488199234
2025-01-08 16:58:35,066 - root - INFO - [Train] Epoch 7 [8000/35182]: Training loss 0.11307254433631897, current lr 9.638903101105711e-05, contrastive loss 3.5561060905456543, task loss 0.0775114893913269
2025-01-08 17:00:15,480 - root - INFO - [Train] Epoch 7 [8100/35182]: Training loss 0.11998531222343445, current lr 9.638760978937496e-05, contrastive loss 3.7208681106567383, task loss 0.08277663588523865
2025-01-08 17:01:56,154 - root - INFO - [Train] Epoch 7 [8200/35182]: Training loss 0.08411887288093567, current lr 9.638618856769279e-05, contrastive loss 3.744022846221924, task loss 0.04667864739894867
2025-01-08 17:03:36,835 - root - INFO - [Train] Epoch 7 [8300/35182]: Training loss 0.17789702117443085, current lr 9.638476734601064e-05, contrastive loss 3.74539852142334, task loss 0.14044304192066193
2025-01-08 17:05:17,241 - root - INFO - [Train] Epoch 7 [8400/35182]: Training loss 0.08993437886238098, current lr 9.638334612432847e-05, contrastive loss 3.7867679595947266, task loss 0.052066702395677567
2025-01-08 17:06:57,917 - root - INFO - [Train] Epoch 7 [8500/35182]: Training loss 0.1120753288269043, current lr 9.638192490264632e-05, contrastive loss 3.42457914352417, task loss 0.07782953977584839
2025-01-08 17:08:38,334 - root - INFO - [Train] Epoch 7 [8600/35182]: Training loss 0.09474946558475494, current lr 9.638050368096416e-05, contrastive loss 3.531421661376953, task loss 0.05943525210022926
2025-01-08 17:10:19,015 - root - INFO - [Train] Epoch 7 [8700/35182]: Training loss 0.13233669102191925, current lr 9.6379082459282e-05, contrastive loss 3.40907621383667, task loss 0.09824593365192413
2025-01-08 17:11:59,419 - root - INFO - [Train] Epoch 7 [8800/35182]: Training loss 0.07991155236959457, current lr 9.637766123759985e-05, contrastive loss 3.7039613723754883, task loss 0.04287194088101387
2025-01-08 17:13:40,093 - root - INFO - [Train] Epoch 7 [8900/35182]: Training loss 0.11111754924058914, current lr 9.637624001591769e-05, contrastive loss 3.572370767593384, task loss 0.07539384067058563
2025-01-08 17:15:20,500 - root - INFO - [Train] Epoch 7 [9000/35182]: Training loss 0.10639980435371399, current lr 9.637481879423553e-05, contrastive loss 3.6070199012756348, task loss 0.07032960653305054
2025-01-08 17:17:01,175 - root - INFO - [Train] Epoch 7 [9100/35182]: Training loss 0.09358220547437668, current lr 9.637339757255336e-05, contrastive loss 3.5326895713806152, task loss 0.058255311101675034
2025-01-08 17:18:41,855 - root - INFO - [Train] Epoch 7 [9200/35182]: Training loss 0.15169039368629456, current lr 9.637197635087122e-05, contrastive loss 3.58665132522583, task loss 0.11582387238740921
2025-01-08 17:20:22,260 - root - INFO - [Train] Epoch 7 [9300/35182]: Training loss 0.15056587755680084, current lr 9.637055512918905e-05, contrastive loss 3.5220134258270264, task loss 0.11534574627876282
2025-01-08 17:22:02,938 - root - INFO - [Train] Epoch 7 [9400/35182]: Training loss 0.1409381479024887, current lr 9.63691339075069e-05, contrastive loss 3.5339999198913574, task loss 0.10559814423322678
2025-01-08 17:23:43,355 - root - INFO - [Train] Epoch 7 [9500/35182]: Training loss 0.10476592928171158, current lr 9.636771268582474e-05, contrastive loss 3.6238701343536377, task loss 0.0685272291302681
2025-01-08 17:25:24,033 - root - INFO - [Train] Epoch 7 [9600/35182]: Training loss 0.10200856626033783, current lr 9.636629146414258e-05, contrastive loss 3.400221824645996, task loss 0.06800635159015656
2025-01-08 17:27:04,445 - root - INFO - [Train] Epoch 7 [9700/35182]: Training loss 0.11179029941558838, current lr 9.636487024246042e-05, contrastive loss 3.6130530834198, task loss 0.07565977424383163
2025-01-08 17:28:45,108 - root - INFO - [Train] Epoch 7 [9800/35182]: Training loss 0.09189446270465851, current lr 9.636344902077827e-05, contrastive loss 3.4915311336517334, task loss 0.05697914958000183
2025-01-08 17:30:25,517 - root - INFO - [Train] Epoch 7 [9900/35182]: Training loss 0.16184261441230774, current lr 9.636202779909611e-05, contrastive loss 3.4762985706329346, task loss 0.12707963585853577
2025-01-08 17:32:06,202 - root - INFO - [Train] Epoch 7 [10000/35182]: Training loss 0.1144862174987793, current lr 9.636060657741395e-05, contrastive loss 3.712498188018799, task loss 0.07736124098300934
2025-01-08 17:33:46,874 - root - INFO - [Train] Epoch 7 [10100/35182]: Training loss 0.14149025082588196, current lr 9.63591853557318e-05, contrastive loss 3.624635934829712, task loss 0.10524388402700424
2025-01-08 17:35:27,286 - root - INFO - [Train] Epoch 7 [10200/35182]: Training loss 0.12013928592205048, current lr 9.635776413404963e-05, contrastive loss 3.656935214996338, task loss 0.08356992900371552
2025-01-08 17:37:07,960 - root - INFO - [Train] Epoch 7 [10300/35182]: Training loss 0.08794032782316208, current lr 9.635634291236748e-05, contrastive loss 3.406022071838379, task loss 0.053880106657743454
2025-01-08 17:38:48,371 - root - INFO - [Train] Epoch 7 [10400/35182]: Training loss 0.17730243504047394, current lr 9.635492169068531e-05, contrastive loss 3.6396799087524414, task loss 0.14090563356876373
2025-01-08 17:40:29,066 - root - INFO - [Train] Epoch 7 [10500/35182]: Training loss 0.10384461283683777, current lr 9.635350046900316e-05, contrastive loss 3.7251946926116943, task loss 0.06659267097711563
2025-01-08 17:42:09,476 - root - INFO - [Train] Epoch 7 [10600/35182]: Training loss 0.0973682552576065, current lr 9.6352079247321e-05, contrastive loss 3.5282034873962402, task loss 0.06208622083067894
2025-01-08 17:43:50,142 - root - INFO - [Train] Epoch 7 [10700/35182]: Training loss 0.1273382008075714, current lr 9.635065802563884e-05, contrastive loss 3.5360162258148193, task loss 0.0919780358672142
2025-01-08 17:45:30,813 - root - INFO - [Train] Epoch 7 [10800/35182]: Training loss 0.0732765793800354, current lr 9.634923680395669e-05, contrastive loss 3.527643918991089, task loss 0.03800014406442642
2025-01-08 17:47:11,215 - root - INFO - [Train] Epoch 7 [10900/35182]: Training loss 0.1461210995912552, current lr 9.634781558227453e-05, contrastive loss 3.613340377807617, task loss 0.10998769104480743
2025-01-08 17:48:51,894 - root - INFO - [Train] Epoch 7 [11000/35182]: Training loss 0.0788211077451706, current lr 9.634639436059237e-05, contrastive loss 3.4890260696411133, task loss 0.043930843472480774
2025-01-08 17:50:32,318 - root - INFO - [Train] Epoch 7 [11100/35182]: Training loss 0.14888346195220947, current lr 9.63449731389102e-05, contrastive loss 3.6038436889648438, task loss 0.11284501850605011
2025-01-08 17:52:12,983 - root - INFO - [Train] Epoch 7 [11200/35182]: Training loss 0.09747892618179321, current lr 9.634355191722806e-05, contrastive loss 3.515254259109497, task loss 0.06232638284564018
2025-01-08 17:53:53,406 - root - INFO - [Train] Epoch 7 [11300/35182]: Training loss 0.13358142971992493, current lr 9.634213069554589e-05, contrastive loss 3.5657050609588623, task loss 0.0979243814945221
2025-01-08 17:55:34,082 - root - INFO - [Train] Epoch 7 [11400/35182]: Training loss 0.11647451668977737, current lr 9.634070947386373e-05, contrastive loss 3.5770959854125977, task loss 0.08070355653762817
2025-01-08 17:57:14,496 - root - INFO - [Train] Epoch 7 [11500/35182]: Training loss 0.14967909455299377, current lr 9.633928825218158e-05, contrastive loss 3.756321907043457, task loss 0.11211586743593216
2025-01-08 17:58:55,148 - root - INFO - [Train] Epoch 7 [11600/35182]: Training loss 0.10956741869449615, current lr 9.633786703049942e-05, contrastive loss 3.643893241882324, task loss 0.07312849164009094
2025-01-08 18:00:35,832 - root - INFO - [Train] Epoch 7 [11700/35182]: Training loss 0.12740623950958252, current lr 9.633644580881726e-05, contrastive loss 3.6071629524230957, task loss 0.09133461862802505
2025-01-08 18:02:16,255 - root - INFO - [Train] Epoch 7 [11800/35182]: Training loss 0.11260795593261719, current lr 9.633502458713511e-05, contrastive loss 3.6826257705688477, task loss 0.07578170299530029
2025-01-08 18:03:56,920 - root - INFO - [Train] Epoch 7 [11900/35182]: Training loss 0.17648163437843323, current lr 9.633360336545295e-05, contrastive loss 3.5968356132507324, task loss 0.14051327109336853
2025-01-08 18:05:37,328 - root - INFO - [Train] Epoch 7 [12000/35182]: Training loss 0.14285701513290405, current lr 9.63321821437708e-05, contrastive loss 3.7134482860565186, task loss 0.10572253912687302
2025-01-08 18:07:17,997 - root - INFO - [Train] Epoch 7 [12100/35182]: Training loss 0.10917972028255463, current lr 9.633076092208864e-05, contrastive loss 3.57206654548645, task loss 0.07345905900001526
2025-01-08 18:08:58,417 - root - INFO - [Train] Epoch 7 [12200/35182]: Training loss 0.06905147433280945, current lr 9.632933970040647e-05, contrastive loss 3.722534656524658, task loss 0.03182612359523773
2025-01-08 18:10:39,095 - root - INFO - [Train] Epoch 7 [12300/35182]: Training loss 0.17883718013763428, current lr 9.632791847872431e-05, contrastive loss 3.628946304321289, task loss 0.14254771173000336
2025-01-08 18:12:19,520 - root - INFO - [Train] Epoch 7 [12400/35182]: Training loss 0.13683590292930603, current lr 9.632649725704215e-05, contrastive loss 3.748443841934204, task loss 0.09935146570205688
2025-01-08 18:14:00,170 - root - INFO - [Train] Epoch 7 [12500/35182]: Training loss 0.16787965595722198, current lr 9.632507603536e-05, contrastive loss 3.3655834197998047, task loss 0.1342238187789917
2025-01-08 18:15:40,852 - root - INFO - [Train] Epoch 7 [12600/35182]: Training loss 0.09710777550935745, current lr 9.632365481367784e-05, contrastive loss 3.600693702697754, task loss 0.06110084056854248
2025-01-08 18:17:21,275 - root - INFO - [Train] Epoch 7 [12700/35182]: Training loss 0.11820060014724731, current lr 9.632223359199569e-05, contrastive loss 3.4678144454956055, task loss 0.08352245390415192
2025-01-08 18:19:01,937 - root - INFO - [Train] Epoch 7 [12800/35182]: Training loss 0.11425435543060303, current lr 9.632081237031353e-05, contrastive loss 3.5938947200775146, task loss 0.07831540703773499
2025-01-08 18:20:42,334 - root - INFO - [Train] Epoch 7 [12900/35182]: Training loss 0.10863369703292847, current lr 9.631939114863137e-05, contrastive loss 3.4996819496154785, task loss 0.07363687455654144
2025-01-08 18:22:23,024 - root - INFO - [Train] Epoch 7 [13000/35182]: Training loss 0.11716440320014954, current lr 9.631796992694922e-05, contrastive loss 3.5330493450164795, task loss 0.08183391392230988
2025-01-08 18:24:03,434 - root - INFO - [Train] Epoch 7 [13100/35182]: Training loss 0.1162908673286438, current lr 9.631654870526705e-05, contrastive loss 3.656362533569336, task loss 0.07972723990678787
2025-01-08 18:25:44,106 - root - INFO - [Train] Epoch 7 [13200/35182]: Training loss 0.17142964899539948, current lr 9.63151274835849e-05, contrastive loss 3.5113525390625, task loss 0.13631612062454224
2025-01-08 18:27:24,536 - root - INFO - [Train] Epoch 7 [13300/35182]: Training loss 0.1200767308473587, current lr 9.631370626190273e-05, contrastive loss 3.646013021469116, task loss 0.08361660689115524
2025-01-08 18:29:05,190 - root - INFO - [Train] Epoch 7 [13400/35182]: Training loss 0.10001172870397568, current lr 9.631228504022058e-05, contrastive loss 3.763329029083252, task loss 0.06237844005227089
2025-01-08 18:30:45,870 - root - INFO - [Train] Epoch 7 [13500/35182]: Training loss 0.1496545970439911, current lr 9.631086381853842e-05, contrastive loss 3.586475372314453, task loss 0.11378984153270721
2025-01-08 18:32:26,291 - root - INFO - [Train] Epoch 7 [13600/35182]: Training loss 0.12569758296012878, current lr 9.630944259685626e-05, contrastive loss 3.6745452880859375, task loss 0.08895212411880493
2025-01-08 18:34:06,965 - root - INFO - [Train] Epoch 7 [13700/35182]: Training loss 0.1018439531326294, current lr 9.63080213751741e-05, contrastive loss 3.6284265518188477, task loss 0.06555968523025513
2025-01-08 18:35:47,375 - root - INFO - [Train] Epoch 7 [13800/35182]: Training loss 0.16920524835586548, current lr 9.630660015349195e-05, contrastive loss 3.5644359588623047, task loss 0.1335608810186386
2025-01-08 18:37:28,041 - root - INFO - [Train] Epoch 7 [13900/35182]: Training loss 0.17389710247516632, current lr 9.630517893180979e-05, contrastive loss 3.485088348388672, task loss 0.13904622197151184
2025-01-08 18:39:08,448 - root - INFO - [Train] Epoch 7 [14000/35182]: Training loss 0.11980202794075012, current lr 9.630375771012764e-05, contrastive loss 3.5596189498901367, task loss 0.08420584350824356
2025-01-08 18:40:49,135 - root - INFO - [Train] Epoch 7 [14100/35182]: Training loss 0.12944385409355164, current lr 9.630233648844548e-05, contrastive loss 3.5711350440979004, task loss 0.09373249858617783
2025-01-08 18:42:29,807 - root - INFO - [Train] Epoch 7 [14200/35182]: Training loss 0.14247426390647888, current lr 9.630091526676331e-05, contrastive loss 3.5403614044189453, task loss 0.10707064718008041
2025-01-08 18:44:10,221 - root - INFO - [Train] Epoch 7 [14300/35182]: Training loss 0.13686832785606384, current lr 9.629949404508115e-05, contrastive loss 3.7172350883483887, task loss 0.09969598054885864
2025-01-08 18:45:50,890 - root - INFO - [Train] Epoch 7 [14400/35182]: Training loss 0.1602444052696228, current lr 9.6298072823399e-05, contrastive loss 3.4520626068115234, task loss 0.12572377920150757
2025-01-08 18:47:31,310 - root - INFO - [Train] Epoch 7 [14500/35182]: Training loss 0.15792685747146606, current lr 9.629665160171684e-05, contrastive loss 3.8109939098358154, task loss 0.11981692165136337
2025-01-08 18:49:11,977 - root - INFO - [Train] Epoch 7 [14600/35182]: Training loss 0.12995365262031555, current lr 9.629523038003468e-05, contrastive loss 3.7377431392669678, task loss 0.09257622063159943
2025-01-08 18:50:52,391 - root - INFO - [Train] Epoch 7 [14700/35182]: Training loss 0.1157335638999939, current lr 9.629380915835253e-05, contrastive loss 3.527491807937622, task loss 0.08045864850282669
2025-01-08 18:52:33,068 - root - INFO - [Train] Epoch 7 [14800/35182]: Training loss 0.09442608803510666, current lr 9.629238793667037e-05, contrastive loss 3.657656669616699, task loss 0.057849522680044174
2025-01-08 18:54:13,472 - root - INFO - [Train] Epoch 7 [14900/35182]: Training loss 0.08140013366937637, current lr 9.629096671498821e-05, contrastive loss 3.7844138145446777, task loss 0.04355599731206894
2025-01-08 18:55:54,139 - root - INFO - [Train] Epoch 7 [15000/35182]: Training loss 0.06919562071561813, current lr 9.628954549330606e-05, contrastive loss 3.5718789100646973, task loss 0.03347683325409889
2025-01-08 18:57:34,828 - root - INFO - [Train] Epoch 7 [15100/35182]: Training loss 0.12675443291664124, current lr 9.628812427162389e-05, contrastive loss 3.6817972660064697, task loss 0.08993645757436752
2025-01-08 18:59:15,236 - root - INFO - [Train] Epoch 7 [15200/35182]: Training loss 0.10213194787502289, current lr 9.628670304994174e-05, contrastive loss 3.665283203125, task loss 0.06547911465167999
2025-01-08 19:00:55,922 - root - INFO - [Train] Epoch 7 [15300/35182]: Training loss 0.1275724172592163, current lr 9.628528182825957e-05, contrastive loss 3.595449686050415, task loss 0.09161791950464249
2025-01-08 19:02:36,342 - root - INFO - [Train] Epoch 7 [15400/35182]: Training loss 0.14126795530319214, current lr 9.628386060657742e-05, contrastive loss 3.609724283218384, task loss 0.10517070442438126
2025-01-08 19:04:17,008 - root - INFO - [Train] Epoch 7 [15500/35182]: Training loss 0.12164993584156036, current lr 9.628243938489526e-05, contrastive loss 3.831766366958618, task loss 0.08333227038383484
2025-01-08 19:05:57,409 - root - INFO - [Train] Epoch 7 [15600/35182]: Training loss 0.09966666251420975, current lr 9.62810181632131e-05, contrastive loss 3.640657901763916, task loss 0.06326008588075638
2025-01-08 19:07:38,093 - root - INFO - [Train] Epoch 7 [15700/35182]: Training loss 0.13334092497825623, current lr 9.627959694153095e-05, contrastive loss 3.658752918243408, task loss 0.09675340354442596
2025-01-08 19:09:18,499 - root - INFO - [Train] Epoch 7 [15800/35182]: Training loss 0.13045881688594818, current lr 9.627817571984879e-05, contrastive loss 3.5184707641601562, task loss 0.09527410566806793
2025-01-08 19:10:59,179 - root - INFO - [Train] Epoch 7 [15900/35182]: Training loss 0.12637561559677124, current lr 9.627675449816663e-05, contrastive loss 3.662271499633789, task loss 0.08975290507078171
2025-01-08 19:12:39,854 - root - INFO - [Train] Epoch 7 [16000/35182]: Training loss 0.10336003452539444, current lr 9.627533327648448e-05, contrastive loss 3.7270994186401367, task loss 0.06608904153108597
2025-01-08 19:14:20,273 - root - INFO - [Train] Epoch 7 [16100/35182]: Training loss 0.15719211101531982, current lr 9.627391205480232e-05, contrastive loss 3.7134194374084473, task loss 0.12005791068077087
2025-01-08 19:16:00,938 - root - INFO - [Train] Epoch 7 [16200/35182]: Training loss 0.08795934170484543, current lr 9.627249083312015e-05, contrastive loss 3.662816286087036, task loss 0.051331181079149246
2025-01-08 19:17:41,361 - root - INFO - [Train] Epoch 7 [16300/35182]: Training loss 0.11602192372083664, current lr 9.627106961143799e-05, contrastive loss 3.5215566158294678, task loss 0.08080635964870453
2025-01-08 19:19:22,014 - root - INFO - [Train] Epoch 7 [16400/35182]: Training loss 0.08725576102733612, current lr 9.626964838975584e-05, contrastive loss 3.6805272102355957, task loss 0.05045049265027046
2025-01-08 19:21:02,423 - root - INFO - [Train] Epoch 7 [16500/35182]: Training loss 0.08774268627166748, current lr 9.626822716807368e-05, contrastive loss 3.612576961517334, task loss 0.05161692202091217
2025-01-08 19:22:43,108 - root - INFO - [Train] Epoch 7 [16600/35182]: Training loss 0.09568308293819427, current lr 9.626680594639152e-05, contrastive loss 3.6485962867736816, task loss 0.05919712036848068
2025-01-08 19:24:23,518 - root - INFO - [Train] Epoch 7 [16700/35182]: Training loss 0.1336517632007599, current lr 9.626538472470937e-05, contrastive loss 3.6113343238830566, task loss 0.09753842651844025
2025-01-08 19:26:04,196 - root - INFO - [Train] Epoch 7 [16800/35182]: Training loss 0.10097763687372208, current lr 9.626396350302721e-05, contrastive loss 3.5448508262634277, task loss 0.06552913039922714
2025-01-08 19:27:44,874 - root - INFO - [Train] Epoch 7 [16900/35182]: Training loss 0.10121649503707886, current lr 9.626254228134505e-05, contrastive loss 3.491732358932495, task loss 0.06629917770624161
2025-01-08 19:29:25,293 - root - INFO - [Train] Epoch 7 [17000/35182]: Training loss 0.09920509159564972, current lr 9.62611210596629e-05, contrastive loss 3.4742860794067383, task loss 0.06446222960948944
2025-01-08 19:31:05,952 - root - INFO - [Train] Epoch 7 [17100/35182]: Training loss 0.18523462116718292, current lr 9.625969983798073e-05, contrastive loss 3.6914944648742676, task loss 0.14831967651844025
2025-01-08 19:32:46,361 - root - INFO - [Train] Epoch 7 [17200/35182]: Training loss 0.10016823559999466, current lr 9.625827861629857e-05, contrastive loss 3.6076769828796387, task loss 0.06409146636724472
2025-01-08 19:34:27,039 - root - INFO - [Train] Epoch 7 [17300/35182]: Training loss 0.14869609475135803, current lr 9.625685739461641e-05, contrastive loss 3.576619863510132, task loss 0.11292989552021027
2025-01-08 19:36:07,443 - root - INFO - [Train] Epoch 7 [17400/35182]: Training loss 0.11139342188835144, current lr 9.625543617293426e-05, contrastive loss 3.6307058334350586, task loss 0.07508636265993118
2025-01-08 19:37:48,135 - root - INFO - [Train] Epoch 7 [17500/35182]: Training loss 0.1229444146156311, current lr 9.62540149512521e-05, contrastive loss 3.5498743057250977, task loss 0.08744567632675171
2025-01-08 22:23:25,488 - root - INFO - [Valid] Evaluation Results: HR@5:0.8066,NDCG@5:0.7445,HR@10:0.8642,NDCG@10:0.7632,HR@20:0.9186,NDCG@20:0.7770
2025-01-08 22:23:26,327 - root - INFO - Best model saved to ./outputs/best_model.pt
2025-01-08 22:23:36,368 - root - INFO - [Train] Epoch 7 [17600/35182]: Training loss 0.09685012698173523, current lr 9.625259372956994e-05, contrastive loss 3.5267295837402344, task loss 0.061582837253808975
2025-01-08 22:25:16,994 - root - INFO - [Train] Epoch 7 [17700/35182]: Training loss 0.09373771399259567, current lr 9.625117250788779e-05, contrastive loss 3.6042943000793457, task loss 0.05769477039575577
2025-01-08 22:26:57,395 - root - INFO - [Train] Epoch 7 [17800/35182]: Training loss 0.13363169133663177, current lr 9.624975128620563e-05, contrastive loss 3.5680644512176514, task loss 0.09795104712247849
2025-01-08 22:28:38,080 - root - INFO - [Train] Epoch 7 [17900/35182]: Training loss 0.11105877161026001, current lr 9.624833006452347e-05, contrastive loss 3.5740175247192383, task loss 0.0753185972571373
2025-01-08 22:30:18,467 - root - INFO - [Train] Epoch 7 [18000/35182]: Training loss 0.1753804236650467, current lr 9.624690884284132e-05, contrastive loss 3.477975845336914, task loss 0.14060066640377045
2025-01-08 22:31:59,151 - root - INFO - [Train] Epoch 7 [18100/35182]: Training loss 0.10987325012683868, current lr 9.624548762115916e-05, contrastive loss 3.433105945587158, task loss 0.07554218918085098
2025-01-08 22:33:39,846 - root - INFO - [Train] Epoch 7 [18200/35182]: Training loss 0.1327599138021469, current lr 9.624406639947699e-05, contrastive loss 3.5877044200897217, task loss 0.09688286483287811
2025-01-08 22:35:20,237 - root - INFO - [Train] Epoch 7 [18300/35182]: Training loss 0.14527855813503265, current lr 9.624264517779483e-05, contrastive loss 3.637157917022705, task loss 0.10890697687864304
2025-01-08 22:37:00,871 - root - INFO - [Train] Epoch 7 [18400/35182]: Training loss 0.1682368516921997, current lr 9.624122395611268e-05, contrastive loss 3.7320847511291504, task loss 0.13091599941253662
2025-01-08 22:38:41,253 - root - INFO - [Train] Epoch 7 [18500/35182]: Training loss 0.14218613505363464, current lr 9.623980273443052e-05, contrastive loss 3.6509547233581543, task loss 0.10567659139633179
2025-01-08 22:40:21,900 - root - INFO - [Train] Epoch 7 [18600/35182]: Training loss 0.10104919224977493, current lr 9.623838151274836e-05, contrastive loss 3.579875946044922, task loss 0.06525043398141861
2025-01-08 22:42:02,307 - root - INFO - [Train] Epoch 7 [18700/35182]: Training loss 0.1677609384059906, current lr 9.623696029106621e-05, contrastive loss 3.5721616744995117, task loss 0.13203932344913483
2025-01-08 22:43:42,978 - root - INFO - [Train] Epoch 7 [18800/35182]: Training loss 0.168202206492424, current lr 9.623553906938405e-05, contrastive loss 3.676551342010498, task loss 0.13143669068813324
2025-01-08 22:45:23,363 - root - INFO - [Train] Epoch 7 [18900/35182]: Training loss 0.150587260723114, current lr 9.62341178477019e-05, contrastive loss 3.555342674255371, task loss 0.1150338426232338
2025-01-08 22:47:03,949 - root - INFO - [Train] Epoch 7 [19000/35182]: Training loss 0.09895983338356018, current lr 9.623269662601974e-05, contrastive loss 3.703598737716675, task loss 0.06192385032773018
2025-01-08 22:48:44,347 - root - INFO - [Train] Epoch 7 [19100/35182]: Training loss 0.11863143742084503, current lr 9.623127540433757e-05, contrastive loss 3.78322696685791, task loss 0.0807991698384285
2025-01-08 22:50:25,012 - root - INFO - [Train] Epoch 7 [19200/35182]: Training loss 0.07672432065010071, current lr 9.622985418265541e-05, contrastive loss 3.613917112350464, task loss 0.040585145354270935
2025-01-08 22:52:05,397 - root - INFO - [Train] Epoch 7 [19300/35182]: Training loss 0.12797455489635468, current lr 9.622843296097325e-05, contrastive loss 3.4814987182617188, task loss 0.09315956383943558
2025-01-08 22:53:45,987 - root - INFO - [Train] Epoch 7 [19400/35182]: Training loss 0.11401069164276123, current lr 9.62270117392911e-05, contrastive loss 3.6834444999694824, task loss 0.07717624306678772
2025-01-08 22:55:26,383 - root - INFO - [Train] Epoch 7 [19500/35182]: Training loss 0.11390915513038635, current lr 9.622559051760894e-05, contrastive loss 3.627556800842285, task loss 0.07763358950614929
2025-01-08 22:57:07,053 - root - INFO - [Train] Epoch 7 [19600/35182]: Training loss 0.07745043933391571, current lr 9.622416929592678e-05, contrastive loss 3.518979549407959, task loss 0.04226064682006836
2025-01-08 22:58:47,433 - root - INFO - [Train] Epoch 7 [19700/35182]: Training loss 0.1171654462814331, current lr 9.622274807424463e-05, contrastive loss 3.562310218811035, task loss 0.08154234290122986
2025-01-08 23:00:28,036 - root - INFO - [Train] Epoch 7 [19800/35182]: Training loss 0.1209191381931305, current lr 9.622132685256247e-05, contrastive loss 3.601405143737793, task loss 0.08490508794784546
2025-01-08 23:02:08,431 - root - INFO - [Train] Epoch 7 [19900/35182]: Training loss 0.13858023285865784, current lr 9.621990563088031e-05, contrastive loss 3.606895923614502, task loss 0.10251127183437347
2025-01-08 23:03:49,028 - root - INFO - [Train] Epoch 7 [20000/35182]: Training loss 0.06996982544660568, current lr 9.621848440919816e-05, contrastive loss 3.5237698554992676, task loss 0.03473212942481041
2025-01-08 23:05:29,439 - root - INFO - [Train] Epoch 7 [20100/35182]: Training loss 0.14907024800777435, current lr 9.6217063187516e-05, contrastive loss 3.7330007553100586, task loss 0.11174024641513824
2025-01-08 23:07:10,086 - root - INFO - [Train] Epoch 7 [20200/35182]: Training loss 0.12125816941261292, current lr 9.621564196583383e-05, contrastive loss 3.6067099571228027, task loss 0.08519107103347778
2025-01-08 23:08:50,508 - root - INFO - [Train] Epoch 7 [20300/35182]: Training loss 0.09152194857597351, current lr 9.621422074415167e-05, contrastive loss 3.486278533935547, task loss 0.056659165769815445
2025-01-08 23:10:31,187 - root - INFO - [Train] Epoch 7 [20400/35182]: Training loss 0.0885729044675827, current lr 9.621279952246952e-05, contrastive loss 3.7346057891845703, task loss 0.051226843148469925
2025-01-08 23:12:11,856 - root - INFO - [Train] Epoch 7 [20500/35182]: Training loss 0.09062513709068298, current lr 9.621137830078736e-05, contrastive loss 3.7438223361968994, task loss 0.05318691208958626
2025-01-08 23:13:52,265 - root - INFO - [Train] Epoch 7 [20600/35182]: Training loss 0.1515475958585739, current lr 9.62099570791052e-05, contrastive loss 3.645008087158203, task loss 0.1150975152850151
2025-01-08 23:15:32,944 - root - INFO - [Train] Epoch 7 [20700/35182]: Training loss 0.15903417766094208, current lr 9.620853585742305e-05, contrastive loss 3.6777429580688477, task loss 0.12225674837827682
2025-01-08 23:17:13,343 - root - INFO - [Train] Epoch 7 [20800/35182]: Training loss 0.11798417568206787, current lr 9.620711463574089e-05, contrastive loss 3.587252140045166, task loss 0.082111656665802
2025-01-08 23:18:54,033 - root - INFO - [Train] Epoch 7 [20900/35182]: Training loss 0.11834502965211868, current lr 9.620569341405873e-05, contrastive loss 3.673015594482422, task loss 0.08161487430334091
2025-01-08 23:20:34,448 - root - INFO - [Train] Epoch 7 [21000/35182]: Training loss 0.11379515379667282, current lr 9.620427219237658e-05, contrastive loss 3.667240619659424, task loss 0.0771227478981018
2025-01-08 23:22:15,122 - root - INFO - [Train] Epoch 7 [21100/35182]: Training loss 0.14277935028076172, current lr 9.620285097069441e-05, contrastive loss 3.6405091285705566, task loss 0.10637426376342773
2025-01-08 23:23:55,517 - root - INFO - [Train] Epoch 7 [21200/35182]: Training loss 0.12387942522764206, current lr 9.620142974901225e-05, contrastive loss 3.4703030586242676, task loss 0.08917639404535294
2025-01-08 23:25:36,196 - root - INFO - [Train] Epoch 7 [21300/35182]: Training loss 0.11831150949001312, current lr 9.62000085273301e-05, contrastive loss 3.552338123321533, task loss 0.0827881321310997
2025-01-08 23:27:16,875 - root - INFO - [Train] Epoch 7 [21400/35182]: Training loss 0.13023652136325836, current lr 9.619858730564794e-05, contrastive loss 3.7375454902648926, task loss 0.09286106377840042
2025-01-08 23:28:57,269 - root - INFO - [Train] Epoch 7 [21500/35182]: Training loss 0.10927745699882507, current lr 9.619716608396578e-05, contrastive loss 3.6726324558258057, task loss 0.07255113869905472
2025-01-08 23:30:37,946 - root - INFO - [Train] Epoch 7 [21600/35182]: Training loss 0.1470855176448822, current lr 9.619574486228362e-05, contrastive loss 3.584127426147461, task loss 0.11124424636363983
2025-01-08 23:32:18,336 - root - INFO - [Train] Epoch 7 [21700/35182]: Training loss 0.13003797829151154, current lr 9.619432364060147e-05, contrastive loss 3.6023688316345215, task loss 0.09401429444551468
2025-01-08 23:33:58,924 - root - INFO - [Train] Epoch 7 [21800/35182]: Training loss 0.0955035611987114, current lr 9.619290241891931e-05, contrastive loss 3.640763759613037, task loss 0.059095922857522964
2025-01-08 23:35:39,337 - root - INFO - [Train] Epoch 7 [21900/35182]: Training loss 0.15107741951942444, current lr 9.619148119723715e-05, contrastive loss 3.4382448196411133, task loss 0.11669496446847916
2025-01-08 23:37:20,006 - root - INFO - [Train] Epoch 7 [22000/35182]: Training loss 0.12186215817928314, current lr 9.619005997555498e-05, contrastive loss 3.497255563735962, task loss 0.0868896022439003
2025-01-08 23:39:00,407 - root - INFO - [Train] Epoch 7 [22100/35182]: Training loss 0.07317674160003662, current lr 9.618863875387283e-05, contrastive loss 3.5006356239318848, task loss 0.038170382380485535
2025-01-08 23:40:41,083 - root - INFO - [Train] Epoch 7 [22200/35182]: Training loss 0.10974637418985367, current lr 9.618721753219067e-05, contrastive loss 3.8979389667510986, task loss 0.07076698541641235
2025-01-08 23:42:21,496 - root - INFO - [Train] Epoch 7 [22300/35182]: Training loss 0.10417494177818298, current lr 9.618579631050851e-05, contrastive loss 3.6053667068481445, task loss 0.0681212767958641
2025-01-08 23:44:02,179 - root - INFO - [Train] Epoch 7 [22400/35182]: Training loss 0.1317272037267685, current lr 9.618437508882636e-05, contrastive loss 3.6233153343200684, task loss 0.09549405425786972
2025-01-08 23:45:42,858 - root - INFO - [Train] Epoch 7 [22500/35182]: Training loss 0.12604522705078125, current lr 9.61829538671442e-05, contrastive loss 3.5326972007751465, task loss 0.09071826189756393
2025-01-08 23:47:23,257 - root - INFO - [Train] Epoch 7 [22600/35182]: Training loss 0.10653988271951675, current lr 9.618153264546204e-05, contrastive loss 3.628420829772949, task loss 0.07025567442178726
2025-01-08 23:49:03,944 - root - INFO - [Train] Epoch 7 [22700/35182]: Training loss 0.14070206880569458, current lr 9.618011142377989e-05, contrastive loss 3.635840892791748, task loss 0.10434365272521973
2025-01-08 23:50:44,343 - root - INFO - [Train] Epoch 7 [22800/35182]: Training loss 0.10544953495264053, current lr 9.617869020209773e-05, contrastive loss 3.5529143810272217, task loss 0.06992039084434509
2025-01-08 23:52:25,028 - root - INFO - [Train] Epoch 7 [22900/35182]: Training loss 0.15316128730773926, current lr 9.617726898041558e-05, contrastive loss 3.515827178955078, task loss 0.11800301820039749
2025-01-08 23:54:05,422 - root - INFO - [Train] Epoch 7 [23000/35182]: Training loss 0.12158273160457611, current lr 9.617584775873342e-05, contrastive loss 3.638765573501587, task loss 0.08519507199525833
2025-01-08 23:55:46,094 - root - INFO - [Train] Epoch 7 [23100/35182]: Training loss 0.08519300818443298, current lr 9.617442653705125e-05, contrastive loss 3.546679973602295, task loss 0.0497262142598629
2025-01-08 23:57:26,489 - root - INFO - [Train] Epoch 7 [23200/35182]: Training loss 0.12041860818862915, current lr 9.617300531536909e-05, contrastive loss 3.558195114135742, task loss 0.08483665436506271
2025-01-08 23:59:07,077 - root - INFO - [Train] Epoch 7 [23300/35182]: Training loss 0.10524114966392517, current lr 9.617158409368694e-05, contrastive loss 3.4821839332580566, task loss 0.0704193115234375
2025-01-09 00:00:47,486 - root - INFO - [Train] Epoch 7 [23400/35182]: Training loss 0.12706878781318665, current lr 9.617016287200478e-05, contrastive loss 3.5065534114837646, task loss 0.09200325608253479
2025-01-09 00:02:28,166 - root - INFO - [Train] Epoch 7 [23500/35182]: Training loss 0.14478710293769836, current lr 9.616874165032262e-05, contrastive loss 3.4903388023376465, task loss 0.10988371074199677
2025-01-09 00:04:08,841 - root - INFO - [Train] Epoch 7 [23600/35182]: Training loss 0.118308886885643, current lr 9.616732042864047e-05, contrastive loss 3.4322128295898438, task loss 0.08398675918579102
2025-01-09 00:05:49,253 - root - INFO - [Train] Epoch 7 [23700/35182]: Training loss 0.1373927891254425, current lr 9.616589920695831e-05, contrastive loss 3.539914608001709, task loss 0.10199365019798279
2025-01-09 00:07:29,933 - root - INFO - [Train] Epoch 7 [23800/35182]: Training loss 0.09797046333551407, current lr 9.616447798527615e-05, contrastive loss 3.5343072414398193, task loss 0.0626273900270462
2025-01-09 00:09:10,340 - root - INFO - [Train] Epoch 7 [23900/35182]: Training loss 0.10684138536453247, current lr 9.6163056763594e-05, contrastive loss 3.6338844299316406, task loss 0.07050254195928574
2025-01-09 00:10:51,027 - root - INFO - [Train] Epoch 7 [24000/35182]: Training loss 0.10172243416309357, current lr 9.616163554191183e-05, contrastive loss 3.3229541778564453, task loss 0.06849288940429688
2025-01-09 00:12:31,426 - root - INFO - [Train] Epoch 7 [24100/35182]: Training loss 0.09741085022687912, current lr 9.616021432022967e-05, contrastive loss 3.647796154022217, task loss 0.060932889580726624
2025-01-09 00:14:12,092 - root - INFO - [Train] Epoch 7 [24200/35182]: Training loss 0.11207273602485657, current lr 9.615879309854751e-05, contrastive loss 3.785684585571289, task loss 0.07421588897705078
2025-01-09 00:15:52,502 - root - INFO - [Train] Epoch 7 [24300/35182]: Training loss 0.13662275671958923, current lr 9.615737187686536e-05, contrastive loss 3.517062187194824, task loss 0.10145213454961777
2025-01-09 00:17:33,185 - root - INFO - [Train] Epoch 7 [24400/35182]: Training loss 0.13597774505615234, current lr 9.61559506551832e-05, contrastive loss 3.615501880645752, task loss 0.09982272237539291
2025-01-09 00:19:13,868 - root - INFO - [Train] Epoch 7 [24500/35182]: Training loss 0.10282579809427261, current lr 9.615452943350104e-05, contrastive loss 3.5859689712524414, task loss 0.06696610897779465
2025-01-09 00:20:54,270 - root - INFO - [Train] Epoch 7 [24600/35182]: Training loss 0.10594666004180908, current lr 9.615310821181889e-05, contrastive loss 3.626070976257324, task loss 0.06968595087528229
2025-01-09 00:22:34,947 - root - INFO - [Train] Epoch 7 [24700/35182]: Training loss 0.2106972336769104, current lr 9.615168699013673e-05, contrastive loss 3.570713520050049, task loss 0.17499010264873505
2025-01-09 00:24:15,352 - root - INFO - [Train] Epoch 7 [24800/35182]: Training loss 0.13499605655670166, current lr 9.615026576845457e-05, contrastive loss 3.595388174057007, task loss 0.09904216974973679
2025-01-09 00:25:56,032 - root - INFO - [Train] Epoch 7 [24900/35182]: Training loss 0.19335797429084778, current lr 9.614884454677242e-05, contrastive loss 3.6224443912506104, task loss 0.1571335345506668
2025-01-09 00:27:36,436 - root - INFO - [Train] Epoch 7 [25000/35182]: Training loss 0.15767543017864227, current lr 9.614742332509025e-05, contrastive loss 3.548736095428467, task loss 0.12218806892633438
2025-01-09 00:29:17,124 - root - INFO - [Train] Epoch 7 [25100/35182]: Training loss 0.1484379768371582, current lr 9.614600210340809e-05, contrastive loss 3.5356476306915283, task loss 0.11308149248361588
2025-01-09 00:30:57,798 - root - INFO - [Train] Epoch 7 [25200/35182]: Training loss 0.13460823893547058, current lr 9.614458088172593e-05, contrastive loss 3.4850106239318848, task loss 0.09975812584161758
2025-01-09 00:32:38,192 - root - INFO - [Train] Epoch 7 [25300/35182]: Training loss 0.09242406487464905, current lr 9.614315966004378e-05, contrastive loss 3.558566093444824, task loss 0.05683840438723564
2025-01-09 00:34:18,880 - root - INFO - [Train] Epoch 7 [25400/35182]: Training loss 0.07618635892868042, current lr 9.614173843836162e-05, contrastive loss 3.6394002437591553, task loss 0.03979235887527466
2025-01-09 00:35:59,299 - root - INFO - [Train] Epoch 7 [25500/35182]: Training loss 0.13175083696842194, current lr 9.614031721667946e-05, contrastive loss 3.596281051635742, task loss 0.09578803181648254
2025-01-09 00:37:39,993 - root - INFO - [Train] Epoch 7 [25600/35182]: Training loss 0.1538752019405365, current lr 9.61388959949973e-05, contrastive loss 3.6807355880737305, task loss 0.117067851126194
2025-01-09 00:39:20,403 - root - INFO - [Train] Epoch 7 [25700/35182]: Training loss 0.13303238153457642, current lr 9.613747477331515e-05, contrastive loss 3.600177764892578, task loss 0.09703060984611511
2025-01-09 00:41:01,053 - root - INFO - [Train] Epoch 7 [25800/35182]: Training loss 0.09965664148330688, current lr 9.613605355163299e-05, contrastive loss 3.4615118503570557, task loss 0.06504151970148087
2025-01-09 00:42:41,457 - root - INFO - [Train] Epoch 7 [25900/35182]: Training loss 0.1394735872745514, current lr 9.613463232995084e-05, contrastive loss 3.654325485229492, task loss 0.10293033719062805
2025-01-09 00:44:22,130 - root - INFO - [Train] Epoch 7 [26000/35182]: Training loss 0.14079290628433228, current lr 9.613321110826867e-05, contrastive loss 3.5857536792755127, task loss 0.10493537038564682
2025-01-09 00:46:02,820 - root - INFO - [Train] Epoch 7 [26100/35182]: Training loss 0.1002337783575058, current lr 9.613178988658651e-05, contrastive loss 3.5860095024108887, task loss 0.0643736869096756
2025-01-09 00:47:43,218 - root - INFO - [Train] Epoch 7 [26200/35182]: Training loss 0.08633381873369217, current lr 9.613036866490435e-05, contrastive loss 3.4884424209594727, task loss 0.05144939571619034
2025-01-09 00:49:23,912 - root - INFO - [Train] Epoch 7 [26300/35182]: Training loss 0.09988552331924438, current lr 9.61289474432222e-05, contrastive loss 3.68983793258667, task loss 0.06298714876174927
2025-01-09 00:51:04,332 - root - INFO - [Train] Epoch 7 [26400/35182]: Training loss 0.13005517423152924, current lr 9.612752622154004e-05, contrastive loss 3.549450397491455, task loss 0.0945606678724289
2025-01-09 00:52:45,000 - root - INFO - [Train] Epoch 7 [26500/35182]: Training loss 0.07858802378177643, current lr 9.612610499985788e-05, contrastive loss 3.602220058441162, task loss 0.04256582260131836
2025-01-09 00:54:25,399 - root - INFO - [Train] Epoch 7 [26600/35182]: Training loss 0.10093700885772705, current lr 9.612468377817573e-05, contrastive loss 3.46931791305542, task loss 0.06624382734298706
2025-01-09 00:56:06,078 - root - INFO - [Train] Epoch 7 [26700/35182]: Training loss 0.12446509301662445, current lr 9.612326255649357e-05, contrastive loss 3.5432989597320557, task loss 0.08903210610151291
2025-01-09 00:57:46,473 - root - INFO - [Train] Epoch 7 [26800/35182]: Training loss 0.09917981177568436, current lr 9.612184133481141e-05, contrastive loss 3.6867222785949707, task loss 0.06231258809566498
2025-01-09 00:59:27,158 - root - INFO - [Train] Epoch 7 [26900/35182]: Training loss 0.15937164425849915, current lr 9.612042011312926e-05, contrastive loss 3.641420841217041, task loss 0.12295743823051453
2025-01-09 01:01:07,840 - root - INFO - [Train] Epoch 7 [27000/35182]: Training loss 0.08683881163597107, current lr 9.611899889144709e-05, contrastive loss 3.445305347442627, task loss 0.052385762333869934
2025-01-09 01:02:48,234 - root - INFO - [Train] Epoch 7 [27100/35182]: Training loss 0.1315280795097351, current lr 9.611757766976493e-05, contrastive loss 3.5582451820373535, task loss 0.09594562649726868
2025-01-09 01:04:28,807 - root - INFO - [Train] Epoch 7 [27200/35182]: Training loss 0.17455032467842102, current lr 9.611615644808277e-05, contrastive loss 3.553988456726074, task loss 0.1390104442834854
2025-01-09 01:06:09,210 - root - INFO - [Train] Epoch 7 [27300/35182]: Training loss 0.19417838752269745, current lr 9.611473522640062e-05, contrastive loss 3.652719020843506, task loss 0.15765120089054108
2025-01-09 01:07:49,889 - root - INFO - [Train] Epoch 7 [27400/35182]: Training loss 0.13937129080295563, current lr 9.611331400471846e-05, contrastive loss 3.6351571083068848, task loss 0.10301972180604935
2025-01-09 01:09:30,285 - root - INFO - [Train] Epoch 7 [27500/35182]: Training loss 0.10209733247756958, current lr 9.61118927830363e-05, contrastive loss 3.486527442932129, task loss 0.06723205745220184
2025-01-09 01:11:10,965 - root - INFO - [Train] Epoch 7 [27600/35182]: Training loss 0.09175075590610504, current lr 9.611047156135415e-05, contrastive loss 3.6157586574554443, task loss 0.05559316650032997
2025-01-09 01:12:51,358 - root - INFO - [Train] Epoch 7 [27700/35182]: Training loss 0.07742030918598175, current lr 9.610905033967199e-05, contrastive loss 3.515659809112549, task loss 0.042263712733983994
2025-01-09 01:14:31,936 - root - INFO - [Train] Epoch 7 [27800/35182]: Training loss 0.14767318964004517, current lr 9.610762911798983e-05, contrastive loss 3.4566564559936523, task loss 0.1131066307425499
2025-01-09 01:16:12,329 - root - INFO - [Train] Epoch 7 [27900/35182]: Training loss 0.11661233752965927, current lr 9.610620789630768e-05, contrastive loss 3.462505340576172, task loss 0.08198728412389755
2025-01-09 01:17:52,899 - root - INFO - [Train] Epoch 7 [28000/35182]: Training loss 0.1127619668841362, current lr 9.61047866746255e-05, contrastive loss 3.3203208446502686, task loss 0.07955875992774963
2025-01-09 01:19:33,296 - root - INFO - [Train] Epoch 7 [28100/35182]: Training loss 0.11453983187675476, current lr 9.610336545294335e-05, contrastive loss 3.655538558959961, task loss 0.07798444479703903
2025-01-09 01:21:13,976 - root - INFO - [Train] Epoch 7 [28200/35182]: Training loss 0.11378878355026245, current lr 9.61019442312612e-05, contrastive loss 3.5775673389434814, task loss 0.0780131071805954
2025-01-09 01:22:54,393 - root - INFO - [Train] Epoch 7 [28300/35182]: Training loss 0.1185944676399231, current lr 9.610052300957904e-05, contrastive loss 3.72959041595459, task loss 0.08129855990409851
2025-01-09 01:24:35,063 - root - INFO - [Train] Epoch 7 [28400/35182]: Training loss 0.08896110206842422, current lr 9.609910178789688e-05, contrastive loss 3.7517426013946533, task loss 0.0514436773955822
2025-01-09 01:26:15,445 - root - INFO - [Train] Epoch 7 [28500/35182]: Training loss 0.08860443532466888, current lr 9.609768056621472e-05, contrastive loss 3.7489919662475586, task loss 0.051114521920681
2025-01-09 01:27:56,029 - root - INFO - [Train] Epoch 7 [28600/35182]: Training loss 0.0923084244132042, current lr 9.609625934453257e-05, contrastive loss 3.6761627197265625, task loss 0.055546797811985016
2025-01-09 01:29:36,425 - root - INFO - [Train] Epoch 7 [28700/35182]: Training loss 0.1393025815486908, current lr 9.609483812285041e-05, contrastive loss 3.4610018730163574, task loss 0.10469257086515427
2025-01-09 01:31:17,107 - root - INFO - [Train] Epoch 7 [28800/35182]: Training loss 0.09373681247234344, current lr 9.609341690116825e-05, contrastive loss 3.5708742141723633, task loss 0.058028072118759155
2025-01-09 01:32:57,794 - root - INFO - [Train] Epoch 7 [28900/35182]: Training loss 0.09136553108692169, current lr 9.60919956794861e-05, contrastive loss 3.4870944023132324, task loss 0.056494589895009995
2025-01-09 01:34:38,206 - root - INFO - [Train] Epoch 7 [29000/35182]: Training loss 0.09099918603897095, current lr 9.609057445780393e-05, contrastive loss 3.644577980041504, task loss 0.05455340817570686
2025-01-09 01:36:18,885 - root - INFO - [Train] Epoch 7 [29100/35182]: Training loss 0.10623733699321747, current lr 9.608915323612177e-05, contrastive loss 3.558168411254883, task loss 0.07065565884113312
2025-01-09 01:37:59,299 - root - INFO - [Train] Epoch 7 [29200/35182]: Training loss 0.14970028400421143, current lr 9.608773201443961e-05, contrastive loss 3.521839141845703, task loss 0.11448189616203308
2025-01-09 01:39:39,978 - root - INFO - [Train] Epoch 7 [29300/35182]: Training loss 0.09634490311145782, current lr 9.608631079275746e-05, contrastive loss 3.3950953483581543, task loss 0.062393952161073685
2025-01-09 01:41:20,390 - root - INFO - [Train] Epoch 7 [29400/35182]: Training loss 0.12778183817863464, current lr 9.60848895710753e-05, contrastive loss 3.670177936553955, task loss 0.09108005464076996
2025-01-09 01:43:01,056 - root - INFO - [Train] Epoch 7 [29500/35182]: Training loss 0.1274665892124176, current lr 9.608346834939314e-05, contrastive loss 3.704315185546875, task loss 0.09042343497276306
2025-01-09 01:44:41,470 - root - INFO - [Train] Epoch 7 [29600/35182]: Training loss 0.12227103859186172, current lr 9.608204712771099e-05, contrastive loss 3.5832204818725586, task loss 0.08643883466720581
2025-01-09 01:46:22,137 - root - INFO - [Train] Epoch 7 [29700/35182]: Training loss 0.14714393019676208, current lr 9.608062590602883e-05, contrastive loss 3.5185604095458984, task loss 0.1119583323597908
2025-01-09 01:48:02,822 - root - INFO - [Train] Epoch 7 [29800/35182]: Training loss 0.10559984296560287, current lr 9.607920468434667e-05, contrastive loss 3.55526065826416, task loss 0.07004723697900772
2025-01-09 01:49:43,245 - root - INFO - [Train] Epoch 7 [29900/35182]: Training loss 0.11944624781608582, current lr 9.60777834626645e-05, contrastive loss 3.6539769172668457, task loss 0.08290648460388184
2025-01-09 01:51:23,899 - root - INFO - [Train] Epoch 7 [30000/35182]: Training loss 0.12825042009353638, current lr 9.607636224098235e-05, contrastive loss 3.608750581741333, task loss 0.09216292202472687
2025-01-09 01:53:04,312 - root - INFO - [Train] Epoch 7 [30100/35182]: Training loss 0.08637255430221558, current lr 9.607494101930019e-05, contrastive loss 3.639549732208252, task loss 0.04997706040740013
2025-01-09 01:54:44,990 - root - INFO - [Train] Epoch 7 [30200/35182]: Training loss 0.11569379270076752, current lr 9.607351979761803e-05, contrastive loss 3.5292577743530273, task loss 0.080401211977005
2025-01-09 01:56:25,404 - root - INFO - [Train] Epoch 7 [30300/35182]: Training loss 0.15079918503761292, current lr 9.607209857593588e-05, contrastive loss 3.649779796600342, task loss 0.11430138349533081
2025-01-09 01:58:06,079 - root - INFO - [Train] Epoch 7 [30400/35182]: Training loss 0.10302890837192535, current lr 9.607067735425372e-05, contrastive loss 3.459260940551758, task loss 0.06843629479408264
2025-01-09 01:59:46,488 - root - INFO - [Train] Epoch 7 [30500/35182]: Training loss 0.13416405022144318, current lr 9.606925613257156e-05, contrastive loss 3.5077857971191406, task loss 0.09908619523048401
2025-01-09 02:01:27,163 - root - INFO - [Train] Epoch 7 [30600/35182]: Training loss 0.11246444284915924, current lr 9.606783491088941e-05, contrastive loss 3.7114944458007812, task loss 0.07534949481487274
2025-01-09 02:03:07,841 - root - INFO - [Train] Epoch 7 [30700/35182]: Training loss 0.0786031037569046, current lr 9.606641368920725e-05, contrastive loss 3.4155349731445312, task loss 0.044447749853134155
2025-01-09 02:04:48,261 - root - INFO - [Train] Epoch 7 [30800/35182]: Training loss 0.09474702179431915, current lr 9.60649924675251e-05, contrastive loss 3.4820973873138428, task loss 0.05992604419589043
2025-01-09 02:06:28,936 - root - INFO - [Train] Epoch 7 [30900/35182]: Training loss 0.16030722856521606, current lr 9.606357124584294e-05, contrastive loss 3.691926956176758, task loss 0.12338796257972717
2025-01-09 02:08:09,347 - root - INFO - [Train] Epoch 7 [31000/35182]: Training loss 0.10796016454696655, current lr 9.606215002416077e-05, contrastive loss 3.535352945327759, task loss 0.0726066380739212
2025-01-09 02:09:50,022 - root - INFO - [Train] Epoch 7 [31100/35182]: Training loss 0.10341408103704453, current lr 9.606072880247861e-05, contrastive loss 3.5697200298309326, task loss 0.06771688163280487
2025-01-09 02:11:30,420 - root - INFO - [Train] Epoch 7 [31200/35182]: Training loss 0.11101864278316498, current lr 9.605930758079645e-05, contrastive loss 3.564324140548706, task loss 0.07537540048360825
2025-01-09 02:13:11,103 - root - INFO - [Train] Epoch 7 [31300/35182]: Training loss 0.12776704132556915, current lr 9.60578863591143e-05, contrastive loss 3.693840503692627, task loss 0.09082863479852676
2025-01-09 02:14:51,523 - root - INFO - [Train] Epoch 7 [31400/35182]: Training loss 0.09662285447120667, current lr 9.605646513743214e-05, contrastive loss 3.579319953918457, task loss 0.06082965433597565
2025-01-09 02:16:32,197 - root - INFO - [Train] Epoch 7 [31500/35182]: Training loss 0.11597558856010437, current lr 9.605504391574998e-05, contrastive loss 3.674628257751465, task loss 0.07922931015491486
2025-01-09 02:18:12,859 - root - INFO - [Train] Epoch 7 [31600/35182]: Training loss 0.13570788502693176, current lr 9.605362269406783e-05, contrastive loss 3.443739891052246, task loss 0.10127048194408417
2025-01-09 02:19:53,259 - root - INFO - [Train] Epoch 7 [31700/35182]: Training loss 0.13342179358005524, current lr 9.605220147238567e-05, contrastive loss 3.6220145225524902, task loss 0.0972016453742981
2025-01-09 02:21:33,948 - root - INFO - [Train] Epoch 7 [31800/35182]: Training loss 0.13680249452590942, current lr 9.605078025070351e-05, contrastive loss 3.5113439559936523, task loss 0.10168905556201935
2025-01-09 02:23:14,353 - root - INFO - [Train] Epoch 7 [31900/35182]: Training loss 0.10133689641952515, current lr 9.604935902902134e-05, contrastive loss 3.459764003753662, task loss 0.06673925369977951
2025-01-09 02:24:55,030 - root - INFO - [Train] Epoch 7 [32000/35182]: Training loss 0.10107427090406418, current lr 9.604793780733919e-05, contrastive loss 3.508409023284912, task loss 0.06599017977714539
2025-01-09 02:26:35,455 - root - INFO - [Train] Epoch 7 [32100/35182]: Training loss 0.0801745280623436, current lr 9.604651658565703e-05, contrastive loss 3.522172689437866, task loss 0.04495280236005783
2025-01-09 02:28:16,127 - root - INFO - [Train] Epoch 7 [32200/35182]: Training loss 0.17011822760105133, current lr 9.604509536397487e-05, contrastive loss 3.574676752090454, task loss 0.13437145948410034
2025-01-09 02:29:56,533 - root - INFO - [Train] Epoch 7 [32300/35182]: Training loss 0.1660216897726059, current lr 9.604367414229272e-05, contrastive loss 3.6057772636413574, task loss 0.1299639195203781
2025-01-09 02:31:37,203 - root - INFO - [Train] Epoch 7 [32400/35182]: Training loss 0.1281728744506836, current lr 9.604225292061056e-05, contrastive loss 3.732595443725586, task loss 0.09084691852331161
2025-01-09 02:33:17,881 - root - INFO - [Train] Epoch 7 [32500/35182]: Training loss 0.14745262265205383, current lr 9.60408316989284e-05, contrastive loss 3.5396904945373535, task loss 0.1120557188987732
2025-01-09 02:34:58,272 - root - INFO - [Train] Epoch 7 [32600/35182]: Training loss 0.12971149384975433, current lr 9.603941047724625e-05, contrastive loss 3.5977375507354736, task loss 0.09373411536216736
2025-01-09 02:36:38,965 - root - INFO - [Train] Epoch 7 [32700/35182]: Training loss 0.09225200861692429, current lr 9.603798925556409e-05, contrastive loss 3.483771800994873, task loss 0.05741428956389427
2025-01-09 02:38:19,360 - root - INFO - [Train] Epoch 7 [32800/35182]: Training loss 0.09934894740581512, current lr 9.603656803388192e-05, contrastive loss 3.5747413635253906, task loss 0.0636015310883522
2025-01-09 02:40:00,044 - root - INFO - [Train] Epoch 7 [32900/35182]: Training loss 0.11839856207370758, current lr 9.603514681219978e-05, contrastive loss 3.545121669769287, task loss 0.08294735103845596
2025-01-09 02:41:40,457 - root - INFO - [Train] Epoch 7 [33000/35182]: Training loss 0.15007975697517395, current lr 9.603372559051761e-05, contrastive loss 3.548347234725952, task loss 0.11459627747535706
2025-01-09 02:43:21,132 - root - INFO - [Train] Epoch 7 [33100/35182]: Training loss 0.09304116666316986, current lr 9.603230436883545e-05, contrastive loss 3.501054286956787, task loss 0.0580306276679039
2025-01-09 02:45:01,819 - root - INFO - [Train] Epoch 7 [33200/35182]: Training loss 0.08670758455991745, current lr 9.60308831471533e-05, contrastive loss 3.5740303993225098, task loss 0.05096727982163429
2025-01-09 02:46:42,215 - root - INFO - [Train] Epoch 7 [33300/35182]: Training loss 0.10859546810388565, current lr 9.602946192547114e-05, contrastive loss 3.593897819519043, task loss 0.07265648990869522
2025-01-09 02:48:22,904 - root - INFO - [Train] Epoch 7 [33400/35182]: Training loss 0.16536767780780792, current lr 9.602804070378898e-05, contrastive loss 3.6014349460601807, task loss 0.129353329539299
2025-01-09 02:50:03,319 - root - INFO - [Train] Epoch 7 [33500/35182]: Training loss 0.13989219069480896, current lr 9.602661948210683e-05, contrastive loss 3.6895108222961426, task loss 0.10299708694219589
2025-01-09 02:51:43,986 - root - INFO - [Train] Epoch 7 [33600/35182]: Training loss 0.12344678491353989, current lr 9.602519826042467e-05, contrastive loss 3.6630451679229736, task loss 0.08681633323431015
2025-01-09 02:53:24,397 - root - INFO - [Train] Epoch 7 [33700/35182]: Training loss 0.15259204804897308, current lr 9.602377703874251e-05, contrastive loss 3.538130521774292, task loss 0.1172107383608818
2025-01-09 02:55:05,076 - root - INFO - [Train] Epoch 7 [33800/35182]: Training loss 0.10736098885536194, current lr 9.602235581706036e-05, contrastive loss 3.584477424621582, task loss 0.07151621580123901
2025-01-09 02:56:45,499 - root - INFO - [Train] Epoch 7 [33900/35182]: Training loss 0.13960374891757965, current lr 9.602093459537819e-05, contrastive loss 3.663148880004883, task loss 0.10297226160764694
2025-01-09 02:58:26,159 - root - INFO - [Train] Epoch 7 [34000/35182]: Training loss 0.14488187432289124, current lr 9.601951337369603e-05, contrastive loss 3.713749408721924, task loss 0.10774438083171844
2025-01-09 03:00:06,838 - root - INFO - [Train] Epoch 7 [34100/35182]: Training loss 0.07139512151479721, current lr 9.601809215201387e-05, contrastive loss 3.5894622802734375, task loss 0.03550050035119057
2025-01-09 03:01:47,245 - root - INFO - [Train] Epoch 7 [34200/35182]: Training loss 0.1337602287530899, current lr 9.601667093033172e-05, contrastive loss 3.604358673095703, task loss 0.09771664440631866
2025-01-09 03:03:27,916 - root - INFO - [Train] Epoch 7 [34300/35182]: Training loss 0.12854856252670288, current lr 9.601524970864956e-05, contrastive loss 3.749692916870117, task loss 0.09105163812637329
2025-01-09 03:05:08,314 - root - INFO - [Train] Epoch 7 [34400/35182]: Training loss 0.13007918000221252, current lr 9.60138284869674e-05, contrastive loss 3.3371031284332275, task loss 0.09670815616846085
2025-01-09 03:06:48,894 - root - INFO - [Train] Epoch 7 [34500/35182]: Training loss 0.1347091794013977, current lr 9.601240726528525e-05, contrastive loss 3.5893208980560303, task loss 0.09881597757339478
2025-01-09 03:08:29,289 - root - INFO - [Train] Epoch 7 [34600/35182]: Training loss 0.08382225781679153, current lr 9.601098604360309e-05, contrastive loss 3.4673497676849365, task loss 0.04914876073598862
2025-01-09 03:10:09,973 - root - INFO - [Train] Epoch 7 [34700/35182]: Training loss 0.13103844225406647, current lr 9.600956482192093e-05, contrastive loss 3.381193161010742, task loss 0.09722651541233063
2025-01-09 03:11:50,382 - root - INFO - [Train] Epoch 7 [34800/35182]: Training loss 0.10406926274299622, current lr 9.600814360023876e-05, contrastive loss 3.6783857345581055, task loss 0.06728540360927582
2025-01-09 03:13:31,057 - root - INFO - [Train] Epoch 7 [34900/35182]: Training loss 0.14457368850708008, current lr 9.600672237855662e-05, contrastive loss 3.5727992057800293, task loss 0.10884570330381393
2025-01-09 03:15:11,442 - root - INFO - [Train] Epoch 7 [35000/35182]: Training loss 0.11314954608678818, current lr 9.600530115687445e-05, contrastive loss 3.522062301635742, task loss 0.07792892307043076
2025-01-09 03:16:52,027 - root - INFO - [Train] Epoch 7 [35100/35182]: Training loss 0.13353389501571655, current lr 9.600387993519229e-05, contrastive loss 3.5163941383361816, task loss 0.09836994856595993
2025-01-09 06:01:58,621 - root - INFO - [Valid] Evaluation Results: HR@5:0.8032,NDCG@5:0.7433,HR@10:0.8593,NDCG@10:0.7615,HR@20:0.9134,NDCG@20:0.7752
2025-01-09 06:02:01,653 - root - INFO - [Train] Epoch 7: Overall Training loss 0.0
2025-01-09 06:02:01,653 - root - INFO - [Time] Epoch 7: Time taken 15.29 hours
2025-01-09 06:02:06,945 - root - INFO - [Train] Epoch 8 [0/35182]: Training loss 0.11488472670316696, current lr 9.600271453341293e-05, contrastive loss 3.6708006858825684, task loss 0.07817672193050385
2025-01-09 06:03:47,386 - root - INFO - [Train] Epoch 8 [100/35182]: Training loss 0.10768043249845505, current lr 9.600129331173077e-05, contrastive loss 3.7709715366363525, task loss 0.06997071951627731
2025-01-09 06:05:28,049 - root - INFO - [Train] Epoch 8 [200/35182]: Training loss 0.06842228770256042, current lr 9.599987209004861e-05, contrastive loss 3.5617454051971436, task loss 0.032804835587739944
2025-01-09 06:07:08,452 - root - INFO - [Train] Epoch 8 [300/35182]: Training loss 0.09553831815719604, current lr 9.599845086836646e-05, contrastive loss 3.583542823791504, task loss 0.05970288813114166
2025-01-09 06:08:49,123 - root - INFO - [Train] Epoch 8 [400/35182]: Training loss 0.18004409968852997, current lr 9.599702964668429e-05, contrastive loss 3.6632580757141113, task loss 0.1434115171432495
2025-01-09 06:10:29,805 - root - INFO - [Train] Epoch 8 [500/35182]: Training loss 0.07412000000476837, current lr 9.599560842500214e-05, contrastive loss 3.5584378242492676, task loss 0.038535624742507935
2025-01-09 06:12:10,209 - root - INFO - [Train] Epoch 8 [600/35182]: Training loss 0.17829975485801697, current lr 9.599418720331997e-05, contrastive loss 3.521292209625244, task loss 0.14308683574199677
2025-01-09 06:13:50,890 - root - INFO - [Train] Epoch 8 [700/35182]: Training loss 0.10468802601099014, current lr 9.599276598163782e-05, contrastive loss 3.6774158477783203, task loss 0.06791386753320694
2025-01-09 06:15:31,342 - root - INFO - [Train] Epoch 8 [800/35182]: Training loss 0.1167307123541832, current lr 9.599134475995566e-05, contrastive loss 3.635983943939209, task loss 0.08037087321281433
2025-01-09 06:17:11,976 - root - INFO - [Train] Epoch 8 [900/35182]: Training loss 0.14241380989551544, current lr 9.59899235382735e-05, contrastive loss 3.6220617294311523, task loss 0.1061931923031807
2025-01-09 06:18:52,383 - root - INFO - [Train] Epoch 8 [1000/35182]: Training loss 0.14250919222831726, current lr 9.598850231659135e-05, contrastive loss 3.920346260070801, task loss 0.10330572724342346
2025-01-09 06:20:33,055 - root - INFO - [Train] Epoch 8 [1100/35182]: Training loss 0.10890477895736694, current lr 9.598708109490919e-05, contrastive loss 3.7483344078063965, task loss 0.07142143696546555
2025-01-09 06:22:13,456 - root - INFO - [Train] Epoch 8 [1200/35182]: Training loss 0.11681708693504333, current lr 9.598565987322703e-05, contrastive loss 3.5721802711486816, task loss 0.08109528571367264
2025-01-09 06:23:54,149 - root - INFO - [Train] Epoch 8 [1300/35182]: Training loss 0.08910450339317322, current lr 9.598423865154488e-05, contrastive loss 3.7348928451538086, task loss 0.05175558105111122
2025-01-09 06:25:34,825 - root - INFO - [Train] Epoch 8 [1400/35182]: Training loss 0.09931695461273193, current lr 9.598281742986272e-05, contrastive loss 3.528928279876709, task loss 0.06402766704559326
2025-01-09 06:27:15,232 - root - INFO - [Train] Epoch 8 [1500/35182]: Training loss 0.09304997324943542, current lr 9.598139620818055e-05, contrastive loss 3.5670809745788574, task loss 0.057379160076379776
2025-01-09 06:28:55,904 - root - INFO - [Train] Epoch 8 [1600/35182]: Training loss 0.10515686869621277, current lr 9.59799749864984e-05, contrastive loss 3.5403285026550293, task loss 0.06975358724594116
2025-01-09 06:30:36,311 - root - INFO - [Train] Epoch 8 [1700/35182]: Training loss 0.12991786003112793, current lr 9.597855376481624e-05, contrastive loss 3.6286401748657227, task loss 0.09363146126270294
2025-01-09 06:32:16,996 - root - INFO - [Train] Epoch 8 [1800/35182]: Training loss 0.1265878826379776, current lr 9.597713254313408e-05, contrastive loss 3.54581880569458, task loss 0.09112969040870667
2025-01-09 06:33:57,387 - root - INFO - [Train] Epoch 8 [1900/35182]: Training loss 0.10990045964717865, current lr 9.597571132145192e-05, contrastive loss 3.6481237411499023, task loss 0.0734192281961441
2025-01-09 06:35:38,061 - root - INFO - [Train] Epoch 8 [2000/35182]: Training loss 0.12079378962516785, current lr 9.597429009976977e-05, contrastive loss 3.394756317138672, task loss 0.08684622496366501
2025-01-09 06:37:18,443 - root - INFO - [Train] Epoch 8 [2100/35182]: Training loss 0.10500963777303696, current lr 9.597286887808761e-05, contrastive loss 3.6327414512634277, task loss 0.0686822235584259
2025-01-09 06:38:59,038 - root - INFO - [Train] Epoch 8 [2200/35182]: Training loss 0.11780454218387604, current lr 9.597144765640545e-05, contrastive loss 3.5288314819335938, task loss 0.08251622319221497
2025-01-09 06:40:39,435 - root - INFO - [Train] Epoch 8 [2300/35182]: Training loss 0.10269774496555328, current lr 9.59700264347233e-05, contrastive loss 3.6410393714904785, task loss 0.06628735363483429
2025-01-09 06:42:20,008 - root - INFO - [Train] Epoch 8 [2400/35182]: Training loss 0.10533638298511505, current lr 9.596860521304113e-05, contrastive loss 3.5846266746520996, task loss 0.06949011981487274
2025-01-09 06:44:00,411 - root - INFO - [Train] Epoch 8 [2500/35182]: Training loss 0.15560394525527954, current lr 9.596718399135897e-05, contrastive loss 3.5928189754486084, task loss 0.11967574805021286
2025-01-09 06:45:41,091 - root - INFO - [Train] Epoch 8 [2600/35182]: Training loss 0.14882104098796844, current lr 9.596576276967681e-05, contrastive loss 3.6261138916015625, task loss 0.11255990713834763
2025-01-09 06:47:21,488 - root - INFO - [Train] Epoch 8 [2700/35182]: Training loss 0.1208675354719162, current lr 9.596434154799466e-05, contrastive loss 3.683675765991211, task loss 0.08403077721595764
2025-01-09 06:49:02,187 - root - INFO - [Train] Epoch 8 [2800/35182]: Training loss 0.1371656060218811, current lr 9.59629203263125e-05, contrastive loss 3.611083984375, task loss 0.10105476528406143
2025-01-09 06:50:42,853 - root - INFO - [Train] Epoch 8 [2900/35182]: Training loss 0.13844820857048035, current lr 9.596149910463034e-05, contrastive loss 3.516817569732666, task loss 0.10328003019094467
2025-01-09 06:52:23,248 - root - INFO - [Train] Epoch 8 [3000/35182]: Training loss 0.07518233358860016, current lr 9.596007788294819e-05, contrastive loss 3.535541534423828, task loss 0.03982691839337349
2025-01-09 06:54:03,821 - root - INFO - [Train] Epoch 8 [3100/35182]: Training loss 0.1238749772310257, current lr 9.595865666126603e-05, contrastive loss 3.5826241970062256, task loss 0.08804873377084732
2025-01-09 06:55:44,206 - root - INFO - [Train] Epoch 8 [3200/35182]: Training loss 0.07658839225769043, current lr 9.595723543958387e-05, contrastive loss 3.649228096008301, task loss 0.040096115320920944
2025-01-09 06:57:24,597 - root - INFO - [Train] Epoch 8 [3300/35182]: Training loss 0.132726788520813, current lr 9.595581421790172e-05, contrastive loss 3.6153335571289062, task loss 0.09657345712184906
2025-01-09 06:59:05,182 - root - INFO - [Train] Epoch 8 [3400/35182]: Training loss 0.17596961557865143, current lr 9.595439299621956e-05, contrastive loss 3.691995620727539, task loss 0.13904966413974762
2025-01-09 07:00:46,004 - root - INFO - [Train] Epoch 8 [3500/35182]: Training loss 0.14026789367198944, current lr 9.595297177453739e-05, contrastive loss 3.4951212406158447, task loss 0.10531668365001678
2025-01-09 07:02:26,405 - root - INFO - [Train] Epoch 8 [3600/35182]: Training loss 0.08602295815944672, current lr 9.595155055285523e-05, contrastive loss 3.5618557929992676, task loss 0.05040439963340759
2025-01-09 07:04:07,081 - root - INFO - [Train] Epoch 8 [3700/35182]: Training loss 0.1490442156791687, current lr 9.595012933117308e-05, contrastive loss 3.755861520767212, task loss 0.11148559302091599
2025-01-09 07:05:47,486 - root - INFO - [Train] Epoch 8 [3800/35182]: Training loss 0.0966840535402298, current lr 9.594870810949092e-05, contrastive loss 3.541036367416382, task loss 0.06127369403839111
2025-01-09 07:07:28,168 - root - INFO - [Train] Epoch 8 [3900/35182]: Training loss 0.09714457392692566, current lr 9.594728688780876e-05, contrastive loss 3.593071460723877, task loss 0.06121385470032692
2025-01-09 07:09:08,848 - root - INFO - [Train] Epoch 8 [4000/35182]: Training loss 0.1355605125427246, current lr 9.594586566612661e-05, contrastive loss 3.6416308879852295, task loss 0.09914419800043106
2025-01-09 07:10:49,248 - root - INFO - [Train] Epoch 8 [4100/35182]: Training loss 0.10374171286821365, current lr 9.594444444444445e-05, contrastive loss 3.4733729362487793, task loss 0.0690079852938652
2025-01-09 07:12:29,925 - root - INFO - [Train] Epoch 8 [4200/35182]: Training loss 0.10805832594633102, current lr 9.59430232227623e-05, contrastive loss 3.671159029006958, task loss 0.07134673744440079
2025-01-09 07:14:10,334 - root - INFO - [Train] Epoch 8 [4300/35182]: Training loss 0.08494450151920319, current lr 9.594160200108014e-05, contrastive loss 3.477571964263916, task loss 0.05016878619790077
2025-01-09 07:15:50,998 - root - INFO - [Train] Epoch 8 [4400/35182]: Training loss 0.08900566399097443, current lr 9.594018077939797e-05, contrastive loss 3.4822568893432617, task loss 0.05418309569358826
2025-01-09 07:17:31,391 - root - INFO - [Train] Epoch 8 [4500/35182]: Training loss 0.11136951297521591, current lr 9.593875955771581e-05, contrastive loss 3.523959159851074, task loss 0.07612992078065872
2025-01-09 07:19:11,975 - root - INFO - [Train] Epoch 8 [4600/35182]: Training loss 0.13812372088432312, current lr 9.593733833603365e-05, contrastive loss 3.614879608154297, task loss 0.10197491943836212
2025-01-09 07:20:52,366 - root - INFO - [Train] Epoch 8 [4700/35182]: Training loss 0.09271150827407837, current lr 9.59359171143515e-05, contrastive loss 3.5990750789642334, task loss 0.05672076344490051
2025-01-09 07:22:33,050 - root - INFO - [Train] Epoch 8 [4800/35182]: Training loss 0.09821364283561707, current lr 9.593449589266934e-05, contrastive loss 3.4857120513916016, task loss 0.06335652619600296
2025-01-09 07:24:13,444 - root - INFO - [Train] Epoch 8 [4900/35182]: Training loss 0.0756930410861969, current lr 9.593307467098719e-05, contrastive loss 3.6203269958496094, task loss 0.03948977589607239
2025-01-09 07:25:54,019 - root - INFO - [Train] Epoch 8 [5000/35182]: Training loss 0.12650473415851593, current lr 9.593165344930503e-05, contrastive loss 3.632955551147461, task loss 0.09017517417669296
2025-01-09 07:27:34,424 - root - INFO - [Train] Epoch 8 [5100/35182]: Training loss 0.10741153359413147, current lr 9.593023222762287e-05, contrastive loss 3.543886423110962, task loss 0.07197266817092896
2025-01-09 07:29:15,107 - root - INFO - [Train] Epoch 8 [5200/35182]: Training loss 0.08143724501132965, current lr 9.592881100594072e-05, contrastive loss 3.5341134071350098, task loss 0.04609611630439758
2025-01-09 07:30:55,496 - root - INFO - [Train] Epoch 8 [5300/35182]: Training loss 0.1298934668302536, current lr 9.592738978425855e-05, contrastive loss 3.4956374168395996, task loss 0.09493709355592728
2025-01-09 07:32:36,180 - root - INFO - [Train] Epoch 8 [5400/35182]: Training loss 0.0825987309217453, current lr 9.59259685625764e-05, contrastive loss 3.6056902408599854, task loss 0.04654183238744736
2025-01-09 07:34:16,868 - root - INFO - [Train] Epoch 8 [5500/35182]: Training loss 0.1265418529510498, current lr 9.592454734089423e-05, contrastive loss 3.6306211948394775, task loss 0.0902356430888176
2025-01-09 07:35:57,283 - root - INFO - [Train] Epoch 8 [5600/35182]: Training loss 0.12175105512142181, current lr 9.592312611921208e-05, contrastive loss 3.5479393005371094, task loss 0.08627165853977203
2025-01-09 07:37:37,961 - root - INFO - [Train] Epoch 8 [5700/35182]: Training loss 0.1256977766752243, current lr 9.592170489752992e-05, contrastive loss 3.5690343379974365, task loss 0.09000743180513382
2025-01-09 07:39:18,358 - root - INFO - [Train] Epoch 8 [5800/35182]: Training loss 0.16590885818004608, current lr 9.592028367584776e-05, contrastive loss 3.6670048236846924, task loss 0.1292388141155243
2025-01-09 07:40:59,046 - root - INFO - [Train] Epoch 8 [5900/35182]: Training loss 0.11609086394309998, current lr 9.59188624541656e-05, contrastive loss 3.6832027435302734, task loss 0.07925883680582047
2025-01-09 07:42:39,444 - root - INFO - [Train] Epoch 8 [6000/35182]: Training loss 0.16970506310462952, current lr 9.591744123248345e-05, contrastive loss 3.5734424591064453, task loss 0.13397064805030823
2025-01-09 07:44:20,124 - root - INFO - [Train] Epoch 8 [6100/35182]: Training loss 0.12235841155052185, current lr 9.591602001080129e-05, contrastive loss 3.4685118198394775, task loss 0.08767329901456833
2025-01-09 07:46:00,793 - root - INFO - [Train] Epoch 8 [6200/35182]: Training loss 0.11377350986003876, current lr 9.591459878911914e-05, contrastive loss 3.6178483963012695, task loss 0.07759502530097961
2025-01-09 07:47:41,193 - root - INFO - [Train] Epoch 8 [6300/35182]: Training loss 0.18715134263038635, current lr 9.591317756743698e-05, contrastive loss 3.569023609161377, task loss 0.15146110951900482
2025-01-09 07:49:21,592 - root - INFO - [Train] Epoch 8 [6400/35182]: Training loss 0.16609349846839905, current lr 9.591175634575481e-05, contrastive loss 3.516571521759033, task loss 0.13092778623104095
2025-01-09 07:51:02,181 - root - INFO - [Train] Epoch 8 [6500/35182]: Training loss 0.09914848953485489, current lr 9.591033512407265e-05, contrastive loss 3.6275835037231445, task loss 0.06287265568971634
2025-01-09 07:52:42,869 - root - INFO - [Train] Epoch 8 [6600/35182]: Training loss 0.10443069040775299, current lr 9.59089139023905e-05, contrastive loss 3.470902442932129, task loss 0.06972166150808334
2025-01-09 07:54:23,278 - root - INFO - [Train] Epoch 8 [6700/35182]: Training loss 0.07517537474632263, current lr 9.590749268070834e-05, contrastive loss 3.6322999000549316, task loss 0.03885237127542496
2025-01-09 07:56:03,952 - root - INFO - [Train] Epoch 8 [6800/35182]: Training loss 0.11195789277553558, current lr 9.590607145902618e-05, contrastive loss 3.430269241333008, task loss 0.07765520364046097
2025-01-09 07:57:44,368 - root - INFO - [Train] Epoch 8 [6900/35182]: Training loss 0.11479484289884567, current lr 9.590465023734403e-05, contrastive loss 3.6772730350494385, task loss 0.07802211493253708
2025-01-09 07:59:25,016 - root - INFO - [Train] Epoch 8 [7000/35182]: Training loss 0.12619149684906006, current lr 9.590322901566187e-05, contrastive loss 3.438044309616089, task loss 0.09181105345487595
2025-01-09 08:01:05,428 - root - INFO - [Train] Epoch 8 [7100/35182]: Training loss 0.10065630078315735, current lr 9.590180779397971e-05, contrastive loss 3.4694459438323975, task loss 0.06596184521913528
2025-01-09 08:02:46,102 - root - INFO - [Train] Epoch 8 [7200/35182]: Training loss 0.1546175479888916, current lr 9.590038657229756e-05, contrastive loss 3.56447696685791, task loss 0.1189727708697319
2025-01-09 08:04:26,491 - root - INFO - [Train] Epoch 8 [7300/35182]: Training loss 0.11677888035774231, current lr 9.589896535061539e-05, contrastive loss 3.3634696006774902, task loss 0.0831441879272461
2025-01-09 08:06:07,078 - root - INFO - [Train] Epoch 8 [7400/35182]: Training loss 0.12374089658260345, current lr 9.589754412893324e-05, contrastive loss 3.742751121520996, task loss 0.0863133892416954
2025-01-09 08:07:47,492 - root - INFO - [Train] Epoch 8 [7500/35182]: Training loss 0.11305459588766098, current lr 9.589612290725107e-05, contrastive loss 3.6482625007629395, task loss 0.07657197117805481
2025-01-09 08:09:28,163 - root - INFO - [Train] Epoch 8 [7600/35182]: Training loss 0.09141165763139725, current lr 9.589470168556892e-05, contrastive loss 3.446727752685547, task loss 0.056944381445646286
2025-01-09 08:11:08,846 - root - INFO - [Train] Epoch 8 [7700/35182]: Training loss 0.08781874179840088, current lr 9.589328046388676e-05, contrastive loss 3.603367805480957, task loss 0.05178506672382355
2025-01-09 08:12:49,252 - root - INFO - [Train] Epoch 8 [7800/35182]: Training loss 0.12052740156650543, current lr 9.58918592422046e-05, contrastive loss 3.436532974243164, task loss 0.08616206794977188
2025-01-09 08:14:29,936 - root - INFO - [Train] Epoch 8 [7900/35182]: Training loss 0.12609101831912994, current lr 9.589043802052245e-05, contrastive loss 3.5152926445007324, task loss 0.09093809127807617
2025-01-09 08:16:10,340 - root - INFO - [Train] Epoch 8 [8000/35182]: Training loss 0.1110796183347702, current lr 9.588901679884029e-05, contrastive loss 3.6342594623565674, task loss 0.07473702728748322
2025-01-09 08:17:51,005 - root - INFO - [Train] Epoch 8 [8100/35182]: Training loss 0.1284438818693161, current lr 9.588759557715813e-05, contrastive loss 3.5713229179382324, task loss 0.09273064881563187
2025-01-09 08:19:31,427 - root - INFO - [Train] Epoch 8 [8200/35182]: Training loss 0.08988375961780548, current lr 9.588617435547598e-05, contrastive loss 3.778209924697876, task loss 0.052101656794548035
2025-01-09 08:21:12,093 - root - INFO - [Train] Epoch 8 [8300/35182]: Training loss 0.1521206498146057, current lr 9.588475313379382e-05, contrastive loss 3.764164686203003, task loss 0.11447900533676147
2025-01-09 08:22:52,500 - root - INFO - [Train] Epoch 8 [8400/35182]: Training loss 0.09210829436779022, current lr 9.588333191211165e-05, contrastive loss 3.656444549560547, task loss 0.05554385110735893
2025-01-09 08:24:33,175 - root - INFO - [Train] Epoch 8 [8500/35182]: Training loss 0.10726407170295715, current lr 9.588191069042949e-05, contrastive loss 3.4469943046569824, task loss 0.0727941244840622
2025-01-09 08:26:13,859 - root - INFO - [Train] Epoch 8 [8600/35182]: Training loss 0.09333859384059906, current lr 9.588048946874734e-05, contrastive loss 3.425468921661377, task loss 0.059083908796310425
2025-01-09 08:27:54,258 - root - INFO - [Train] Epoch 8 [8700/35182]: Training loss 0.12470841407775879, current lr 9.587906824706518e-05, contrastive loss 3.4481959342956543, task loss 0.09022645652294159
2025-01-09 08:29:34,952 - root - INFO - [Train] Epoch 8 [8800/35182]: Training loss 0.08279305696487427, current lr 9.587764702538302e-05, contrastive loss 3.6528067588806152, task loss 0.04626499488949776
2025-01-09 08:31:15,349 - root - INFO - [Train] Epoch 8 [8900/35182]: Training loss 0.09024456888437271, current lr 9.587622580370087e-05, contrastive loss 3.5116748809814453, task loss 0.0551278218626976
2025-01-09 08:32:56,033 - root - INFO - [Train] Epoch 8 [9000/35182]: Training loss 0.10275910794734955, current lr 9.587480458201871e-05, contrastive loss 3.6489195823669434, task loss 0.06626991182565689
2025-01-09 08:34:36,433 - root - INFO - [Train] Epoch 8 [9100/35182]: Training loss 0.08934231102466583, current lr 9.587338336033655e-05, contrastive loss 3.4882702827453613, task loss 0.05445960909128189
2025-01-09 08:36:17,107 - root - INFO - [Train] Epoch 8 [9200/35182]: Training loss 0.13571296632289886, current lr 9.58719621386544e-05, contrastive loss 3.663241386413574, task loss 0.09908055514097214
2025-01-09 08:37:57,795 - root - INFO - [Train] Epoch 8 [9300/35182]: Training loss 0.13151544332504272, current lr 9.587054091697223e-05, contrastive loss 3.533318042755127, task loss 0.09618225693702698
2025-01-09 08:39:38,210 - root - INFO - [Train] Epoch 8 [9400/35182]: Training loss 0.10614806413650513, current lr 9.586911969529008e-05, contrastive loss 3.6531600952148438, task loss 0.06961646676063538
2025-01-09 08:41:18,879 - root - INFO - [Train] Epoch 8 [9500/35182]: Training loss 0.11104614287614822, current lr 9.586769847360791e-05, contrastive loss 3.69486141204834, task loss 0.07409752905368805
2025-01-09 08:42:59,271 - root - INFO - [Train] Epoch 8 [9600/35182]: Training loss 0.0943450927734375, current lr 9.586627725192576e-05, contrastive loss 3.426588535308838, task loss 0.06007920578122139
2025-01-09 08:44:39,846 - root - INFO - [Train] Epoch 8 [9700/35182]: Training loss 0.10761038959026337, current lr 9.58648560302436e-05, contrastive loss 3.567462205886841, task loss 0.07193577289581299
2025-01-09 08:46:20,240 - root - INFO - [Train] Epoch 8 [9800/35182]: Training loss 0.0803181603550911, current lr 9.586343480856144e-05, contrastive loss 3.3468451499938965, task loss 0.04684970900416374
2025-01-09 08:48:00,931 - root - INFO - [Train] Epoch 8 [9900/35182]: Training loss 0.1621425598859787, current lr 9.586201358687929e-05, contrastive loss 3.594521999359131, task loss 0.12619733810424805
2025-01-09 08:49:41,322 - root - INFO - [Train] Epoch 8 [10000/35182]: Training loss 0.09505845606327057, current lr 9.586059236519713e-05, contrastive loss 3.5714168548583984, task loss 0.05934429168701172
2025-01-09 08:51:22,009 - root - INFO - [Train] Epoch 8 [10100/35182]: Training loss 0.12142550200223923, current lr 9.585917114351497e-05, contrastive loss 3.709202289581299, task loss 0.08433347940444946
2025-01-09 08:53:02,426 - root - INFO - [Train] Epoch 8 [10200/35182]: Training loss 0.0994611382484436, current lr 9.58577499218328e-05, contrastive loss 3.5983951091766357, task loss 0.06347718834877014
2025-01-09 08:54:43,089 - root - INFO - [Train] Epoch 8 [10300/35182]: Training loss 0.07015886902809143, current lr 9.585632870015066e-05, contrastive loss 3.4098684787750244, task loss 0.036060184240341187
2025-01-09 08:56:23,494 - root - INFO - [Train] Epoch 8 [10400/35182]: Training loss 0.17658469080924988, current lr 9.585490747846849e-05, contrastive loss 3.707716941833496, task loss 0.13950751721858978
2025-01-09 08:58:04,191 - root - INFO - [Train] Epoch 8 [10500/35182]: Training loss 0.09742085635662079, current lr 9.585348625678633e-05, contrastive loss 3.716165065765381, task loss 0.06025920435786247
2025-01-09 08:59:44,863 - root - INFO - [Train] Epoch 8 [10600/35182]: Training loss 0.09654426574707031, current lr 9.585206503510418e-05, contrastive loss 3.4752683639526367, task loss 0.06179158389568329
2025-01-09 09:01:25,270 - root - INFO - [Train] Epoch 8 [10700/35182]: Training loss 0.1437515914440155, current lr 9.585064381342202e-05, contrastive loss 3.4944849014282227, task loss 0.10880674421787262
2025-01-09 09:03:05,945 - root - INFO - [Train] Epoch 8 [10800/35182]: Training loss 0.07206341624259949, current lr 9.584922259173986e-05, contrastive loss 3.429591178894043, task loss 0.037767499685287476
2025-01-09 09:04:46,340 - root - INFO - [Train] Epoch 8 [10900/35182]: Training loss 0.1480301469564438, current lr 9.584780137005771e-05, contrastive loss 3.5381386280059814, task loss 0.11264876276254654
2025-01-09 09:06:26,924 - root - INFO - [Train] Epoch 8 [11000/35182]: Training loss 0.08499135822057724, current lr 9.584638014837555e-05, contrastive loss 3.540158748626709, task loss 0.049589771777391434
2025-01-09 09:08:07,339 - root - INFO - [Train] Epoch 8 [11100/35182]: Training loss 0.12724566459655762, current lr 9.58449589266934e-05, contrastive loss 3.7062530517578125, task loss 0.09018313139677048
2025-01-09 09:09:47,998 - root - INFO - [Train] Epoch 8 [11200/35182]: Training loss 0.098467618227005, current lr 9.584353770501124e-05, contrastive loss 3.604316234588623, task loss 0.06242445483803749
2025-01-09 09:11:28,413 - root - INFO - [Train] Epoch 8 [11300/35182]: Training loss 0.14481812715530396, current lr 9.584211648332907e-05, contrastive loss 3.650442123413086, task loss 0.10831370204687119
2025-01-09 09:13:09,093 - root - INFO - [Train] Epoch 8 [11400/35182]: Training loss 0.10462749004364014, current lr 9.584069526164692e-05, contrastive loss 3.5161142349243164, task loss 0.06946634501218796
2025-01-09 09:14:49,508 - root - INFO - [Train] Epoch 8 [11500/35182]: Training loss 0.16917133331298828, current lr 9.583927403996475e-05, contrastive loss 3.7289581298828125, task loss 0.13188175857067108
2025-01-09 09:16:30,172 - root - INFO - [Train] Epoch 8 [11600/35182]: Training loss 0.10397037863731384, current lr 9.58378528182826e-05, contrastive loss 3.5243186950683594, task loss 0.06872719526290894
2025-01-09 09:18:10,848 - root - INFO - [Train] Epoch 8 [11700/35182]: Training loss 0.13602924346923828, current lr 9.583643159660044e-05, contrastive loss 3.4576292037963867, task loss 0.10145295411348343
2025-01-09 09:19:51,250 - root - INFO - [Train] Epoch 8 [11800/35182]: Training loss 0.0920923501253128, current lr 9.583501037491828e-05, contrastive loss 3.609487771987915, task loss 0.05599747225642204
2025-01-09 09:21:31,942 - root - INFO - [Train] Epoch 8 [11900/35182]: Training loss 0.14759725332260132, current lr 9.583358915323613e-05, contrastive loss 3.782090902328491, task loss 0.10977634787559509
2025-01-09 09:23:12,349 - root - INFO - [Train] Epoch 8 [12000/35182]: Training loss 0.12814749777317047, current lr 9.583216793155397e-05, contrastive loss 3.66652250289917, task loss 0.09148227423429489
2025-01-09 09:24:53,020 - root - INFO - [Train] Epoch 8 [12100/35182]: Training loss 0.11851426213979721, current lr 9.583074670987181e-05, contrastive loss 3.6519832611083984, task loss 0.08199442923069
2025-01-09 09:26:33,436 - root - INFO - [Train] Epoch 8 [12200/35182]: Training loss 0.07226572185754776, current lr 9.582932548818964e-05, contrastive loss 3.513838768005371, task loss 0.037127334624528885
2025-01-09 09:28:14,104 - root - INFO - [Train] Epoch 8 [12300/35182]: Training loss 0.16813810169696808, current lr 9.58279042665075e-05, contrastive loss 3.622666120529175, task loss 0.13191144168376923
2025-01-09 09:29:54,509 - root - INFO - [Train] Epoch 8 [12400/35182]: Training loss 0.11838562786579132, current lr 9.582648304482533e-05, contrastive loss 3.7519378662109375, task loss 0.0808662474155426
2025-01-09 09:31:35,193 - root - INFO - [Train] Epoch 8 [12500/35182]: Training loss 0.16289682686328888, current lr 9.582506182314317e-05, contrastive loss 3.347107410430908, task loss 0.1294257491827011
2025-01-09 09:33:15,867 - root - INFO - [Train] Epoch 8 [12600/35182]: Training loss 0.12733544409275055, current lr 9.582364060146102e-05, contrastive loss 3.4589428901672363, task loss 0.09274601936340332
2025-01-09 09:34:56,272 - root - INFO - [Train] Epoch 8 [12700/35182]: Training loss 0.10268114507198334, current lr 9.582221937977886e-05, contrastive loss 3.3918890953063965, task loss 0.06876225769519806
2025-01-09 09:36:36,955 - root - INFO - [Train] Epoch 8 [12800/35182]: Training loss 0.12851472198963165, current lr 9.58207981580967e-05, contrastive loss 3.635647773742676, task loss 0.0921582505106926
2025-01-09 09:38:17,361 - root - INFO - [Train] Epoch 8 [12900/35182]: Training loss 0.10308055579662323, current lr 9.581937693641455e-05, contrastive loss 3.4337539672851562, task loss 0.0687430202960968
2025-01-09 09:39:58,034 - root - INFO - [Train] Epoch 8 [13000/35182]: Training loss 0.15367987751960754, current lr 9.581795571473239e-05, contrastive loss 3.5523109436035156, task loss 0.11815676093101501
2025-01-09 09:41:38,452 - root - INFO - [Train] Epoch 8 [13100/35182]: Training loss 0.08135523647069931, current lr 9.581653449305022e-05, contrastive loss 3.679133415222168, task loss 0.0445639044046402
2025-01-09 09:43:19,125 - root - INFO - [Train] Epoch 8 [13200/35182]: Training loss 0.18109257519245148, current lr 9.581511327136808e-05, contrastive loss 3.554976463317871, task loss 0.14554281532764435
2025-01-09 09:44:59,807 - root - INFO - [Train] Epoch 8 [13300/35182]: Training loss 0.1364118456840515, current lr 9.581369204968591e-05, contrastive loss 3.694347381591797, task loss 0.09946836531162262
2025-01-09 09:46:40,201 - root - INFO - [Train] Epoch 8 [13400/35182]: Training loss 0.09779930859804153, current lr 9.581227082800376e-05, contrastive loss 3.545985221862793, task loss 0.062339458614587784
2025-01-09 09:48:20,877 - root - INFO - [Train] Epoch 8 [13500/35182]: Training loss 0.14650236070156097, current lr 9.58108496063216e-05, contrastive loss 3.4505410194396973, task loss 0.11199695616960526
2025-01-09 09:50:01,278 - root - INFO - [Train] Epoch 8 [13600/35182]: Training loss 0.1366790533065796, current lr 9.580942838463944e-05, contrastive loss 3.7564620971679688, task loss 0.09911442548036575
2025-01-09 09:51:41,857 - root - INFO - [Train] Epoch 8 [13700/35182]: Training loss 0.08906958997249603, current lr 9.580800716295728e-05, contrastive loss 3.6395320892333984, task loss 0.052674271166324615
2025-01-09 09:53:22,266 - root - INFO - [Train] Epoch 8 [13800/35182]: Training loss 0.1497717648744583, current lr 9.580658594127512e-05, contrastive loss 3.495680093765259, task loss 0.11481496691703796
2025-01-09 09:55:02,940 - root - INFO - [Train] Epoch 8 [13900/35182]: Training loss 0.15607915818691254, current lr 9.580516471959297e-05, contrastive loss 3.4537553787231445, task loss 0.12154160439968109
2025-01-09 09:56:43,351 - root - INFO - [Train] Epoch 8 [14000/35182]: Training loss 0.12282881885766983, current lr 9.580374349791081e-05, contrastive loss 3.541316032409668, task loss 0.0874156579375267
2025-01-09 09:58:24,016 - root - INFO - [Train] Epoch 8 [14100/35182]: Training loss 0.13890424370765686, current lr 9.580232227622865e-05, contrastive loss 3.540966749191284, task loss 0.10349458456039429
2025-01-09 10:00:04,422 - root - INFO - [Train] Epoch 8 [14200/35182]: Training loss 0.11028159409761429, current lr 9.580090105454648e-05, contrastive loss 3.4405858516693115, task loss 0.07587573677301407
2025-01-09 10:01:45,107 - root - INFO - [Train] Epoch 8 [14300/35182]: Training loss 0.11705061793327332, current lr 9.579947983286434e-05, contrastive loss 3.568455696105957, task loss 0.08136606216430664
2025-01-09 10:03:25,516 - root - INFO - [Train] Epoch 8 [14400/35182]: Training loss 0.14443014562129974, current lr 9.579805861118217e-05, contrastive loss 3.522414207458496, task loss 0.10920600593090057
2025-01-09 10:05:06,196 - root - INFO - [Train] Epoch 8 [14500/35182]: Training loss 0.12862378358840942, current lr 9.579663738950001e-05, contrastive loss 3.677379608154297, task loss 0.09184999763965607
2025-01-09 10:06:46,879 - root - INFO - [Train] Epoch 8 [14600/35182]: Training loss 0.12753233313560486, current lr 9.579521616781786e-05, contrastive loss 3.592207908630371, task loss 0.09161025285720825
2025-01-09 10:08:27,282 - root - INFO - [Train] Epoch 8 [14700/35182]: Training loss 0.09992808103561401, current lr 9.57937949461357e-05, contrastive loss 3.474940776824951, task loss 0.06517867743968964
2025-01-09 10:10:07,964 - root - INFO - [Train] Epoch 8 [14800/35182]: Training loss 0.08997586369514465, current lr 9.579237372445355e-05, contrastive loss 3.5235772132873535, task loss 0.054740093648433685
2025-01-09 10:11:48,373 - root - INFO - [Train] Epoch 8 [14900/35182]: Training loss 0.08089552074670792, current lr 9.579095250277139e-05, contrastive loss 3.67887544631958, task loss 0.04410676658153534
2025-01-09 10:13:29,044 - root - INFO - [Train] Epoch 8 [15000/35182]: Training loss 0.07198043167591095, current lr 9.578953128108923e-05, contrastive loss 3.5869107246398926, task loss 0.03611132502555847
2025-01-09 10:15:09,448 - root - INFO - [Train] Epoch 8 [15100/35182]: Training loss 0.13711367547512054, current lr 9.578811005940706e-05, contrastive loss 3.5764565467834473, task loss 0.1013491079211235
2025-01-09 10:16:50,129 - root - INFO - [Train] Epoch 8 [15200/35182]: Training loss 0.09065144509077072, current lr 9.578668883772492e-05, contrastive loss 3.496216297149658, task loss 0.05568928271532059
2025-01-09 10:18:30,810 - root - INFO - [Train] Epoch 8 [15300/35182]: Training loss 0.14715881645679474, current lr 9.578526761604275e-05, contrastive loss 3.5416741371154785, task loss 0.11174207925796509
2025-01-09 10:20:11,210 - root - INFO - [Train] Epoch 8 [15400/35182]: Training loss 0.11702606081962585, current lr 9.57838463943606e-05, contrastive loss 3.572547435760498, task loss 0.08130058646202087
2025-01-09 10:21:51,889 - root - INFO - [Train] Epoch 8 [15500/35182]: Training loss 0.10605669021606445, current lr 9.578242517267844e-05, contrastive loss 3.6388468742370605, task loss 0.06966821849346161
2025-01-09 10:23:32,305 - root - INFO - [Train] Epoch 8 [15600/35182]: Training loss 0.09990476071834564, current lr 9.578100395099628e-05, contrastive loss 3.5506393909454346, task loss 0.06439836323261261
2025-01-09 10:25:12,975 - root - INFO - [Train] Epoch 8 [15700/35182]: Training loss 0.12227852642536163, current lr 9.577958272931412e-05, contrastive loss 3.5271925926208496, task loss 0.08700660616159439
2025-01-09 10:26:53,372 - root - INFO - [Train] Epoch 8 [15800/35182]: Training loss 0.12173691391944885, current lr 9.577816150763197e-05, contrastive loss 3.4831161499023438, task loss 0.08690574765205383
2025-01-09 10:28:34,088 - root - INFO - [Train] Epoch 8 [15900/35182]: Training loss 0.1339086890220642, current lr 9.577674028594981e-05, contrastive loss 3.702632427215576, task loss 0.09688237309455872
2025-01-09 10:30:14,502 - root - INFO - [Train] Epoch 8 [16000/35182]: Training loss 0.09377461671829224, current lr 9.577531906426765e-05, contrastive loss 3.8049964904785156, task loss 0.05572465807199478
2025-01-09 10:31:55,150 - root - INFO - [Train] Epoch 8 [16100/35182]: Training loss 0.15168355405330658, current lr 9.57738978425855e-05, contrastive loss 3.5442252159118652, task loss 0.11624130606651306
2025-01-09 10:33:35,829 - root - INFO - [Train] Epoch 8 [16200/35182]: Training loss 0.0828351303935051, current lr 9.577247662090333e-05, contrastive loss 3.7535810470581055, task loss 0.04529932141304016
2025-01-09 10:35:16,228 - root - INFO - [Train] Epoch 8 [16300/35182]: Training loss 0.11333238333463669, current lr 9.577105539922118e-05, contrastive loss 3.6144533157348633, task loss 0.07718785107135773
2025-01-09 10:36:56,907 - root - INFO - [Train] Epoch 8 [16400/35182]: Training loss 0.08852209150791168, current lr 9.576963417753901e-05, contrastive loss 3.673387050628662, task loss 0.05178822576999664
2025-01-09 10:38:37,314 - root - INFO - [Train] Epoch 8 [16500/35182]: Training loss 0.08783015608787537, current lr 9.576821295585686e-05, contrastive loss 3.633687973022461, task loss 0.05149327591061592
2025-01-09 10:40:18,004 - root - INFO - [Train] Epoch 8 [16600/35182]: Training loss 0.0949663519859314, current lr 9.57667917341747e-05, contrastive loss 3.46983003616333, task loss 0.06026804819703102
2025-01-09 10:41:58,402 - root - INFO - [Train] Epoch 8 [16700/35182]: Training loss 0.119004987180233, current lr 9.576537051249254e-05, contrastive loss 3.4463019371032715, task loss 0.08454196900129318
2025-01-09 10:43:39,082 - root - INFO - [Train] Epoch 8 [16800/35182]: Training loss 0.0974186509847641, current lr 9.576394929081039e-05, contrastive loss 3.4385299682617188, task loss 0.06303335726261139
2025-01-09 10:45:19,483 - root - INFO - [Train] Epoch 8 [16900/35182]: Training loss 0.10508781671524048, current lr 9.576252806912823e-05, contrastive loss 3.341869831085205, task loss 0.07166912406682968
2025-01-09 10:47:00,170 - root - INFO - [Train] Epoch 8 [17000/35182]: Training loss 0.08336351811885834, current lr 9.576110684744607e-05, contrastive loss 3.4323270320892334, task loss 0.04904025048017502
2025-01-09 10:48:40,846 - root - INFO - [Train] Epoch 8 [17100/35182]: Training loss 0.17095869779586792, current lr 9.57596856257639e-05, contrastive loss 3.495645523071289, task loss 0.13600224256515503
2025-01-09 10:50:21,250 - root - INFO - [Train] Epoch 8 [17200/35182]: Training loss 0.08948229253292084, current lr 9.575826440408176e-05, contrastive loss 3.5021986961364746, task loss 0.054460301995277405
2025-01-09 10:52:01,932 - root - INFO - [Train] Epoch 8 [17300/35182]: Training loss 0.14874087274074554, current lr 9.575684318239959e-05, contrastive loss 3.6083908081054688, task loss 0.11265696585178375
2025-01-09 10:53:42,329 - root - INFO - [Train] Epoch 8 [17400/35182]: Training loss 0.0931137204170227, current lr 9.575542196071745e-05, contrastive loss 3.5468759536743164, task loss 0.05764496698975563
2025-01-09 10:55:23,013 - root - INFO - [Train] Epoch 8 [17500/35182]: Training loss 0.10794907808303833, current lr 9.575400073903528e-05, contrastive loss 3.4174418449401855, task loss 0.07377466559410095
2025-01-09 13:40:41,124 - root - INFO - [Valid] Evaluation Results: HR@5:0.8024,NDCG@5:0.7421,HR@10:0.8600,NDCG@10:0.7608,HR@20:0.9141,NDCG@20:0.7745
2025-01-09 13:40:51,170 - root - INFO - [Train] Epoch 8 [17600/35182]: Training loss 0.10628491640090942, current lr 9.575257951735312e-05, contrastive loss 3.4004573822021484, task loss 0.07228033989667892
2025-01-09 13:42:31,818 - root - INFO - [Train] Epoch 8 [17700/35182]: Training loss 0.0971439927816391, current lr 9.575115829567096e-05, contrastive loss 3.550609827041626, task loss 0.06163789704442024
2025-01-09 13:44:12,223 - root - INFO - [Train] Epoch 8 [17800/35182]: Training loss 0.11912503838539124, current lr 9.57497370739888e-05, contrastive loss 3.509894371032715, task loss 0.08402609080076218
2025-01-09 13:45:52,896 - root - INFO - [Train] Epoch 8 [17900/35182]: Training loss 0.10878267884254456, current lr 9.574831585230665e-05, contrastive loss 3.5093894004821777, task loss 0.07368878275156021
2025-01-09 13:47:33,324 - root - INFO - [Train] Epoch 8 [18000/35182]: Training loss 0.14747720956802368, current lr 9.574689463062448e-05, contrastive loss 3.561427354812622, task loss 0.11186292767524719
2025-01-09 13:49:14,000 - root - INFO - [Train] Epoch 8 [18100/35182]: Training loss 0.09834720939397812, current lr 9.574547340894234e-05, contrastive loss 3.452157974243164, task loss 0.06382562965154648
2025-01-09 13:50:54,397 - root - INFO - [Train] Epoch 8 [18200/35182]: Training loss 0.13481861352920532, current lr 9.574405218726017e-05, contrastive loss 3.4019718170166016, task loss 0.10079890489578247
2025-01-09 13:52:35,059 - root - INFO - [Train] Epoch 8 [18300/35182]: Training loss 0.15525946021080017, current lr 9.574263096557802e-05, contrastive loss 3.582637071609497, task loss 0.11943309009075165
2025-01-09 13:54:15,466 - root - INFO - [Train] Epoch 8 [18400/35182]: Training loss 0.16061145067214966, current lr 9.574120974389585e-05, contrastive loss 3.5384116172790527, task loss 0.12522733211517334
2025-01-09 13:55:56,144 - root - INFO - [Train] Epoch 8 [18500/35182]: Training loss 0.14044451713562012, current lr 9.57397885222137e-05, contrastive loss 3.5344901084899902, task loss 0.105099618434906
2025-01-09 13:57:36,834 - root - INFO - [Train] Epoch 8 [18600/35182]: Training loss 0.10140818357467651, current lr 9.573836730053154e-05, contrastive loss 3.542820692062378, task loss 0.0659799724817276
2025-01-09 13:59:17,247 - root - INFO - [Train] Epoch 8 [18700/35182]: Training loss 0.13840340077877045, current lr 9.573694607884938e-05, contrastive loss 3.5266940593719482, task loss 0.10313646495342255
2025-01-09 14:00:57,914 - root - INFO - [Train] Epoch 8 [18800/35182]: Training loss 0.1578785479068756, current lr 9.573552485716723e-05, contrastive loss 3.6002583503723145, task loss 0.12187597155570984
2025-01-09 14:02:38,312 - root - INFO - [Train] Epoch 8 [18900/35182]: Training loss 0.1503094732761383, current lr 9.573410363548507e-05, contrastive loss 3.5172972679138184, task loss 0.11513649672269821
2025-01-09 14:04:19,000 - root - INFO - [Train] Epoch 8 [19000/35182]: Training loss 0.09345771372318268, current lr 9.573268241380291e-05, contrastive loss 3.6308951377868652, task loss 0.057148758322000504
2025-01-09 14:05:59,417 - root - INFO - [Train] Epoch 8 [19100/35182]: Training loss 0.11636301875114441, current lr 9.573126119212074e-05, contrastive loss 3.811814785003662, task loss 0.07824487239122391
2025-01-09 14:07:40,097 - root - INFO - [Train] Epoch 8 [19200/35182]: Training loss 0.07993534952402115, current lr 9.57298399704386e-05, contrastive loss 3.63765811920166, task loss 0.043558768928050995
2025-01-09 14:09:20,494 - root - INFO - [Train] Epoch 8 [19300/35182]: Training loss 0.11757740378379822, current lr 9.572841874875643e-05, contrastive loss 3.499788761138916, task loss 0.08257951587438583
2025-01-09 14:11:01,167 - root - INFO - [Train] Epoch 8 [19400/35182]: Training loss 0.10772346705198288, current lr 9.572699752707429e-05, contrastive loss 3.6117353439331055, task loss 0.0716061145067215
2025-01-09 14:12:41,863 - root - INFO - [Train] Epoch 8 [19500/35182]: Training loss 0.08999857306480408, current lr 9.572557630539212e-05, contrastive loss 3.4293839931488037, task loss 0.05570473521947861
2025-01-09 14:14:22,265 - root - INFO - [Train] Epoch 8 [19600/35182]: Training loss 0.07415066659450531, current lr 9.572415508370996e-05, contrastive loss 3.7026731967926025, task loss 0.037123940885066986
2025-01-09 14:16:02,944 - root - INFO - [Train] Epoch 8 [19700/35182]: Training loss 0.1302614063024521, current lr 9.57227338620278e-05, contrastive loss 3.5986480712890625, task loss 0.09427492320537567
2025-01-09 14:17:43,333 - root - INFO - [Train] Epoch 8 [19800/35182]: Training loss 0.11702422797679901, current lr 9.572131264034565e-05, contrastive loss 3.488365650177002, task loss 0.08214057236909866
2025-01-09 14:19:24,012 - root - INFO - [Train] Epoch 8 [19900/35182]: Training loss 0.13219133019447327, current lr 9.571989141866349e-05, contrastive loss 3.5777225494384766, task loss 0.09641409665346146
2025-01-09 14:21:04,406 - root - INFO - [Train] Epoch 8 [20000/35182]: Training loss 0.07137131690979004, current lr 9.571847019698132e-05, contrastive loss 3.419288396835327, task loss 0.037178438156843185
2025-01-09 14:22:44,984 - root - INFO - [Train] Epoch 8 [20100/35182]: Training loss 0.1372491866350174, current lr 9.571704897529918e-05, contrastive loss 3.6109228134155273, task loss 0.10113995522260666
2025-01-09 14:24:25,372 - root - INFO - [Train] Epoch 8 [20200/35182]: Training loss 0.13225217163562775, current lr 9.5715627753617e-05, contrastive loss 3.63784122467041, task loss 0.09587375819683075
2025-01-09 14:26:05,959 - root - INFO - [Train] Epoch 8 [20300/35182]: Training loss 0.07286098599433899, current lr 9.571420653193486e-05, contrastive loss 3.5418896675109863, task loss 0.03744209185242653
2025-01-09 14:27:46,358 - root - INFO - [Train] Epoch 8 [20400/35182]: Training loss 0.12297634780406952, current lr 9.57127853102527e-05, contrastive loss 3.8347315788269043, task loss 0.08462903648614883
2025-01-09 14:29:27,062 - root - INFO - [Train] Epoch 8 [20500/35182]: Training loss 0.08755351603031158, current lr 9.571136408857054e-05, contrastive loss 3.650723457336426, task loss 0.051046282052993774
2025-01-09 14:31:07,459 - root - INFO - [Train] Epoch 8 [20600/35182]: Training loss 0.15602701902389526, current lr 9.570994286688838e-05, contrastive loss 3.658738136291504, task loss 0.11943963915109634
2025-01-09 14:32:48,119 - root - INFO - [Train] Epoch 8 [20700/35182]: Training loss 0.1611529439687729, current lr 9.570852164520622e-05, contrastive loss 3.608503818511963, task loss 0.12506790459156036
2025-01-09 14:34:28,802 - root - INFO - [Train] Epoch 8 [20800/35182]: Training loss 0.14090734720230103, current lr 9.570710042352407e-05, contrastive loss 3.504061698913574, task loss 0.10586673766374588
2025-01-09 14:36:09,210 - root - INFO - [Train] Epoch 8 [20900/35182]: Training loss 0.1200808435678482, current lr 9.57056792018419e-05, contrastive loss 3.5854811668395996, task loss 0.08422603458166122
2025-01-09 14:37:49,892 - root - INFO - [Train] Epoch 8 [21000/35182]: Training loss 0.09206169843673706, current lr 9.570425798015975e-05, contrastive loss 3.54595947265625, task loss 0.056602101773023605
2025-01-09 14:39:30,295 - root - INFO - [Train] Epoch 8 [21100/35182]: Training loss 0.13365238904953003, current lr 9.570283675847758e-05, contrastive loss 3.408308982849121, task loss 0.09956929832696915
2025-01-09 14:41:10,978 - root - INFO - [Train] Epoch 8 [21200/35182]: Training loss 0.1176026314496994, current lr 9.570141553679544e-05, contrastive loss 3.557466983795166, task loss 0.08202795684337616
2025-01-09 14:42:51,371 - root - INFO - [Train] Epoch 8 [21300/35182]: Training loss 0.08348029106855392, current lr 9.569999431511327e-05, contrastive loss 3.5601444244384766, task loss 0.047878846526145935
2025-01-09 14:44:32,053 - root - INFO - [Train] Epoch 8 [21400/35182]: Training loss 0.1215544044971466, current lr 9.569857309343113e-05, contrastive loss 3.6984469890594482, task loss 0.08456993848085403
2025-01-09 14:46:12,449 - root - INFO - [Train] Epoch 8 [21500/35182]: Training loss 0.12367868423461914, current lr 9.569715187174896e-05, contrastive loss 3.534140110015869, task loss 0.08833727985620499
2025-01-09 14:47:53,120 - root - INFO - [Train] Epoch 8 [21600/35182]: Training loss 0.17717602849006653, current lr 9.56957306500668e-05, contrastive loss 3.72021746635437, task loss 0.13997386395931244
2025-01-09 14:49:33,823 - root - INFO - [Train] Epoch 8 [21700/35182]: Training loss 0.12259230017662048, current lr 9.569430942838464e-05, contrastive loss 3.430324077606201, task loss 0.0882890596985817
2025-01-09 14:51:14,232 - root - INFO - [Train] Epoch 8 [21800/35182]: Training loss 0.08222920447587967, current lr 9.569288820670249e-05, contrastive loss 3.3658647537231445, task loss 0.04857055842876434
2025-01-09 14:52:54,912 - root - INFO - [Train] Epoch 8 [21900/35182]: Training loss 0.12312363088130951, current lr 9.569146698502033e-05, contrastive loss 3.4485812187194824, task loss 0.0886378213763237
2025-01-09 14:54:35,305 - root - INFO - [Train] Epoch 8 [22000/35182]: Training loss 0.1203463077545166, current lr 9.569004576333816e-05, contrastive loss 3.5061676502227783, task loss 0.08528463542461395
2025-01-09 14:56:15,987 - root - INFO - [Train] Epoch 8 [22100/35182]: Training loss 0.07076869904994965, current lr 9.568862454165602e-05, contrastive loss 3.501898765563965, task loss 0.035749711096286774
2025-01-09 14:57:56,389 - root - INFO - [Train] Epoch 8 [22200/35182]: Training loss 0.1093161553144455, current lr 9.568720331997385e-05, contrastive loss 3.70988392829895, task loss 0.07221731543540955
2025-01-09 14:59:37,092 - root - INFO - [Train] Epoch 8 [22300/35182]: Training loss 0.10505326092243195, current lr 9.56857820982917e-05, contrastive loss 3.5999460220336914, task loss 0.06905380636453629
2025-01-09 15:01:17,488 - root - INFO - [Train] Epoch 8 [22400/35182]: Training loss 0.12903504073619843, current lr 9.568436087660953e-05, contrastive loss 3.620373249053955, task loss 0.09283130615949631
2025-01-09 15:02:58,176 - root - INFO - [Train] Epoch 8 [22500/35182]: Training loss 0.1268080770969391, current lr 9.568293965492738e-05, contrastive loss 3.571427822113037, task loss 0.09109380096197128
2025-01-09 15:04:38,844 - root - INFO - [Train] Epoch 8 [22600/35182]: Training loss 0.10777750611305237, current lr 9.568151843324522e-05, contrastive loss 3.590559482574463, task loss 0.07187190651893616
2025-01-09 15:06:19,266 - root - INFO - [Train] Epoch 8 [22700/35182]: Training loss 0.12212039530277252, current lr 9.568009721156306e-05, contrastive loss 3.659334897994995, task loss 0.08552704751491547
2025-01-09 15:07:59,929 - root - INFO - [Train] Epoch 8 [22800/35182]: Training loss 0.09434425830841064, current lr 9.567867598988091e-05, contrastive loss 3.466219425201416, task loss 0.05968206375837326
2025-01-09 15:09:40,353 - root - INFO - [Train] Epoch 8 [22900/35182]: Training loss 0.12194076180458069, current lr 9.567725476819874e-05, contrastive loss 3.49123477935791, task loss 0.08702841401100159
2025-01-09 15:11:21,021 - root - INFO - [Train] Epoch 8 [23000/35182]: Training loss 0.15686510503292084, current lr 9.56758335465166e-05, contrastive loss 3.604279041290283, task loss 0.12082231789827347
2025-01-09 15:13:01,431 - root - INFO - [Train] Epoch 8 [23100/35182]: Training loss 0.07526372373104095, current lr 9.567441232483442e-05, contrastive loss 3.5016140937805176, task loss 0.04024757817387581
2025-01-09 15:14:42,099 - root - INFO - [Train] Epoch 8 [23200/35182]: Training loss 0.13471992313861847, current lr 9.567299110315228e-05, contrastive loss 3.557426929473877, task loss 0.09914565831422806
2025-01-09 15:16:22,495 - root - INFO - [Train] Epoch 8 [23300/35182]: Training loss 0.11266861855983734, current lr 9.567156988147011e-05, contrastive loss 3.4967923164367676, task loss 0.07770069688558578
2025-01-09 15:18:03,173 - root - INFO - [Train] Epoch 8 [23400/35182]: Training loss 0.13267524540424347, current lr 9.567014865978797e-05, contrastive loss 3.513213634490967, task loss 0.09754310548305511
2025-01-09 15:19:43,863 - root - INFO - [Train] Epoch 8 [23500/35182]: Training loss 0.11289109289646149, current lr 9.56687274381058e-05, contrastive loss 3.4385929107666016, task loss 0.07850516587495804
2025-01-09 15:21:24,279 - root - INFO - [Train] Epoch 8 [23600/35182]: Training loss 0.10822921991348267, current lr 9.566730621642364e-05, contrastive loss 3.409412384033203, task loss 0.07413509488105774
2025-01-09 15:23:04,955 - root - INFO - [Train] Epoch 8 [23700/35182]: Training loss 0.12828969955444336, current lr 9.566588499474148e-05, contrastive loss 3.5476622581481934, task loss 0.09281307458877563
2025-01-09 15:24:45,348 - root - INFO - [Train] Epoch 8 [23800/35182]: Training loss 0.10814660787582397, current lr 9.566446377305933e-05, contrastive loss 3.50429105758667, task loss 0.07310369610786438
2025-01-09 15:26:26,034 - root - INFO - [Train] Epoch 8 [23900/35182]: Training loss 0.1143379956483841, current lr 9.566304255137717e-05, contrastive loss 3.6693780422210693, task loss 0.0776442214846611
2025-01-09 15:28:06,429 - root - INFO - [Train] Epoch 8 [24000/35182]: Training loss 0.08358290791511536, current lr 9.5661621329695e-05, contrastive loss 3.4820990562438965, task loss 0.048761919140815735
2025-01-09 15:29:47,116 - root - INFO - [Train] Epoch 8 [24100/35182]: Training loss 0.09198329597711563, current lr 9.566020010801286e-05, contrastive loss 3.5660786628723145, task loss 0.05632251128554344
2025-01-09 15:31:27,798 - root - INFO - [Train] Epoch 8 [24200/35182]: Training loss 0.09032014012336731, current lr 9.565877888633069e-05, contrastive loss 3.479982614517212, task loss 0.05552031844854355
2025-01-09 15:33:08,198 - root - INFO - [Train] Epoch 8 [24300/35182]: Training loss 0.12671828269958496, current lr 9.565735766464854e-05, contrastive loss 3.5108213424682617, task loss 0.09161006659269333
2025-01-09 15:34:48,889 - root - INFO - [Train] Epoch 8 [24400/35182]: Training loss 0.1349894106388092, current lr 9.565593644296637e-05, contrastive loss 3.8826162815093994, task loss 0.09616325050592422
2025-01-09 15:36:29,284 - root - INFO - [Train] Epoch 8 [24500/35182]: Training loss 0.10049444437026978, current lr 9.565451522128422e-05, contrastive loss 3.636570930480957, task loss 0.06412873417139053
2025-01-09 15:38:09,971 - root - INFO - [Train] Epoch 8 [24600/35182]: Training loss 0.08315823972225189, current lr 9.565309399960206e-05, contrastive loss 3.4734809398651123, task loss 0.04842343181371689
2025-01-09 15:39:50,363 - root - INFO - [Train] Epoch 8 [24700/35182]: Training loss 0.1931503415107727, current lr 9.56516727779199e-05, contrastive loss 3.6252760887145996, task loss 0.15689758956432343
2025-01-09 15:41:31,043 - root - INFO - [Train] Epoch 8 [24800/35182]: Training loss 0.12413215637207031, current lr 9.565025155623775e-05, contrastive loss 3.4906764030456543, task loss 0.0892253965139389
2025-01-09 15:43:11,434 - root - INFO - [Train] Epoch 8 [24900/35182]: Training loss 0.17398110032081604, current lr 9.564883033455558e-05, contrastive loss 3.5309040546417236, task loss 0.13867206871509552
2025-01-09 15:44:52,133 - root - INFO - [Train] Epoch 8 [25000/35182]: Training loss 0.15245133638381958, current lr 9.564740911287344e-05, contrastive loss 3.504793167114258, task loss 0.11740340292453766
2025-01-09 15:46:32,821 - root - INFO - [Train] Epoch 8 [25100/35182]: Training loss 0.15741680562496185, current lr 9.564598789119126e-05, contrastive loss 3.4413390159606934, task loss 0.12300341576337814
2025-01-09 15:48:13,232 - root - INFO - [Train] Epoch 8 [25200/35182]: Training loss 0.12346862256526947, current lr 9.564456666950912e-05, contrastive loss 3.49403977394104, task loss 0.08852823078632355
2025-01-09 15:49:53,899 - root - INFO - [Train] Epoch 8 [25300/35182]: Training loss 0.08585935831069946, current lr 9.564314544782695e-05, contrastive loss 3.4829790592193604, task loss 0.05102956295013428
2025-01-09 15:51:34,300 - root - INFO - [Train] Epoch 8 [25400/35182]: Training loss 0.07438362389802933, current lr 9.564172422614481e-05, contrastive loss 3.662787437438965, task loss 0.03775575011968613
2025-01-09 15:53:14,990 - root - INFO - [Train] Epoch 8 [25500/35182]: Training loss 0.11950403451919556, current lr 9.564030300446264e-05, contrastive loss 3.5525481700897217, task loss 0.0839785560965538
2025-01-09 15:54:55,397 - root - INFO - [Train] Epoch 8 [25600/35182]: Training loss 0.1498045176267624, current lr 9.563888178278048e-05, contrastive loss 3.4996337890625, task loss 0.11480817943811417
2025-01-09 15:56:36,076 - root - INFO - [Train] Epoch 8 [25700/35182]: Training loss 0.12013547122478485, current lr 9.563746056109833e-05, contrastive loss 3.4747490882873535, task loss 0.08538798242807388
2025-01-09 15:58:16,473 - root - INFO - [Train] Epoch 8 [25800/35182]: Training loss 0.09404434263706207, current lr 9.563603933941616e-05, contrastive loss 3.496856212615967, task loss 0.05907577648758888
2025-01-09 15:59:57,149 - root - INFO - [Train] Epoch 8 [25900/35182]: Training loss 0.11879727244377136, current lr 9.563461811773401e-05, contrastive loss 3.5058274269104004, task loss 0.08373899757862091
2025-01-09 16:01:37,840 - root - INFO - [Train] Epoch 8 [26000/35182]: Training loss 0.12832011282444, current lr 9.563319689605184e-05, contrastive loss 3.5573887825012207, task loss 0.09274622052907944
2025-01-09 16:03:18,236 - root - INFO - [Train] Epoch 8 [26100/35182]: Training loss 0.08669273555278778, current lr 9.56317756743697e-05, contrastive loss 3.574272632598877, task loss 0.050950005650520325
2025-01-09 16:04:58,805 - root - INFO - [Train] Epoch 8 [26200/35182]: Training loss 0.07391605526208878, current lr 9.563035445268753e-05, contrastive loss 3.3494672775268555, task loss 0.04042138159275055
2025-01-09 16:06:39,202 - root - INFO - [Train] Epoch 8 [26300/35182]: Training loss 0.08809857815504074, current lr 9.562893323100539e-05, contrastive loss 3.63850736618042, task loss 0.051713503897190094
2025-01-09 16:08:19,881 - root - INFO - [Train] Epoch 8 [26400/35182]: Training loss 0.12441091239452362, current lr 9.562751200932322e-05, contrastive loss 3.4807164669036865, task loss 0.08960375189781189
2025-01-09 16:10:00,304 - root - INFO - [Train] Epoch 8 [26500/35182]: Training loss 0.06657940149307251, current lr 9.562609078764106e-05, contrastive loss 3.6250815391540527, task loss 0.03032858856022358
2025-01-09 16:11:40,971 - root - INFO - [Train] Epoch 8 [26600/35182]: Training loss 0.10391482710838318, current lr 9.56246695659589e-05, contrastive loss 3.4743170738220215, task loss 0.06917165964841843
2025-01-09 16:13:21,382 - root - INFO - [Train] Epoch 8 [26700/35182]: Training loss 0.11767388880252838, current lr 9.562324834427675e-05, contrastive loss 3.561155319213867, task loss 0.08206234127283096
2025-01-09 16:15:02,057 - root - INFO - [Train] Epoch 8 [26800/35182]: Training loss 0.09351390600204468, current lr 9.562182712259459e-05, contrastive loss 3.642500877380371, task loss 0.05708890035748482
2025-01-09 16:16:42,465 - root - INFO - [Train] Epoch 8 [26900/35182]: Training loss 0.16370047628879547, current lr 9.562040590091242e-05, contrastive loss 3.5611865520477295, task loss 0.12808860838413239
2025-01-09 16:18:23,138 - root - INFO - [Train] Epoch 8 [27000/35182]: Training loss 0.082892045378685, current lr 9.561898467923028e-05, contrastive loss 3.5407309532165527, task loss 0.04748474061489105
2025-01-09 16:20:03,820 - root - INFO - [Train] Epoch 8 [27100/35182]: Training loss 0.12615811824798584, current lr 9.56175634575481e-05, contrastive loss 3.441922664642334, task loss 0.09173889458179474
2025-01-09 16:21:44,238 - root - INFO - [Train] Epoch 8 [27200/35182]: Training loss 0.17028774321079254, current lr 9.561614223586596e-05, contrastive loss 3.491981029510498, task loss 0.13536792993545532
2025-01-09 16:23:24,913 - root - INFO - [Train] Epoch 8 [27300/35182]: Training loss 0.17711369693279266, current lr 9.561472101418379e-05, contrastive loss 3.5676796436309814, task loss 0.14143690466880798
2025-01-09 16:25:05,325 - root - INFO - [Train] Epoch 8 [27400/35182]: Training loss 0.12850899994373322, current lr 9.561329979250165e-05, contrastive loss 3.588143825531006, task loss 0.09262756258249283
2025-01-09 16:26:45,995 - root - INFO - [Train] Epoch 8 [27500/35182]: Training loss 0.11055297404527664, current lr 9.561187857081948e-05, contrastive loss 3.5721065998077393, task loss 0.07483191043138504
2025-01-09 16:28:26,415 - root - INFO - [Train] Epoch 8 [27600/35182]: Training loss 0.08429615199565887, current lr 9.561045734913732e-05, contrastive loss 3.586622714996338, task loss 0.04842992126941681
2025-01-09 16:30:07,084 - root - INFO - [Train] Epoch 8 [27700/35182]: Training loss 0.09431220591068268, current lr 9.560903612745517e-05, contrastive loss 3.538763999938965, task loss 0.05892456695437431
2025-01-09 16:31:47,480 - root - INFO - [Train] Epoch 8 [27800/35182]: Training loss 0.14759701490402222, current lr 9.5607614905773e-05, contrastive loss 3.5436959266662598, task loss 0.11216006428003311
2025-01-09 16:33:28,160 - root - INFO - [Train] Epoch 8 [27900/35182]: Training loss 0.1130560040473938, current lr 9.560619368409085e-05, contrastive loss 3.5291502475738525, task loss 0.07776450365781784
2025-01-09 16:35:08,847 - root - INFO - [Train] Epoch 8 [28000/35182]: Training loss 0.11425409466028214, current lr 9.560477246240868e-05, contrastive loss 3.463503837585449, task loss 0.07961905747652054
2025-01-09 16:36:49,243 - root - INFO - [Train] Epoch 8 [28100/35182]: Training loss 0.0909186452627182, current lr 9.560335124072654e-05, contrastive loss 3.717228889465332, task loss 0.053746357560157776
2025-01-09 16:38:29,924 - root - INFO - [Train] Epoch 8 [28200/35182]: Training loss 0.09369973093271255, current lr 9.560193001904437e-05, contrastive loss 3.607481002807617, task loss 0.05762492120265961
2025-01-09 16:40:10,331 - root - INFO - [Train] Epoch 8 [28300/35182]: Training loss 0.12143364548683167, current lr 9.560050879736223e-05, contrastive loss 3.7365379333496094, task loss 0.08406826853752136
2025-01-09 16:41:51,008 - root - INFO - [Train] Epoch 8 [28400/35182]: Training loss 0.0854579508304596, current lr 9.559908757568006e-05, contrastive loss 3.6841063499450684, task loss 0.048616886138916016
2025-01-09 16:43:31,406 - root - INFO - [Train] Epoch 8 [28500/35182]: Training loss 0.08264757692813873, current lr 9.55976663539979e-05, contrastive loss 3.853292942047119, task loss 0.044114645570516586
2025-01-09 16:45:12,105 - root - INFO - [Train] Epoch 8 [28600/35182]: Training loss 0.0805450826883316, current lr 9.559624513231574e-05, contrastive loss 3.719273805618286, task loss 0.04335234686732292
2025-01-09 16:46:52,503 - root - INFO - [Train] Epoch 8 [28700/35182]: Training loss 0.13910506665706635, current lr 9.559482391063359e-05, contrastive loss 3.3903238773345947, task loss 0.1052018329501152
2025-01-09 16:48:33,176 - root - INFO - [Train] Epoch 8 [28800/35182]: Training loss 0.09543202072381973, current lr 9.559340268895143e-05, contrastive loss 3.663172721862793, task loss 0.05880029499530792
2025-01-09 16:50:13,565 - root - INFO - [Train] Epoch 8 [28900/35182]: Training loss 0.08013732731342316, current lr 9.559198146726926e-05, contrastive loss 3.439225435256958, task loss 0.04574507847428322
2025-01-09 16:51:54,143 - root - INFO - [Train] Epoch 8 [29000/35182]: Training loss 0.07866397500038147, current lr 9.559056024558712e-05, contrastive loss 3.5736136436462402, task loss 0.04292783886194229
2025-01-09 16:53:34,828 - root - INFO - [Train] Epoch 8 [29100/35182]: Training loss 0.10860610753297806, current lr 9.558913902390495e-05, contrastive loss 3.6849634647369385, task loss 0.07175647467374802
2025-01-09 16:55:15,226 - root - INFO - [Train] Epoch 8 [29200/35182]: Training loss 0.12128356844186783, current lr 9.55877178022228e-05, contrastive loss 3.6951980590820312, task loss 0.08433158695697784
2025-01-09 16:56:55,909 - root - INFO - [Train] Epoch 8 [29300/35182]: Training loss 0.08680474758148193, current lr 9.558629658054063e-05, contrastive loss 3.421337127685547, task loss 0.05259137600660324
2025-01-09 16:58:36,324 - root - INFO - [Train] Epoch 8 [29400/35182]: Training loss 0.11280561983585358, current lr 9.558487535885849e-05, contrastive loss 3.7068841457366943, task loss 0.07573677599430084
2025-01-09 17:00:16,994 - root - INFO - [Train] Epoch 8 [29500/35182]: Training loss 0.13049596548080444, current lr 9.558345413717632e-05, contrastive loss 3.5840911865234375, task loss 0.09465505927801132
2025-01-09 17:01:57,391 - root - INFO - [Train] Epoch 8 [29600/35182]: Training loss 0.11834956705570221, current lr 9.558203291549416e-05, contrastive loss 3.4533236026763916, task loss 0.08381632715463638
2025-01-09 17:03:38,079 - root - INFO - [Train] Epoch 8 [29700/35182]: Training loss 0.17100456357002258, current lr 9.5580611693812e-05, contrastive loss 3.348723888397217, task loss 0.13751733303070068
2025-01-09 17:05:18,465 - root - INFO - [Train] Epoch 8 [29800/35182]: Training loss 0.12037655711174011, current lr 9.557919047212984e-05, contrastive loss 3.625760555267334, task loss 0.08411895483732224
2025-01-09 17:06:59,050 - root - INFO - [Train] Epoch 8 [29900/35182]: Training loss 0.10008843243122101, current lr 9.55777692504477e-05, contrastive loss 3.554015636444092, task loss 0.06454827636480331
2025-01-09 17:08:39,459 - root - INFO - [Train] Epoch 8 [30000/35182]: Training loss 0.1357748955488205, current lr 9.557634802876552e-05, contrastive loss 3.49017071723938, task loss 0.1008731946349144
2025-01-09 17:10:20,129 - root - INFO - [Train] Epoch 8 [30100/35182]: Training loss 0.08493660390377045, current lr 9.557492680708338e-05, contrastive loss 3.528165817260742, task loss 0.049654945731163025
2025-01-09 17:12:00,819 - root - INFO - [Train] Epoch 8 [30200/35182]: Training loss 0.11088770627975464, current lr 9.557350558540121e-05, contrastive loss 3.368865489959717, task loss 0.0771990567445755
2025-01-09 17:13:41,226 - root - INFO - [Train] Epoch 8 [30300/35182]: Training loss 0.14969328045845032, current lr 9.557208436371907e-05, contrastive loss 3.540961742401123, task loss 0.11428366601467133
2025-01-09 17:15:21,895 - root - INFO - [Train] Epoch 8 [30400/35182]: Training loss 0.09597266465425491, current lr 9.55706631420369e-05, contrastive loss 3.407111167907715, task loss 0.061901554465293884
2025-01-09 17:17:02,312 - root - INFO - [Train] Epoch 8 [30500/35182]: Training loss 0.11549879610538483, current lr 9.556924192035474e-05, contrastive loss 3.605548858642578, task loss 0.0794433057308197
2025-01-09 17:18:42,987 - root - INFO - [Train] Epoch 8 [30600/35182]: Training loss 0.10595418512821198, current lr 9.556782069867258e-05, contrastive loss 3.688746452331543, task loss 0.06906671822071075
2025-01-09 17:20:23,376 - root - INFO - [Train] Epoch 8 [30700/35182]: Training loss 0.09014232456684113, current lr 9.556639947699043e-05, contrastive loss 3.34431791305542, task loss 0.05669914558529854
2025-01-09 17:22:04,062 - root - INFO - [Train] Epoch 8 [30800/35182]: Training loss 0.0936335101723671, current lr 9.556497825530827e-05, contrastive loss 3.493102550506592, task loss 0.058702483773231506
2025-01-09 17:23:44,478 - root - INFO - [Train] Epoch 8 [30900/35182]: Training loss 0.15146119892597198, current lr 9.55635570336261e-05, contrastive loss 3.6244144439697266, task loss 0.11521705985069275
2025-01-09 17:25:25,154 - root - INFO - [Train] Epoch 8 [31000/35182]: Training loss 0.11913028359413147, current lr 9.556213581194396e-05, contrastive loss 3.5571415424346924, task loss 0.08355886489152908
2025-01-09 17:27:05,836 - root - INFO - [Train] Epoch 8 [31100/35182]: Training loss 0.11699622869491577, current lr 9.556071459026179e-05, contrastive loss 3.46268892288208, task loss 0.08236933499574661
2025-01-09 17:28:46,238 - root - INFO - [Train] Epoch 8 [31200/35182]: Training loss 0.09756962954998016, current lr 9.555929336857964e-05, contrastive loss 3.4918363094329834, task loss 0.06265126168727875
2025-01-09 17:30:26,920 - root - INFO - [Train] Epoch 8 [31300/35182]: Training loss 0.11519978940486908, current lr 9.555787214689747e-05, contrastive loss 3.545198917388916, task loss 0.07974780350923538
2025-01-09 17:32:07,317 - root - INFO - [Train] Epoch 8 [31400/35182]: Training loss 0.10287647694349289, current lr 9.555645092521533e-05, contrastive loss 3.6643686294555664, task loss 0.06623279303312302
2025-01-09 17:33:48,010 - root - INFO - [Train] Epoch 8 [31500/35182]: Training loss 0.10505074262619019, current lr 9.555502970353316e-05, contrastive loss 3.7579925060272217, task loss 0.06747081875801086
2025-01-09 17:35:28,420 - root - INFO - [Train] Epoch 8 [31600/35182]: Training loss 0.10858672857284546, current lr 9.5553608481851e-05, contrastive loss 3.3629674911499023, task loss 0.07495705783367157
2025-01-09 17:37:09,091 - root - INFO - [Train] Epoch 8 [31700/35182]: Training loss 0.11668369174003601, current lr 9.555218726016885e-05, contrastive loss 3.4894351959228516, task loss 0.08178934454917908
2025-01-09 17:38:49,495 - root - INFO - [Train] Epoch 8 [31800/35182]: Training loss 0.12342920899391174, current lr 9.555076603848668e-05, contrastive loss 3.5749549865722656, task loss 0.0876796618103981
2025-01-09 17:40:30,174 - root - INFO - [Train] Epoch 8 [31900/35182]: Training loss 0.11340847611427307, current lr 9.554934481680453e-05, contrastive loss 3.5237951278686523, task loss 0.0781705230474472
2025-01-09 17:42:10,855 - root - INFO - [Train] Epoch 8 [32000/35182]: Training loss 0.10891705751419067, current lr 9.554792359512236e-05, contrastive loss 3.488595485687256, task loss 0.07403110712766647
2025-01-09 17:43:51,273 - root - INFO - [Train] Epoch 8 [32100/35182]: Training loss 0.0862775593996048, current lr 9.554650237344022e-05, contrastive loss 3.4402201175689697, task loss 0.05187535658478737
2025-01-09 17:45:31,946 - root - INFO - [Train] Epoch 8 [32200/35182]: Training loss 0.17669494450092316, current lr 9.554508115175805e-05, contrastive loss 3.4739418029785156, task loss 0.14195552468299866
2025-01-09 17:47:12,351 - root - INFO - [Train] Epoch 8 [32300/35182]: Training loss 0.16519616544246674, current lr 9.554365993007591e-05, contrastive loss 3.456634044647217, task loss 0.13062982261180878
2025-01-09 17:48:53,021 - root - INFO - [Train] Epoch 8 [32400/35182]: Training loss 0.11869174242019653, current lr 9.554223870839374e-05, contrastive loss 3.555911064147949, task loss 0.08313263207674026
2025-01-09 17:50:33,422 - root - INFO - [Train] Epoch 8 [32500/35182]: Training loss 0.144137442111969, current lr 9.554081748671158e-05, contrastive loss 3.42545223236084, task loss 0.10988292098045349
2025-01-09 17:52:14,106 - root - INFO - [Train] Epoch 8 [32600/35182]: Training loss 0.11099585890769958, current lr 9.553939626502942e-05, contrastive loss 3.5997681617736816, task loss 0.07499817758798599
2025-01-09 17:53:54,504 - root - INFO - [Train] Epoch 8 [32700/35182]: Training loss 0.0760740339756012, current lr 9.553797504334727e-05, contrastive loss 3.4367129802703857, task loss 0.041706908494234085
2025-01-09 17:55:35,184 - root - INFO - [Train] Epoch 8 [32800/35182]: Training loss 0.08288232982158661, current lr 9.553655382166511e-05, contrastive loss 3.5856339931488037, task loss 0.04702598601579666
2025-01-09 17:57:15,731 - root - INFO - [Train] Epoch 8 [32900/35182]: Training loss 0.12063956260681152, current lr 9.553513259998294e-05, contrastive loss 3.526453971862793, task loss 0.08537502586841583
2025-01-09 17:58:56,168 - root - INFO - [Train] Epoch 8 [33000/35182]: Training loss 0.12028239667415619, current lr 9.55337113783008e-05, contrastive loss 3.3169095516204834, task loss 0.08711330592632294
2025-01-09 18:00:36,839 - root - INFO - [Train] Epoch 8 [33100/35182]: Training loss 0.08783267438411713, current lr 9.553229015661863e-05, contrastive loss 3.403573513031006, task loss 0.05379693955183029
2025-01-09 18:02:17,237 - root - INFO - [Train] Epoch 8 [33200/35182]: Training loss 0.09835245460271835, current lr 9.553086893493648e-05, contrastive loss 3.5388708114624023, task loss 0.06296374648809433
2025-01-09 18:03:57,915 - root - INFO - [Train] Epoch 8 [33300/35182]: Training loss 0.10594584792852402, current lr 9.552944771325431e-05, contrastive loss 3.3817782402038574, task loss 0.072128064930439
2025-01-09 18:05:38,322 - root - INFO - [Train] Epoch 8 [33400/35182]: Training loss 0.1482408195734024, current lr 9.552802649157217e-05, contrastive loss 3.547581672668457, task loss 0.11276500672101974
2025-01-09 18:07:19,005 - root - INFO - [Train] Epoch 8 [33500/35182]: Training loss 0.15020397305488586, current lr 9.552660526989e-05, contrastive loss 3.55251145362854, task loss 0.11467885226011276
2025-01-09 18:08:59,409 - root - INFO - [Train] Epoch 8 [33600/35182]: Training loss 0.1247541606426239, current lr 9.552518404820784e-05, contrastive loss 3.5883736610412598, task loss 0.08887042850255966
2025-01-09 18:10:40,097 - root - INFO - [Train] Epoch 8 [33700/35182]: Training loss 0.16533878445625305, current lr 9.552376282652569e-05, contrastive loss 3.4474685192108154, task loss 0.13086409866809845
2025-01-09 18:12:20,489 - root - INFO - [Train] Epoch 8 [33800/35182]: Training loss 0.10362803936004639, current lr 9.552234160484352e-05, contrastive loss 3.545525312423706, task loss 0.06817278265953064
2025-01-09 18:14:01,193 - root - INFO - [Train] Epoch 8 [33900/35182]: Training loss 0.11087803542613983, current lr 9.552092038316137e-05, contrastive loss 3.5975513458251953, task loss 0.07490251958370209
2025-01-09 18:15:41,857 - root - INFO - [Train] Epoch 8 [34000/35182]: Training loss 0.12308570742607117, current lr 9.55194991614792e-05, contrastive loss 3.701366901397705, task loss 0.08607204258441925
2025-01-09 18:17:22,270 - root - INFO - [Train] Epoch 8 [34100/35182]: Training loss 0.07468146085739136, current lr 9.551807793979706e-05, contrastive loss 3.461127758026123, task loss 0.04007017984986305
2025-01-09 18:19:02,951 - root - INFO - [Train] Epoch 8 [34200/35182]: Training loss 0.11297936737537384, current lr 9.551665671811489e-05, contrastive loss 3.538086414337158, task loss 0.07759850472211838
2025-01-09 18:20:43,349 - root - INFO - [Train] Epoch 8 [34300/35182]: Training loss 0.1763334721326828, current lr 9.551523549643275e-05, contrastive loss 3.6656651496887207, task loss 0.13967682421207428
2025-01-09 18:22:24,031 - root - INFO - [Train] Epoch 8 [34400/35182]: Training loss 0.13009098172187805, current lr 9.551381427475058e-05, contrastive loss 3.403433084487915, task loss 0.09605665504932404
2025-01-09 18:24:04,437 - root - INFO - [Train] Epoch 8 [34500/35182]: Training loss 0.14063093066215515, current lr 9.551239305306842e-05, contrastive loss 3.6158294677734375, task loss 0.10447262972593307
2025-01-09 18:25:45,114 - root - INFO - [Train] Epoch 8 [34600/35182]: Training loss 0.09056425094604492, current lr 9.551097183138626e-05, contrastive loss 3.4357247352600098, task loss 0.05620700865983963
2025-01-09 18:27:25,517 - root - INFO - [Train] Epoch 8 [34700/35182]: Training loss 0.15233425796031952, current lr 9.550955060970411e-05, contrastive loss 3.331167697906494, task loss 0.11902257800102234
2025-01-09 18:29:06,206 - root - INFO - [Train] Epoch 8 [34800/35182]: Training loss 0.09433421492576599, current lr 9.550812938802195e-05, contrastive loss 3.5591440200805664, task loss 0.05874278023838997
2025-01-09 18:30:46,872 - root - INFO - [Train] Epoch 8 [34900/35182]: Training loss 0.14008203148841858, current lr 9.550670816633978e-05, contrastive loss 3.4787325859069824, task loss 0.10529470443725586
2025-01-09 18:32:27,261 - root - INFO - [Train] Epoch 8 [35000/35182]: Training loss 0.12258753180503845, current lr 9.550528694465764e-05, contrastive loss 3.525266647338867, task loss 0.08733486384153366
2025-01-09 18:34:07,955 - root - INFO - [Train] Epoch 8 [35100/35182]: Training loss 0.13897687196731567, current lr 9.550386572297547e-05, contrastive loss 3.5338945388793945, task loss 0.1036379262804985
